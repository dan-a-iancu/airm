{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S07-08_Bias_in_Machine_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2GOd-up_VBO_",
        "NK0jRm-Uyyvs",
        "FOsd8P1Vyyv9",
        "Cu9Pwx-IyywE",
        "gtQqi9hOyywG",
        "J2glFCOZyywJ",
        "pDR1145E3Dtw",
        "5A4qX8C2FUGE",
        "yHTfz8ZrsjPF",
        "34k4xVw1BtGk",
        "enz9e84Hb2yk",
        "VYRivVtrJCZ5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dan-a-iancu/airm/blob/master/ML_Bias/S07_08_Bias_in_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxhGE4RiVBO6"
      },
      "source": [
        "**Asessing Bias in Machine Learning Models**\n",
        "\n",
        "This notebook discusses issues of bias in predictive Machine Learning algorithms. As an example, we will be relying on the contentious case of COMPAS, which is a proprietary software designed by Northpointe Inc. (now part of [Equivant](https://www.equivant.com/)) and used throughout the United States in bail and sentencing decisions. For some background about COMPAS and the dispute initiated by journalists from ProPublica, check out these articles:\n",
        "- https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "- https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n",
        "\n",
        "We have three goals with this notebook and the associated analysis in Excel:\n",
        "1. To construct a Machine Learning model that predicts recidivism using the same dataset used in the ProPublica analysis;\n",
        "2. To evaluate our algorithm's performance, and most importantly, see if it suffers from similar biases as COMPAS;\n",
        "3. To discuss broader issues of **bias and fairness** in predictive ML models: how they can arise, how they can be detected, and how they can be fixed (or avoided)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ7HjwudVBO8"
      },
      "source": [
        "______\n",
        "<a id=\"2\"></a>\n",
        "# **Basic Setup**\n",
        "Let's load all the packages we need in Python.\n",
        "\n",
        "_**Please run this entire section.** No need to worry about understanding any of this code._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sicBV7d0VBO8",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@title Import revelant modules and define a few useful functions\n",
        "import os\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)   # Python ≥3.5 is required\n",
        "import urllib.request # for file downloading\n",
        "\n",
        "import numpy as np   # numpy for numerical linear algebra\n",
        "import pandas as pd  # pandas for managing dataframes\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from tensorflow.keras import layers\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn version ≥0.20 required\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# adjust reporting in pandas: max 15 rows and two-digit precision\n",
        "#pd.options.display.max_rows = 15\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "\n",
        "# import matplotlib and pyplot: critical packages for plotting \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "# Make sure Matplotlib runs inline, for nice figures\n",
        "%matplotlib inline  \n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# install the latest version of seaborn for nicer graphics\n",
        "!pip install --prefix {sys.prefix} seaborn==0.11.0  &> /dev/null\n",
        "import seaborn as sns\n",
        "\n",
        "# install facets overview\n",
        "!pip install facets-overview &> /dev/null\n",
        "# import facets overview + some other relevant packages\n",
        "from IPython.core.display import display, HTML\n",
        "import base64\n",
        "from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator\n",
        "\n",
        "# install graphviz for visualizing decision trees\n",
        "!pip install graphviz  &> /dev/null\n",
        "import graphviz\n",
        "\n",
        "# install pdpbox for partial dependency visualization\n",
        "!pip install pdpbox &> /dev/null\n",
        "from pdpbox import info_plots, pdp\n",
        "\n",
        "# import useful utilities from Google colab\n",
        "from google.colab import files\n",
        "\n",
        "# Ignore useless some warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action=\"ignore\")\n",
        "\n",
        "# Create a function to save figures to a desired local folder\n",
        "FIGURE_FOLDER = \"Figures\"\n",
        "FIGURE_PATH = os.path.join(\".\",FIGURE_FOLDER)\n",
        "os.makedirs(FIGURE_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(FIGURE_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# A function to append dummies for specified variables \n",
        "# returns the new dataframe and a dictionary with the categories for each categorical variable turned into a dummy\n",
        "def append_dummies(data, columns):\n",
        "  categories = {}  # the dictionary with categories\n",
        "  data_with_dummies = data.copy() # the changed dataframe\n",
        "  for col in columns:\n",
        "    if col not in data.columns:\n",
        "      print(\"WARNING. Column '{}' not among the columns in the dataframe. Skipping it.\")\n",
        "    elif (data[col].dtype!=int and data[col].dtype!=float):\n",
        "      dummy_df = pd.get_dummies(data[col], prefix=col)\n",
        "      categories[col] = list(dummy_df.columns)  # keep all the categories\n",
        "      data_with_dummies = pd.merge( left=data_with_dummies, \\\n",
        "                                    right=dummy_df, \\\n",
        "                                   left_index=True, right_index=True,\\\n",
        "                                   how=\"inner\", suffixes=(\"\", \"\") )\n",
        "  return data_with_dummies, categories\n",
        "\n",
        "print(\"Done with everything!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GOd-up_VBO_"
      },
      "source": [
        "_________\n",
        "<a id=\"3\"></a>\n",
        "# **Load the Data and Take a Look**\n",
        "We download our data from the ProPublica Github site, and then follow a similar cleaning and filtering process as in their analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlh0YYAWyvwg",
        "cellView": "form"
      },
      "source": [
        "#@title Download the data, process it following ProPublica's rules, and save it\n",
        "\n",
        "#@markdown 1) Download data from ProPublica GitHub account and save it as a CSV file\n",
        "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"  # full URL to the dataset\n",
        "local_csv = \"compas_data.csv\"   # name of local file where you want to store the downloaded file\n",
        "urllib.request.urlretrieve(url, local_csv)    # download from website and save it locally\n",
        "\n",
        "# Read the data into a `pandas` DataFrame\n",
        "raw_data = pd.read_csv(local_csv, index_col=\"id\")\n",
        "\n",
        "#@markdown 2) Filter the data using the same criteria as ProPublica\n",
        "data = raw_data[ [\"age\", \"c_charge_degree\", \"race\", \"age_cat\", \"score_text\", \"sex\", \"priors_count\", \\\n",
        "              \"days_b_screening_arrest\", \"decile_score\", \"is_recid\", \"two_year_recid\", \"c_jail_in\", \"c_jail_out\"] ]\n",
        "\n",
        "data = data.loc[ (data[\"days_b_screening_arrest\"] <= 30) & (data[\"days_b_screening_arrest\"] >= -30) & (data[\"is_recid\"] != -1) & \\\n",
        "       (data[\"c_charge_degree\"] != \"O\") & (data[\"score_text\"] != \"N/A\") ]\n",
        "\n",
        "#@markdown - In addition, we remove a few more columns to avoid confusion:\n",
        "#@markdown  - **c_jail_in**, **c_jail_out**, **days_b_screening_arrest** : these are not useful in the prediction\n",
        "#@markdown  - **is_recid** is a flag used by ProPublica, not needed for prediction\n",
        "#@markdown  - **age** and **age_cat** are redundant; we keep **age**\n",
        "data.drop(columns=[\"c_jail_in\",\"c_jail_out\", 'days_b_screening_arrest','age_cat', \"is_recid\"], inplace=True)\n",
        "data.drop(columns=[\"score_text\"], inplace=True)  # we also drop the text score for COMPAS\n",
        "#data.drop(columns=[\"score_text\",\"decile_score\"], inplace=True)  # drop all the COMPAS scores\n",
        "\n",
        "#@markdown - To focus our classroom discussion, we also remove all records where **race** is different than African-American or Caucasian\n",
        "data = data.loc[ (data[\"race\"]=='African-American') | (data[\"race\"]=='Caucasian') ]\n",
        "# #@markdown  - since the data has very few **Asian** and **Native American** records, we re-label these as **Other**\n",
        "# #data.loc[ (data[\"race\"]=='Asian') | (data[\"race\"]=='Native American'), \"race\" ] = \"Other\"\n",
        "\n",
        "print(\"Done with all data processing tasks!\")\n",
        "\n",
        "#@markdown 3) If desired, save the processed data as an Excel file and download it to your local machine\n",
        "download_to_Excel = False #@param {type:\"boolean\"}\n",
        "\n",
        "#markdown Write the processed data to an Excel file and download that file to your local machine\n",
        "if download_to_Excel :\n",
        "    data.to_excel('compas_data_processed.xlsx', sheet_name='Filtered_data', na_rep='', verbose=True, freeze_panes=None)\n",
        "    files.download('compas_data_processed.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m11zw3VynCX",
        "cellView": "form"
      },
      "source": [
        "#@title Take a quick look at the dataframe\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK0jRm-Uyyvs"
      },
      "source": [
        "__________\n",
        "<a id=\"1\"></a>\n",
        "# **The Prediction Problem and the Data Dictionary**\n",
        "The specific prediction problem we are interested in is a **binary classification** task: we would like to predict whether a defendant is going to reoffend (i.e., commit another crime) within the next two years, based on information including criminal history and other personal features.\n",
        "\n",
        "_For a detailed data dictionary, click to expand the hidden cells._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfGvGkr8-dIj"
      },
      "source": [
        "**How was the data collected and processed?**<br>\n",
        "We are using the same dataset published by ProPublica of its [Github site](https://github.com/propublica/compas-analysis). The data is based on information on defendants from Broward County (Florida), and we used a similar process to ProPublica for cleaning and filtering it. The **data dictionary** for our fields is as follows:  \n",
        "  *   `age`: The defendant's age (numeric)\n",
        "  *   `c_charge_degree`: The type of crime for which the arrest was made (Categorical field with two potential values: **M** for **misdemeanor**, which is a less serious crime or **F** for **felony**, which is a more serious crime)\n",
        "  *   `race`: The defendant's race (categorical field; to simplify our task, we only kept records that were either **African American** or **Caucasian**)\n",
        "  *   `sex`: The defendent's gender (categorical field with value **Male** or **Female**)\n",
        "  *   `priors_count`: Total number of prior convictions for the defendant (numeric)\n",
        "  *   `decile_score`: This is the COMPAS risk score (1-10) received by the defendant when screened with the algorithm. (_We will <font color=red>not</font> use this feature when constructing our own risk-scoring tool, but we will use it to analyze the bias and accuracy of the COMPAS algorithm._) \n",
        "  *    `two_year_recid`: Indicator for whether the defendant reoffended within two years from the date of the scoring (1 for YES, 0 for NO).\n",
        "\n",
        "**What are we trying to predict / what is the precise target?**<br>\n",
        "The specific problem we are interested in is a **binary classification** task: we would like to determine the likelihood that a defendant is going to reoffend (i.e., commit another crime) within the next two years. So **the target in our prediction problem is `two_year_recid`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seCFt-HdCQ0g"
      },
      "source": [
        "# <font color = red>**TASK #1**\n",
        "**Have a look at the associated PDF file for a description of the task, and use the information and analysis below to check your answers.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZR2rHaPyyv9"
      },
      "source": [
        "_________________\n",
        "<a id=\"5\"></a>\n",
        "# **1. Exploratory Data Analysis**\n",
        "Feel free to explore the data using one of the tools we introduced before.\n",
        "\n",
        "*This is obviously an important section. We suggest expanding the hidden cells and then choosing which sub-section to expand further.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOsd8P1Vyyv9"
      },
      "source": [
        "## Full Analysis with **Google Facets**\n",
        "*Expand this sub-section as needed.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BHbdWZSyywA",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@title Visualize the Data in **Facets Overview**\n",
        "\n",
        "# generate the visualization\n",
        "gfsg = FeatureStatisticsGenerator()\n",
        "\n",
        "proto = gfsg.ProtoFromDataFrames([{'name': 'original_data', 'table': data}])\n",
        "# uncomment next two lines if you want to visualize training & test sets\n",
        "#proto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': train_set},\n",
        "#                                  {'name': 'test', 'table': test_set}])\n",
        "protostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-overview id=\"elem\"></facets-overview>\n",
        "        <script>\n",
        "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(protostr=protostr)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gECiKXGpyywb",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@title Correlation Analysis in **Facets Dive**\n",
        "\n",
        "SAMPLE_SIZE = 5000 #@param\n",
        "  \n",
        "data_dive = data.sample(SAMPLE_SIZE).to_json(orient='records')\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
        "        <script>\n",
        "          var data = {jsonstr};\n",
        "          document.querySelector(\"#elem\").data = data;\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(jsonstr=data_dive)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoudwDaryywD"
      },
      "source": [
        "<a id=\"EDA-NutsBolts\"></a>\n",
        "## Nuts & Bolts\n",
        "\n",
        "*As you start conducting <font color=red>**TASK #1**</font>, you may want to expand certain sections here to compare outputs.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Si7apfQJgbz",
        "cellView": "form"
      },
      "source": [
        "#@markdown Define various functions to help with visualizations\n",
        "\n",
        "# #@markdown - a function for simple visualizations of one or more features with histograms/boxplots/countplots\n",
        "def visualize_features(all_data, features, figsize=(6,6), num_plot='histogram', \\\n",
        "                       num_bins='auto', num_cols=3):\n",
        "\n",
        "    # calculate how many figures are needed\n",
        "    num_figs = len(features)\n",
        "    num_columns = (1 if num_figs==1 else num_cols)\n",
        "    num_rows = int(np.ceil(num_figs/num_columns))\n",
        "    \n",
        "    # one large figure with a common title\n",
        "    fig = plt.figure(figsize=(figsize[0]*num_columns,figsize[1]*num_rows))\n",
        "    \n",
        "    # title for plot\n",
        "    title = ''\n",
        "\n",
        "    for i in range(num_figs):\n",
        "\n",
        "      # Create figure\n",
        "      plt.subplot(num_rows,num_columns,i+1)\n",
        "      ax1 = plt.gca()\n",
        "      ax1.grid(alpha=.5, linewidth=1)\n",
        "\n",
        "      if( (all_data[features[i]].dtype == 'float') or (all_data[features[i]].dtype == 'int') ):\n",
        "        # numeric feature \n",
        "        \n",
        "        # plot the feature according to the option\n",
        "        if( num_plot == 'histogram'):\n",
        "          #title = \"Histogram of {}\".format(features[i])\n",
        "          sns.histplot(data = all_data[features[i]], bins=num_bins, color=\"skyblue\", ax=ax1)\n",
        "\n",
        "        elif( num_plot == 'boxplot' ):\n",
        "          #title = \"Boxplot of {}\".format(features[i])\n",
        "          sns.boxplot(data = all_data[features[i]], color=\"skyblue\", ax=ax1)\n",
        "        plt.tight_layout()\n",
        "\n",
        "      else:\n",
        "        # categorical feature\n",
        "        sns.countplot(data = all_data, x=features[i], palette=\"Blues\", ax=ax1)\n",
        "        plt.tight_layout()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # determine the title\n",
        "    #tar_ylabel = '{}'.format(target_name) if target_name else 'target'\n",
        "    #fig.suptitle(\"Predicted value of {} as a function of feature(s).\".format(tar_ylabel), fontsize=16, color=color1)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#\n",
        "#\n",
        "# #@markdown - A function to visualize **average dependency** of one numeric feature as a function of two other features\n",
        "def visualize_average_dependency(all_data_with_dummies, all_categories, \\\n",
        "                                 feature_y, feature_x1, feature_x2=None,\\\n",
        "                                 num_bins=10, uniform_bins=False, \\\n",
        "                                 show_outliers=False, figsize=(10,9)):\n",
        "   \n",
        "    # append to all categories a string for numeric categories\n",
        "    all_feats = [feature_y, feature_x1]\n",
        "    if feature_x2 != None:\n",
        "      all_feats += [feature_x2]\n",
        "\n",
        "    categ_for_infoplot = all_categories.copy()\n",
        "    for feat_ in all_feats:\n",
        "      if feat_ not in all_categories.keys():\n",
        "        # just the names for numeric features\n",
        "        categ_for_infoplot[feat_] = feat_\n",
        "\n",
        "    if uniform_bins:\n",
        "      grid_type='equal'\n",
        "    else:\n",
        "      grid_type='percentile'\n",
        "\n",
        "    # do all the infoplots\n",
        "    if feature_x2==None:\n",
        "      # visualizing just two features, so a simple target_plot will do\n",
        "      title = \"Relation between target '{}' and feature '{}'\".format(feature_y,feature_x1)\n",
        "      subtitle = \"The count/height of bars shows the number of samples where {} takes that value\".format(feature_x1) +\\\n",
        "        \"\\n and the line plot shows the average value of the target.\"\n",
        "\n",
        "      fig, axes, summary_df = \\\n",
        "      info_plots.target_plot(df=all_data_with_dummies, \\\n",
        "                            feature=categ_for_infoplot[feature_x1], \\\n",
        "                            feature_name=feature_x1, \\\n",
        "                            target=categ_for_infoplot[feature_y],\\\n",
        "                            num_grid_points=num_bins, grid_type=grid_type, \\\n",
        "                            percentile_range=None, \\\n",
        "                            grid_range=None, cust_grid_points=None, \\\n",
        "                            show_percentile=False, \\\n",
        "                            show_outliers=show_outliers, endpoint=True, \\\n",
        "                            figsize=figsize, ncols=2, \\\n",
        "                            plot_params={\"title\" : title, \"subtitle\": subtitle})\n",
        "    else:\n",
        "      # plotting y as color-coded, as a function of x1 and x2\n",
        "      title = \"Relation between target '{}' and features '{}' and '{}'\".format(feature_y,feature_x1,feature_x2)\n",
        "      subtitle = \"The circle color shows the average value of the target\" +\\\n",
        "        \"\\n and the circle size shows the number of samples.\"\n",
        "\n",
        "      fig, axes, summary_df = \\\n",
        "      info_plots.target_plot_interact(df=data_with_dummies, \\\n",
        "                                      features=[categ_for_infoplot[feature_x1],categ_for_infoplot[feature_x2]], \\\n",
        "                                      feature_names=[feature_x1,feature_x2], \\\n",
        "                                      target=categ_for_infoplot[feature_y], \\\n",
        "                                      num_grid_points=[num_bins,num_bins], \\\n",
        "                                      grid_types=[grid_type,grid_type], \\\n",
        "                                      percentile_ranges=None, \\\n",
        "                                      grid_ranges=None, cust_grid_points=None, \\\n",
        "                                      show_percentile=False, \\\n",
        "                                      show_outliers=show_outliers, endpoint=True, \\\n",
        "                                      figsize=figsize, ncols=2, annotate=True, \\\n",
        "                                      plot_params={\"title\" : title, \\\n",
        "                                                    \"subtitle\" : subtitle})    \n",
        "#\n",
        "#\n",
        "#\n",
        "# #@markdown - A complex function to visualize relationships among several features\n",
        "def visualize_detailed_dependencies(all_data_with_dummies, all_categories, \\\n",
        "                                    target, feature, feature_color=None,\\\n",
        "                                    feature_breakdown=None, \\\n",
        "                                    plot_type='scatter', min_categ=5, orient=None,\\\n",
        "                                    figs_per_row=3, figsize=(10,9)):\n",
        "\n",
        "    # check to see if there's a need to change the plot type\n",
        "    if plot_type=='boxen':\n",
        "        # check to make sure that either the target or one of the features is numerical with enough values\n",
        "        if (target in data_categories.keys() \\\n",
        "            and feature in data_categories.keys() ) or \\\n",
        "            ( len(all_data_with_dummies[target].unique()) <= min_categ and \\\n",
        "             len(all_data_with_dummies[feature].unique()) <= min_categ ):\n",
        "            # target and first feature are categorical or have very few values\n",
        "            if feature_color in data_categories.keys() or \\\n",
        "            ( len(all_data_with_dummies[feature_color].unique()) <= min_categ):\n",
        "              # second feature is also categorical or with very few values \n",
        "              # it means all features are categorical -> switch to countplot\n",
        "              print('Target and all selected features are categorical or have very few values. Switching to a countplot.')\n",
        "              plot_type = 'count'\n",
        "            else:\n",
        "              # third feature is numeric with enough values --> switch with first feature\n",
        "              aux=feature\n",
        "              feature=feature_color\n",
        "              feature_color=aux\n",
        "\n",
        "    if plot_type=='scatter':\n",
        "        # simple scatter plot\n",
        "        sns.relplot(data=data_with_dummies, x=feature, y=target, \\\n",
        "                    hue=feature_color, col=feature_breakdown, \\\n",
        "                    row=None, style=None, col_wrap=None, row_order=None, \\\n",
        "                    col_order=None, palette=None, hue_order=None, hue_norm=None, \\\n",
        "                    sizes=None, size_order=None, size_norm=None, markers=None, \\\n",
        "                    dashes=None, style_order=None, legend='auto', \\\n",
        "                    kind='scatter', height=figsize[1], \\\n",
        "                    aspect=figsize[0]/figsize[1], facet_kws=None, units=None)\n",
        "\n",
        "    if plot_type=='count':\n",
        "        sns.catplot(data=data_with_dummies, x=target, y=None,\\\n",
        "                    hue=feature, col=feature_breakdown, \\\n",
        "                    kind=plot_type, row=None, col_wrap=None, \\\n",
        "                    row_order=None, col_order=None, \\\n",
        "                    ci=95, n_boot=1000, units=None, seed=None, order=None, \\\n",
        "                    hue_order=None, height=figsize[1], aspect=figsize[0]/figsize[1], \\\n",
        "                    orient=orient, color=None, palette=None, legend=True, \\\n",
        "                    legend_out=True, sharex=True, sharey=True, \\\n",
        "                    margin_titles=False, facet_kws=None)\n",
        "\n",
        "    if plot_type=='boxen':\n",
        "        # check if the number of categories for the target is smaller than for the feature\n",
        "        if( len(all_data_with_dummies[target].unique()) < \\\n",
        "           len(all_data_with_dummies[feature].unique()) ):\n",
        "          orient='h'\n",
        "        sns.catplot(data=data_with_dummies, x=feature, y=target, \\\n",
        "                    hue=feature_color, col=feature_breakdown, \\\n",
        "                    kind=plot_type, row=None, col_wrap=None, \\\n",
        "                    row_order=None, col_order=None, \\\n",
        "                    ci=95, n_boot=1000, units=None, seed=None, order=None, \\\n",
        "                    hue_order=None, height=figsize[1], aspect=figsize[0]/figsize[1], \\\n",
        "                    orient=orient, color=None, palette=None, legend=True, \\\n",
        "                    legend_out=True, sharex=True, sharey=True, \\\n",
        "                    margin_titles=False, facet_kws=None)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu9Pwx-IyywE"
      },
      "source": [
        "### Examine the Data Types\n",
        "A good start is to check more carefully the kind of values that each attribute takes in the dataset.<br>\n",
        "<font color=green>**EXCEL:** You can add a [**Filter**](https://support.microsoft.com/en-us/office/filter-data-in-a-range-or-table-01832226-31b5-4568-8806-38c37dcc180e#:~:text=Select%20any%20cell%20within%20the,filter%20criteria%20and%20select%20OK.) from the **Data** menu, and glance at the values for each field.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7mKMvdVBPH",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@markdown Examine the Data Types\n",
        "\n",
        "# take a look at the type of data\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtQqi9hOyywG"
      },
      "source": [
        "### Summary Statistics and Counts\n",
        "Summarize the data through various statistics and counts.<br>\n",
        "<font color=green>**EXCEL:** Recall that you can do this using [**Descriptive Statistics**](https://www.excel-easy.com/examples/descriptive-statistics.html) from the **Data Analysis** TookPak, under the **Data** menu.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcjRS80nVBPM",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@markdown **Compute summary statistics for the numeric attributes**\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzfsNJcE3srl",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@markdown **Cross-tabulate one feature against one of the categorical features**\n",
        "\n",
        "#@markdown - select a (numeric or categorical) feature and another categorical feature to cross-tabulate\n",
        "feature = \"two_year_recid\" #@param ['age', 'c_charge_degree', 'race', 'sex', 'priors_count','decile_score','two_year_recid']\n",
        "feature_categorical = \"sex\" #@param ['c_charge_degree', 'race', 'sex']\n",
        "\n",
        "#@markdown - select whether to show the values as fractions of the totals on rows or columns (selecting both will show as fraction of grand total)\n",
        "fraction_of_row_total = True #@param { type:\"boolean\"}\n",
        "fraction_of_column_total = True #@param { type:\"boolean\"}\n",
        "\n",
        "normalize = False\n",
        "if fraction_of_row_total:\n",
        "  if fraction_of_column_total:\n",
        "     normalize = 'all'\n",
        "  else:\n",
        "     normalize = 'index'\n",
        "else:\n",
        "  if fraction_of_column_total:\n",
        "     normalize = 'columns'\n",
        "\n",
        "pd.crosstab( index = data[feature], columns = data[feature_categorical] , \\\n",
        "            normalize=normalize, margins=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2glFCOZyywJ"
      },
      "source": [
        "### Histograms and Countplots\n",
        "Build histograms and countplots to visualize the values of individual features.<br>\n",
        "<font color=green>**EXCEL:** You can create histograms and boxplots from  **Insert > Insert Statistic Chart** ([click here](https://support.microsoft.com/en-us/office/create-a-histogram-85680173-064b-4024-b39d-80f17ff2f4e8) for details).<br> The most effective way to visualize categorical features in Excel is by creating a **Pivot Chart** from the **Insert** menu ([click here](https://support.microsoft.com/en-us/office/create-a-pivotchart-c1b1e057-6990-4c38-b52b-8255538e7b1c) for details).</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPxN7HZwyywJ",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@markdown **Build histograms for numeric features or countplots for categorical ones**\n",
        "\n",
        "#@markdown Select which features(s) to visualize (picking **\"All\"** will plot dependencies for all features, but will take a bit more time):\n",
        "feature = \"All\" #@param ['All', 'age', 'c_charge_degree', 'race',  'sex', 'priors_count', 'two_year_recid']\n",
        "\n",
        "# complete list of features, just in case it's needed\n",
        "# ['All', 'age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', 'decile_score', 'is_recid', 'two_year_recid']\n",
        "\n",
        "if(feature==\"All\"):\n",
        "  # all features except ocean proximity (which is categorical)\n",
        "  feature = list(data.columns)\n",
        "else:\n",
        "  feature = [feature]\n",
        "\n",
        "#@markdown <br><br>**Feel free to customize some elements of the plot (leave as \"Auto\" if unsure)**\n",
        "\n",
        "#@markdown Choose or type how many figures to display per row:\n",
        "figures_per_row = \"4\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figures_per_row==\"Auto\":\n",
        "  figures_per_row=4\n",
        "else:\n",
        "  figures_per_row=np.int(figures_per_row)\n",
        "\n",
        "#@markdown Choose or type how many bins to use for the histogram:\n",
        "num_bins = \"Auto\" #@param [\"Auto\",5,10,20,50,100]{allow-input: true}\n",
        "if num_bins==\"Auto\":\n",
        "  num_bins=10\n",
        "else:\n",
        "  num_bins=np.int(num_bins)\n",
        "\n",
        "#@markdown Choose or type the width and height for each figure in inches:\n",
        "figure_width = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=5.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=5.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "# plot the feature(s)\n",
        "visualize_features(all_data=data, features=feature, \\\n",
        "                   figsize=(figure_width,figure_height), num_plot=\"histogram\", \\\n",
        "                   num_bins=num_bins, num_cols=figures_per_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc-gvUlw2TGq"
      },
      "source": [
        "### Examine Dependencies\n",
        "Examine the relationships and dependencies between the features by computing correlation coefficients and through various visualizations like scatter plots, boxen plots, or average dependency plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k0xPu-2yywe",
        "cellView": "form"
      },
      "source": [
        "#@title **Display the entire correlation matrix**\n",
        "# compute the matrix\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "# plot the matrix as a heatmap\n",
        "plt.figure(figsize = (6,3));\n",
        "g = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
        "g.set_yticklabels(g.get_yticklabels(), rotation = 0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31PZQtmF2Pwq",
        "cellView": "form"
      },
      "source": [
        "#@title **Visualize the relationships between features**\n",
        "\n",
        "# append dummies and store categories for all data (useful for plotting routines)\n",
        "data_with_dummies, data_categories = append_dummies(data, data.columns)\n",
        "\n",
        "#@markdown Select the **main feature/target** for which you want to visualize dependencies:\n",
        "main_feature = 'two_year_recid' #@param ['age', 'c_charge_degree', 'race', 'sex', 'priors_count', 'decile_score', 'two_year_recid']\n",
        "\n",
        "#@markdown Select one feature, numeric or categorical:\n",
        "feature_1 = 'age' #@param ['age', 'c_charge_degree', 'race', 'sex', 'priors_count', 'two_year_recid']\n",
        "\n",
        "#@markdown (Optional) An additional numeric or categorical feature to visualize:\n",
        "feature_2 = \"None\" #@param ['None', 'age', 'c_charge_degree', 'race', 'sex', 'priors_count', 'two_year_recid']\n",
        "\n",
        "#@markdown (Optional) A categorical feature to use for breaking down into several sub-plots:\n",
        "breakdown_feature = \"None\" #@param ['None', 'c_charge_degree', 'race', 'sex']\n",
        "\n",
        "#@markdown <br>**Select the type of plot to see**\n",
        "#@markdown - **scatter** is the usual scatter plot you are familiar with\n",
        "#@markdown - **average_dependency** displays the average value of the **`main_feature`**\n",
        "#@markdown - **count** attempts to do a simple count plot\n",
        "#@markdown - **boxen** attempts to do a boxen-plot (more detailed version of a boxplot)\n",
        "plot_type = 'average_dependency' #@param ['average_dependency', 'scatter', 'count', 'boxen']\n",
        "\n",
        "#@markdown <br>**You can further customize some aspects of the plot**\n",
        "\n",
        "#@markdown General settings:\n",
        "fig_width = '20' #@param ['Auto']{allow-input : true}\n",
        "fig_height = '10' #@param ['Auto']{allow-input : true}\n",
        "\n",
        "#@markdown For an average dependency plot:\n",
        "#@markdown - **`num_bins`** : the number of bins to use when discretizing a numeric feature\n",
        "#@markdown - **`equal_probability`** : whether to use bins with roughly equal probability (unchecking this will generate bins with equal width)\n",
        "num_bins = '10' #@param ['Auto']{allow-input : true}\n",
        "equal_probability = False #@param {type:\"boolean\"}\n",
        "\n",
        "# print the selections\n",
        "print(\"Your selections:\")\n",
        "print(\"Main feature/target :  \", main_feature)\n",
        "print(\"Feature_1 :            \", feature_1)\n",
        "print(\"Feature_2 :            \", feature_2)\n",
        "print(\"Breakdown feature :    \", breakdown_feature)\n",
        "print(\"Plot type:             \", plot_type)\n",
        "\n",
        "# set up parameters properly\n",
        "if feature_2==\"None\":\n",
        "  feature_2 = None\n",
        "\n",
        "if breakdown_feature==\"None\":\n",
        "  breakdown_feature = None\n",
        "\n",
        "if fig_width=='Auto':\n",
        "  fig_width=7.0\n",
        "else:\n",
        "  fig_width=np.float(fig_width)\n",
        "\n",
        "if fig_height=='Auto':\n",
        "  fig_height=7.0\n",
        "else:\n",
        "  fig_height=np.float(fig_height)\n",
        "\n",
        "if num_bins=='Auto':\n",
        "  num_bins=10\n",
        "else:\n",
        "  num_bins=np.int(num_bins)\n",
        "\n",
        "if plot_type=='average_dependency':\n",
        "    visualize_average_dependency(all_data_with_dummies=data_with_dummies,\\\n",
        "                                  all_categories=data_categories, \\\n",
        "                                  feature_y=main_feature, feature_x1=feature_1,\\\n",
        "                                  feature_x2=feature_2,num_bins=num_bins,\\\n",
        "                                  uniform_bins=(not equal_probability),\\\n",
        "                                  show_outliers=True,\\\n",
        "                                  figsize=(fig_width,fig_height))\n",
        "\n",
        "else:\n",
        "    if plot_type=='categorical':\n",
        "        # check to make sure that either the target or one of the features is numerical\n",
        "        if main_feature in data_categories.keys() and feature in data_categories.keys():\n",
        "            # target and first feature are categorical\n",
        "            if feature_2 in data_categories.keys():\n",
        "              # second feature is also categorical; since the third feature \n",
        "              # is also categorical by choice, all are categorical -> switch to countplot\n",
        "              print('Target and all selected features are categorical. Switching to a countplot of target.')\n",
        "              plot_type = 'count'\n",
        "            else:\n",
        "              # feature_2 is numeric, will switch with feature_1\n",
        "              aux=feature_1\n",
        "              feature_1=feature_2\n",
        "              feature_2=aux\n",
        "\n",
        "    visualize_detailed_dependencies(all_data_with_dummies=data_with_dummies, \\\n",
        "                          all_categories=data_categories, \\\n",
        "                          target=main_feature, \\\n",
        "                          feature=feature_1, \\\n",
        "                          feature_color=feature_2,\\\n",
        "                          feature_breakdown=breakdown_feature,\\\n",
        "                          plot_type=plot_type,\\\n",
        "                          figs_per_row=3, figsize=(fig_width,fig_height))\n",
        "\n",
        "# uncomment next line to save\n",
        "#save_fig(\"scatterplot_of_{}_and_{}\".format(feature1,feature2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzAev5NvCqwu"
      },
      "source": [
        "# <font color = red>**TASK #2**\n",
        "**Have a look at the associated PDF file for a description of the task, and use the analysis below to answer the questions.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOe3bM8Jyyw5"
      },
      "source": [
        "__________\n",
        "<a id=\"6\"></a>\n",
        "# **2. Split into a Training and a Test Set + Prepare the Data for ML Algorithms**\n",
        "\n",
        "_For simplicity and to focus on the discrimination issues, we will use the entire dataset here as a training set. We already cleaned our data when we imported it, so here we are only transforming the categorical features. Expand for details._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOzPhbBQVBPk",
        "cellView": "form"
      },
      "source": [
        "#@title We will follow a very simple process here \n",
        "\n",
        "#@markdown 1. **Train/Test split**: We use the entire dataset as a training set\n",
        "train_set = data\n",
        "# uncomment if you prefer to do a simple 80-20 Random Split\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "#@markdown 2. **Data cleaning** - already done when importing the data, following ProPublica's procedure\n",
        "\n",
        "# make a copy of the training and testing here, before defining the categorical features\n",
        "train_copy = train_set.copy()\n",
        "#test_copy = test_set.copy()\n",
        "\n",
        "#@markdown 3. **Handling Text and Categorical Features** - we transform any categorical features into \"dummy variables\"<br>\n",
        "#@markdown We delay this step for now because we want to allow you to **select which features to use** in the model\n",
        "\n",
        "#@markdown 4. **Feature scaling** - not needed here since we will be using ML methods that are not sensitive to scaling\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WENSYHnvyyxi"
      },
      "source": [
        "_____\n",
        "# **3. Train the Machine Learning Models**\n",
        "We will train a **decision tree** and a **random forest**.\n",
        "\n",
        "_Expand this section for a few options on which features to use during training._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDR1145E3Dtw"
      },
      "source": [
        "## Our ML Models in a Nutshell \n",
        "_If you have never seen some of these models, you may want to uncomment this and read for a very quick description._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWOtZnsaMEx"
      },
      "source": [
        "The two models that we are training can be quickly summarized as follows:\n",
        "1. A **Classification Decision Tree** is very similar to a **Regression Decision Tree**, with a few small differences. Specifically:\n",
        "* The basic logic of the tree is exactly the same as in a regression task: it relies on a sequence of **IF** statements to partition the space of predictors, and once a leaf node is reached, the final prediction for **<font color=green>the probability $\\hat{p}_1$ that the target is equal to 1</font>** is given by **the average of the samples in the leaf node**. \n",
        "* Since for a classification task we are also interested in a **<font color=blue>final prediction  of 0 or 1 for the actual target $\\hat{y}$</font>**, this can be obtained **by taking a majority vote based on the samples in the leaf**, i.e., by predicting 1 if most of the leaf samples have a target 1, and 0 otherwise. Note that this is exactly equivalent to **predicting 1 if and only if <font color=green>the predicted probability $\\hat{p}_1$</font> exceeds the threshold 0.5**.\n",
        "* Because the predicted target in a classification cannot take any possible values (the values are always between 0 and 1), instead of measuring the prediction error with **mean squared errors** or **mean absolute errors**, one of the following two approaches are used:\n",
        "    - the **gini impurity index** in a node of the tree is given by $\\sum_{k \\in \\{0,1\\}} \\hat{p}_k(1-\\hat{p}_k)$, where $\\hat{p}_k$ is the fraction of samples in the node that have a target value of $k$. To understand this error measure, note that it takes a value of 0 if either all the samples in the node take value 0 (i.e., $\\hat{p}_0=1, \\hat{p}_1=0$) or if all the samples take value 1 (i.e., $\\hat{p}_0=0, \\hat{p}_1=1$). Otherwise, it takes a positive value, and this is actually largest when exactly half the samples have target value 1 (i.e., $\\hat{p}_1=\\hat{p}_0 = 0.5$). Therefore, the gini index is really a measure of **node impurity**, with \"low errors\" when nodes are pure and \"high errors\" when they are split evenly.\n",
        "    - the **entropy** is an alternative to the gini index, and is given by $- \\sum_{k \\in \\{0,1\\}} \\hat{p}_k \\log \\hat{p}_k$. It is based on information theory (and the work of Claude Shannon), and it can also be shown to take a value 0 in pure nodes, and the largest value in nodes that have an even split of targets equal to 1 and 0.<br>\n",
        "  Of the two error measures above, the gini impurity index is the more common one due to its simplicity. For more details on (Classification) Decision Trees, you can refer to [this Wikipedia page](https://en.wikipedia.org/wiki/Decision_tree_learning) or reference \\#1 (see our **References** section).\n",
        "\n",
        "2. A **Random Forest** is just an ensemble of many decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiuauOI1gthF"
      },
      "source": [
        "## **Train** or **Fit** the Models\n",
        "\n",
        "_**Run the entire section.** As you conduct <font color=red>**TASK XXX**</font>, you may want to expand this and change the features used during training to see how this affects your model._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b384uuARyyxg",
        "cellView": "form"
      },
      "source": [
        "#@title Let's train our ML models.\n",
        "\n",
        "#@markdown Select **which features** to include in the training by ticking the box:\n",
        "#@markdown *(Start with all the features and return here later)*\n",
        "age = True #@param {type:\"boolean\"}\n",
        "#age_cat = True #@param {type:\"boolean\"}\n",
        "c_charge_degree = True #@param {type:\"boolean\"}\n",
        "race = True #@param {type:\"boolean\"}\n",
        "sex = True #@param {type:\"boolean\"}\n",
        "priors_count = True #@param {type:\"boolean\"}\n",
        "\n",
        "# mapping between the variables and the names\n",
        "selection = [age, c_charge_degree, race, sex, priors_count]\n",
        "\n",
        "# all the names of features we could include \n",
        "all_features = [\"age\", \"c_charge_degree\", \"race\", \"sex\", \"priors_count\"]\n",
        "\n",
        "target = \"two_year_recid\"\n",
        "included_features = []  # list of names for all included features\n",
        "\n",
        "X_train = None\n",
        "#X_test = None\n",
        "\n",
        "print(\"Variables included in the model and any transformations done to them:\")\n",
        "for i in range(len(all_features)):\n",
        "  if( selection[i] ):\n",
        "      # feature was elected\n",
        "      col = all_features[i]\n",
        "      included_features += [col]\n",
        "      if (train_set[col].dtype==int or train_set[col].dtype==float):\n",
        "        # numeric variable\n",
        "        X_train = pd.concat( [X_train, train_set[col]], axis=1 )\n",
        "        #X_test = pd.concat( [X_test, test_set[col]], axis=1 )\n",
        "        print(\"{:20s} : including as is (numeric)\".format(col))\n",
        "      else:\n",
        "        # categorical variable\n",
        "        X_train = pd.concat( [X_train, pd.get_dummies(train_set[col], prefix=col)], axis=1 )\n",
        "        #X_test = pd.concat( [X_test, pd.get_dummies(test_set[col], prefix=col)], axis=1 )\n",
        "        #print(\"{:20s} : turning into dummies and removing last level '{}'\".format(col,X_train.columns[-1][len(col)+1:]))\n",
        "        print(\"{:20s} : turning into dummies and removing last level '{}'\".format(col,X_train.columns[-1]))\n",
        "        X_train.drop(columns=X_train.columns[-1], inplace=True)\n",
        "        #X_test.drop(columns=X_test.columns[-1], inplace=True)\n",
        "\n",
        "# list of names for all included features + target\n",
        "included_features_with_target = included_features + [target]\n",
        "\n",
        "# the target variables\n",
        "y_train = train_set[target]\n",
        "#y_test = test_set[target]\n",
        "\n",
        "# have a look at the training data\n",
        "X_train.head()\n",
        "\n",
        "# initialize a dictionary to store all the models and their information\n",
        "all_models = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO0vJVnQyyxn"
      },
      "source": [
        "### Model #1: A **Decision Tree** (Classification Tree)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ncQFPHyyyxn",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@title Train and Visualize a **Decision Tree Classifier**\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "#@markdown **Training**: select or type the **maximum depth** depth your tree is allowed to have (**`None`** will allow any depth):\n",
        "max_depth_to_train = \"3\" #@param [1, 2, 3, 4, 5, 6, \"None\"] {allow-input: true}\n",
        "\n",
        "if max_depth_to_train == \"None\":\n",
        "  max_depth_to_train = None\n",
        "else:\n",
        "  max_depth_to_train = np.int(max_depth_to_train)\n",
        "\n",
        "tree_model = DecisionTreeClassifier(criterion='gini', max_depth=max_depth_to_train, \\\n",
        "                                    random_state=123)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# set up a small dictionary with the new model\n",
        "all_models[\"tree\"] = {}\n",
        "all_models[\"tree\"][\"model\"] = tree_model\n",
        "\n",
        "#@markdown **Visualization**: select or type the depth you want to see (**`None`** will try printing the full tree):\n",
        "max_depth_to_see = \"3\" #@param [1, 2, 3, 4, 5, 6, \"None\"] {allow-input: true}\n",
        "\n",
        "if max_depth_to_see == \"None\":\n",
        "  max_depth_to_see = None\n",
        "else:\n",
        "  max_depth_to_see = np.int(max_depth_to_see)\n",
        "\n",
        "dot_data = tree.export_graphviz(tree_model, max_depth=max_depth_to_see, \\\n",
        "                                feature_names=X_train.columns, class_names=None,\n",
        "                                label='all', filled=True, impurity=True, node_ids=True, \\\n",
        "                                rounded=True, proportion=False)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc2idoVJyyxq"
      },
      "source": [
        "When interpreting the tree above, note that two colors are used, **orange** and **blue**, which come in different shades. The two colors correspond to the two possible target values: **orange** corresponds to 0, and **blue** corresponds to 1, so the color used at each node basically tells you the label for the majority of the samples in that node: orange nodes have more zeros, blue nodes have more ones. The **shade** of the color tells you how dominant the label is: the more orange (respectively, blue) a node becomes, the larger the fraction of zeros (respectively, ones). You can also read this kind of information from **value**; there are two values reported there: the first corresponds to how many samples have a label 0, and the second corresponds to how many samples have a value of 1 (and the sum of these two should equal **samples**).\n",
        "\n",
        "**Predictions** for a new sample are done by following all the conditions based on the features until a final leaf is reached. With classification problems, it is common to predict the **probability** of the target taking a value of 1, which is done by taking the **average** of the target variable for all the training samples in that leaf node. One can also obtain a prediction of the actual target **label** by taking **a majority vote** in the leaf node (which is the same as comparing the predicted probability with the threshold of 0.5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A4qX8C2FUGE"
      },
      "source": [
        "### Model #2: A **Random Forest**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6phTnolUFUGH",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@title Train a **Random Forest** with 50 Trees\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "\n",
        "#@markdown Select the number of trees (estimators) to use:\n",
        "n_estimators = 20 #@param [5, 10, 20, 30, 40, 50]\n",
        "\n",
        "#@markdown Select or type the depth you want to use for all the trees (selecting **`None`** will allow any depth):\n",
        "max_depth_to_train = \"5\" #@param [1, 2, 3, 4, 5, 6, \"None\"] {allow-input: true}\n",
        "\n",
        "if max_depth_to_train == \"None\":\n",
        "  max_depth_to_train = None\n",
        "else:\n",
        "  max_depth_to_train = np.int(max_depth_to_train)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth_to_train,\n",
        "                                  criterion='gini', random_state=42, oob_score=True)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# add the model to the list of models\n",
        "all_models[\"forest\"] = {}\n",
        "all_models[\"forest\"][\"model\"] = rf_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_TOSdGyyxq"
      },
      "source": [
        "________\n",
        "# **4. Evaluate the Predictive Accuracy**\n",
        "\n",
        "_**Run this entire section.** Since the main point of our exercise is to discuss issues of bias and discrimination, we will only evaluate performance in the training set and will skip evaluating in the test set for simplicity (but please remember that this step is critically important in practice!)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iVMeQowajee",
        "cellView": "form"
      },
      "source": [
        "#@markdown Define some useful functions to calculate predictive performance\n",
        "\n",
        "# #@markdown Calculate and display useful performance metrics\n",
        "def calculate_metrics_classification(y_true, y_pred, y_probas_pred, \\\n",
        "                                     positive_label_idx=1, \\\n",
        "                                     label_dataset=None, display=False, \\\n",
        "                                     store_other=True):\n",
        "        \n",
        "      # Classification task\n",
        "      labels = [None, None]\n",
        "      labels[positive_label_idx] = 1.0\n",
        "      labels[1-positive_label_idx] = 0.0\n",
        "      conf_mat = metrics.confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "      # in SciKit learn, the rows are the true label, the columns are the predicted label   \n",
        "      TP = conf_mat[positive_label_idx][positive_label_idx]\n",
        "      TN = conf_mat[1-positive_label_idx][1-positive_label_idx]\n",
        "      FP = conf_mat[1-positive_label_idx][positive_label_idx]\n",
        "      FN = conf_mat[positive_label_idx][1-positive_label_idx]\n",
        "\n",
        "      TPR = TP/(TP+FN) if (TP+FN>0) else \"n/d\"\n",
        "      TNR = TN/(TN+FP) if (TN+FP>0) else \"n/d\"\n",
        "      FPR = 1-TNR if TPR!=\"n/d\" else \"n/d\"\n",
        "      FNR = 1-TPR if TNR!=\"n/d\" else \"n/d\"\n",
        "      accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "      precision = metrics.precision_score(y_true, y_pred)\n",
        "      recall = metrics.recall_score(y_true, y_pred)\n",
        "      sensitivity = TPR\n",
        "      specificity = TNR\n",
        "      roc_auc = metrics.roc_auc_score(y_true, y_probas_pred) if len(np.unique(y_true))>1 else \"n/d\"\n",
        "\n",
        "      # the names of all the performance metrics\n",
        "      names = [\"True Positives (TP)\", \"True Negatives (TN)\", \\\n",
        "              \"False Positives (FP)\", \"False Negatives (FN)\", \\\n",
        "              \"True Positive Rate TPR=TP/(TP+FN)\", \\\n",
        "              \"True Negative Rate TNR=TN/(TN+FP)\", \\\n",
        "              \"False Positive Rate FPR=FP/(TN+FP)=1-TNR\", \\\n",
        "              \"False Negative Rate FNR=FN/(TP+FN)=1-TPR\", \\\n",
        "              \"Accuracy =(TP+TN)/(TP+FP+TN+FN)\", \\\n",
        "              \"Area Under ROC Curve (AUC)\", \\\n",
        "              \"Precision = TP/(TP+FP)\", \"Recall = TPR\", \\\n",
        "              \"Sensitivity = TPR\", \"Specificity = TNR\"]\n",
        "\n",
        "      values = [TP, TN, FP, FN, TPR, TNR, FPR, FNR, accuracy, roc_auc, \\\n",
        "                precision, recall, sensitivity, specificity  ]\n",
        "\n",
        "      # also determine roc curve and precision_recall curve\n",
        "      roc_curve = metrics.roc_curve(y_true, y_probas_pred)\n",
        "      prec_recall_curve = metrics.precision_recall_curve(y_true, y_probas_pred)   \n",
        "\n",
        "      if display:\n",
        "        print(\"Results in the {} Set:\".format(label_dataset if label_dataset!=None else \"Data\"))\n",
        "        for i in range(len(names)):\n",
        "          print('{:25s} : {:.2f}'.format(names[i],values[i]))\n",
        "\n",
        "      # compile the results into a dictionary with metrics and plots\n",
        "      results = {}    \n",
        "      results[\"metrics_names\"] = names\n",
        "      results[\"metrics_values\"] = values\n",
        "\n",
        "      if store_other:\n",
        "        results[\"others\"] = {}\n",
        "        results[\"others\"][\"conf_mat\"] = conf_mat\n",
        "        results[\"others\"][\"roc_curve\"] = roc_curve\n",
        "        results[\"others\"][\"prec_recall_curve\"] = prec_recall_curve\n",
        "      return results\n",
        "\n",
        "#\n",
        "#\n",
        "#\n",
        "# #@markdown A function to predict with each ML model for all samples in a dataset\n",
        "def predict_all_methods_full_dataset(all_models, X_data, y_data, \\\n",
        "                                     dataset=\"Dataset\", store_df=False):\n",
        "\n",
        "      # we will store the results in a dictionary\n",
        "      results = {}      # predictions/results for each sample\n",
        "\n",
        "      # predict with each method\n",
        "      for model_name in all_models.keys():\n",
        "        model = all_models[model_name][\"model\"] # pick the model\n",
        "\n",
        "        y_hat = model.predict(X_data)  # predict for entire training set\n",
        "        pos_label_idx = list(model.classes_).index(1)  # locate positive class (i.e., label 1)\n",
        "        y_prob = model.predict_proba(X_data)[:,pos_label_idx]  # get probabilities of y=1\n",
        "\n",
        "        # calculate summary performance\n",
        "        summary = calculate_metrics_classification(y_data, y_hat, y_prob, \\\n",
        "                                                  pos_label_idx, \\\n",
        "                                                  label_dataset=dataset, display=False, \\\n",
        "                                                  store_other=False)\n",
        "\n",
        "        # a dictionary with the results\n",
        "        aux = {}\n",
        "        aux[\"y_hat\"] = y_hat\n",
        "        aux[\"pos_label_idx\"] = pos_label_idx\n",
        "        aux[\"y_prob\"] = y_prob\n",
        "        aux[\"summary\"] = summary\n",
        "\n",
        "        # store the results in the models\n",
        "        all_models[model_name][dataset] = aux\n",
        "\n",
        "        # update the overall results dictionary\n",
        "        results[model_name] = aux\n",
        "\n",
        "        # store the dataframe with summary results as well, if desired\n",
        "        if store_df :\n",
        "          results[model_name][\"summary_df\"] = \\\n",
        "          pd.DataFrame( data={model_name : summary[\"metrics_values\"]}, \\\n",
        "                       index=summary[\"metrics_names\"], columns=[model_name] )\n",
        "\n",
        "      # now aggregate results for all samples into a single dataframe, including the predictions and errors\n",
        "      results[\"all_samples_df\"] = None\n",
        "\n",
        "      for model_name in all_models.keys():\n",
        "        results[\"all_samples_df\"] = \\\n",
        "        pd.concat( [ results[\"all_samples_df\"], \\\n",
        "                    pd.DataFrame({'{}_probability'.format(model_name) : \\\n",
        "                                  results[model_name][\"y_prob\"], \\\n",
        "                                  '{}_prediction'.format(model_name) : \\\n",
        "                                  results[model_name][\"y_hat\"]}, \\\n",
        "                                 index=X_train.index)],\\\n",
        "                  axis=1)\n",
        "\n",
        "      # and similarly join the summary performance metrics into a single dataframe\n",
        "      df_index = results[model_name][\"summary\"][\"metrics_names\"]\n",
        "      df_col_names = []\n",
        "      df_values = {}\n",
        "      for model_name in all_models.keys():\n",
        "        vals = results[model_name][\"summary\"][\"metrics_values\"]\n",
        "        df_col_names += [model_name]\n",
        "        df_values[model_name] = vals\n",
        "      results[\"summary_df\"] = pd.DataFrame( data = df_values, index=df_index, columns=df_col_names )\n",
        "\n",
        "      return results    \n",
        "\n",
        "#\n",
        "#\n",
        "#\n",
        "# #@markdown A function to predict with a specific ML model using different thresholds for a feature\n",
        "def predict_different_thresholds(model_name, model, X_data, y_data, full_data_training, \\\n",
        "                                 feature,  y_thresh_dict, dataset=\"Dataset\", \\\n",
        "                                 save_df=True):\n",
        "\n",
        "    # recover all the feature values from the **original** data \n",
        "    # (we do this because there may be processing for training)\n",
        "    feature_values = full_data_training[feature]\n",
        "\n",
        "    if feature_values.dtype==int or feature_values.dtype==float:\n",
        "      print(\"Error. Predicting with different thresholds only works for categorical for now.\")\n",
        "      return {}\n",
        "\n",
        "    # get the unique values of the feature\n",
        "    unique_vals = feature_values.unique()\n",
        "\n",
        "    y_hat = model.predict(X_data)  # predict for entire training set\n",
        "    pos_label_idx = list(model.classes_).index(1)  # locate positive class (i.e., label 1)\n",
        "    y_prob = model.predict_proba(X_data)[:,pos_label_idx]  # get probabilities of y=1\n",
        "\n",
        "    # figure out the values of y_predicted (potentially using different thresholds)\n",
        "    if set(unique_vals).issubset(set(y_thresh_dict.keys())) :\n",
        "      # all the values are proper keys\n",
        "      for v in unique_vals:\n",
        "        filter = (feature_values==v)\n",
        "        y_hat[filter] = (y_prob[filter] >= y_thresh_dict[v])\n",
        "    else:\n",
        "      print(\"Specified keys for feature {} not sufficient\".feature)\n",
        "      return {}\n",
        "\n",
        "    # we will store the results in a dictionary\n",
        "    results = {}      # predictions/results for each sample\n",
        "\n",
        "    # calculate summary performance\n",
        "    summary = calculate_metrics_classification(y_true=y_data, \\\n",
        "                                               y_pred=y_hat, \\\n",
        "                                               y_probas_pred=y_prob, \\\n",
        "                                               positive_label_idx=pos_label_idx, \\\n",
        "                                               label_dataset=dataset, display=False, \\\n",
        "                                               store_other=False)\n",
        "\n",
        "    # a dictionary with the results\n",
        "    aux = {}\n",
        "    aux[\"y_hat\"] = y_hat\n",
        "    aux[\"pos_label_idx\"] = pos_label_idx\n",
        "    aux[\"y_prob\"] = y_prob\n",
        "    aux[\"summary\"] = summary\n",
        "    aux[\"y_thresh_dict\"] = y_thresh_dict\n",
        "\n",
        "    # update the overall results dictionary\n",
        "    results[model_name] = aux\n",
        "\n",
        "    # store the dataframe with summary results within the model, if desired\n",
        "    if save_df :\n",
        "      results[model_name][\"summary_df\"] = \\\n",
        "      pd.DataFrame( data={model_name : summary[\"metrics_values\"]}, \\\n",
        "                   index=summary[\"metrics_names\"], columns=[model_name] )\n",
        "\n",
        "    return results\n",
        "\n",
        "#\n",
        "#\n",
        "#\n",
        "# #@markdown A function to visualize a binary confusion matrix\n",
        "def our_plot_confusion_matrix(y_true, y_pred, class_names=None, \\\n",
        "                              true_on_row=True, positive_first=True, \\\n",
        "                              prefix_title=\"\", figsize=(8,6), ax=None):\n",
        "\n",
        "    # This is inspired by a similar visualization created by Google Research \n",
        "    # figure out if positive label should be first or not\n",
        "    num_labels = [1, 0] if positive_first else [0, 1]\n",
        "    labels = [\"Positives\", \"Negatives\"] if positive_first else [\"Negatives\", \"Positives\"]\n",
        "    positive_label_idx = 0 if positive_first else 1\n",
        "\n",
        "    strings = np.asarray([['True {}'.format(labels[0]), 'False {}'.format(labels[1])], \\\n",
        "                          ['False {}'.format(labels[0]), 'True {}'.format(labels[1])]])\n",
        "\n",
        "    # get the confusion matrix\n",
        "    # in SciKit, ROWS = true condition, COLUMNS = predicted condition\n",
        "    conf_mat = metrics.confusion_matrix(y_true, y_pred, labels=num_labels)\n",
        "\n",
        "    # calculate the rates\n",
        "    true_conditions = conf_mat.sum(axis=1)  # sum on a row, for the true conditions in each class\n",
        "    conf_mat_rates = np.array([ [conf_mat[0,0]/true_conditions[0] if true_conditions[0] > 0 else \"n/d\", \\\n",
        "                        conf_mat[0,1]/true_conditions[0] if true_conditions[0] > 0 else \"n/d\"], \\\n",
        "                       [conf_mat[1,0]/true_conditions[1] if true_conditions[1] > 0 else \"n/d\", \\\n",
        "                        conf_mat[1,1]/true_conditions[1] if true_conditions[1] > 0 else \"n/d\"] \n",
        "                      ])\n",
        "\n",
        "    # determine whether to switch the row/column based on the preference for displaying\n",
        "    if true_on_row:\n",
        "        # true condition on the row -- this is how it is now; just set up the x/y labels\n",
        "        ylabel='Ground truth'\n",
        "        xlabel='Prediction'\n",
        "    else:\n",
        "        # we need the true condition on the column, and the predicted on the row\n",
        "        for var_ in [strings, conf_mat, conf_mat_rates]:\n",
        "          var_[0,1], var_[1,0] = var_[1,0], var_[0,1]\n",
        "        xlabel='Ground truth'\n",
        "        ylabel='Prediction'\n",
        "\n",
        "    # set up the names to use for each class when printing\n",
        "    class_names = [ class_names[i] for i in num_labels ] if class_names!=None else\\\n",
        "    [str(i) for i in num_labels]\n",
        "\n",
        "    df_cm = pd.DataFrame(\n",
        "        conf_mat, index=class_names, columns=class_names, \n",
        "    )\n",
        "\n",
        "    #sns.set_context(\"notebook\", font_scale=1.25)\n",
        "    ax = ax or plt.gca()\n",
        "    ax.set_title(prefix_title + 'Confusion Matrix ')\n",
        "\n",
        "    # Combine the numerical value with its description\n",
        "    labels = (np.asarray(\n",
        "        [\"{0:g}\\n{1}\\n(Rate: {2:.2f}%)\".format(value, string, rate*100) \\\n",
        "         for value, string, rate in zip(\n",
        "             conf_mat.flatten(), strings.flatten(), conf_mat_rates.flatten())\\\n",
        "         ])).reshape(2, 2)\n",
        "\n",
        "    heatmap = sns.heatmap(df_cm, annot=labels, fmt=\"\", \n",
        "        linewidths=2.0, cmap=sns.color_palette(\"Blues\"), ax=ax, \\\n",
        "        annot_kws={\"fontsize\":12});\n",
        "    heatmap.yaxis.set_ticklabels(\n",
        "        heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "    heatmap.xaxis.set_ticklabels(\n",
        "        heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    return fig\n",
        "\n",
        "#\n",
        "#\n",
        "#\n",
        "# #@markdown Function to evaluate predictive performance depending on a feature \n",
        "def evaluate_performance_with_feature_values(model_name, model, feature, X_data, \\\n",
        "                                             y_data, full_data_training, \\\n",
        "                                             results_full_data,\\\n",
        "                                             num_bins=3, \\\n",
        "                                             prefix_title=\"\", class_names=None,\\\n",
        "                                             display_conf_mat=True,\\\n",
        "                                             display_roc_pr=True, plot_together=False,\\\n",
        "                                             ax_roc=None, ax_pr=None, figsize=(7,7)):\n",
        "\n",
        "  # select the right model to use and the right column to use for summary results\n",
        "  y_hat = results_full_data[\"y_hat\"]\n",
        "  y_prob = results_full_data[\"y_prob\"]\n",
        "  pos_index = results_full_data[\"pos_label_idx\"]\n",
        "\n",
        "  # recover all the feature values from the **original** data \n",
        "  # (we do this because there may be processing for training)\n",
        "  feature_values = full_data_training[feature]\n",
        "\n",
        "  if feature_values.dtype==int or feature_values.dtype==float:\n",
        "    # if numeric feature - handle separately by discretizing\n",
        "    print(\"{} is a numeric feature. Will discretize into {} bins.\".format(feature,num_bins))\n",
        "    feature_values, bins =  pd.cut(feature_values, bins=num_bins, retbins=True,\\\n",
        "                                   include_lowest=True, ordered=True)\n",
        "\n",
        "  # get the unique values of the feature\n",
        "  unique_vals = feature_values.unique()\n",
        "\n",
        "  # get the summary performance for the model in the full data \n",
        "  results = results_full_data[\"summary_df\"].copy()\n",
        "  results.rename(columns={model_name:\"Entire Dataset\"})\n",
        "      \n",
        "  # a few plotting settings\n",
        "  fontsize = 12  # font size for titles in plots\n",
        "\n",
        "  if display_conf_mat :\n",
        "    # display the confusion matrices\n",
        "    num_figs = 1+len(unique_vals)   # calculate how many\n",
        "\n",
        "    fig_cm = plt.figure(figsize=(figsize[0]*num_figs,figsize[1]))  # create a figure of the desired size\n",
        "    \n",
        "    # get all the axes for subplots\n",
        "    axs_cm = fig_cm.subplots(1, num_figs)\n",
        "\n",
        "    # plot the confusion matrix for entire data\n",
        "    aux = our_plot_confusion_matrix(y_true=y_data, y_pred=y_hat,\\\n",
        "                                    class_names=class_names, \\\n",
        "                                    true_on_row=True, positive_first=True, \\\n",
        "                                    prefix_title=\"\", \\\n",
        "                                    figsize=(fig_width,fig_heigth), ax=axs_cm[0])\n",
        "    axs_cm[0].set_title(prefix_title + \"Confusion Matrix with Entire Dataset\",\\\n",
        "                        fontsize=fontsize)\n",
        "\n",
        "\n",
        "  if display_roc_pr:\n",
        "    # if we want to display the ROC / prec-recall curves\n",
        "\n",
        "    # cmap = mpl.cm.get_cmap(\"Blues\")\n",
        "    cmap = mpl.cm.get_cmap(\"jet\")\n",
        "\n",
        "    if (plot_together==False) or (ax_roc==None) or (ax_pr==None):\n",
        "        # cannot plot together\n",
        "        plot_together = False\n",
        "        roc_pr_fig = plt.figure(figsize=(figsize[0]*2,figsize[1]))  # create a figure of the desired size\n",
        "        \n",
        "        ax_roc, ax_pr = roc_pr_fig.subplots(1,2)\n",
        "\n",
        "        # settings for the ROC curve\n",
        "        # set the colors into different shades of blue\n",
        "        ax_roc.set_prop_cycle('color',[cmap(i) for i in np.linspace(0, 1, 1+len(unique_vals))])\n",
        "    \n",
        "        # settings for the precision-recall curve\n",
        "        # set the colors into different shades of blue\n",
        "        ax_pr.set_prop_cycle('color',[cmap(i) for i in np.linspace(0, 1, 1+len(unique_vals))])\n",
        "\n",
        "        # will not use any prefix for the labels in the legend\n",
        "        label_prefix = \"\"\n",
        "        linestyle = \"-\"\n",
        "    else:\n",
        "        # we will be plotting the ROC/prec-recall on top of existing plots\n",
        "        label_prefix = prefix_title   # use prefix in the legend\n",
        "        prefix_title = \"\"     # no prefix in the title\n",
        "        linestyle = \"--\"\n",
        "    \n",
        "    metrics.plot_roc_curve(model, X_data, y_data, \\\n",
        "                           sample_weight=None, response_method='auto', \\\n",
        "                           name='{}Entire data'.format(label_prefix), ax=ax_roc,\\\n",
        "                           linestyle=linestyle)\n",
        "    ax_roc.set_title(\"{}Comparison of ROC Curves for '{}'\".format(prefix_title,feature),\\\n",
        "                     fontsize=fontsize)\n",
        "\n",
        "    # plot precision-recall curve with full dataset\n",
        "    metrics.plot_precision_recall_curve(model, X_data, y_data, \\\n",
        "                                        sample_weight=None, response_method='auto', \\\n",
        "                                        name='{}Entire data'.format(label_prefix), ax=ax_pr, \\\n",
        "                                        linestyle=linestyle)\n",
        "    ax_pr.set_title(\"{}Comparison of Precision-Recall Curves for '{}'\".\\\n",
        "                    format(prefix_title,feature),fontsize=fontsize)\n",
        "  else:\n",
        "    ax_roc = None\n",
        "    ax_pr = None\n",
        "\n",
        "  # go through each individual value for the feature and calculate metrics + do plots\n",
        "  for i in range(len(unique_vals)):\n",
        "    v = unique_vals[i]\n",
        "    filter = (feature_values==v)  # filter for all samples with value v\n",
        "    if (len(y_data[filter])):\n",
        "      results_v = calculate_metrics_classification(y_true=y_data[filter], \\\n",
        "                                                   y_pred=y_hat[filter], \\\n",
        "                                                   y_probas_pred=y_prob[filter], \\\n",
        "                                                   positive_label_idx=pos_index, \\\n",
        "                                                   display=False)\n",
        "            \n",
        "      # copy the column with aggregate results on the entire dataset\n",
        "      col_v_name = \"{}={}\".format(feature,v)\n",
        "      aux = pd.DataFrame( data=results_v[\"metrics_values\"], \\\n",
        "                         index=results_full_data[\"summary_df\"].index, columns=[col_v_name]  )\n",
        "      results = results.merge(right=aux, left_index=True, right_index=True)\n",
        "\n",
        "      # if needed, visualize the confusion matrix\n",
        "      if display_conf_mat:\n",
        "          # plot the confusion matrix with the correct feature\n",
        "          aux = our_plot_confusion_matrix(y_true=y_data[filter], \\\n",
        "                                          y_pred=y_hat[filter],\\\n",
        "                                          class_names=class_names, \\\n",
        "                                          true_on_row=True, positive_first=True, \\\n",
        "                                          prefix_title=\"\", \\\n",
        "                                          figsize=(fig_width,fig_heigth), ax=axs_cm[1+i])\n",
        "          axs_cm[1+i].set_title(prefix_title + col_v_name,fontsize=fontsize)\n",
        "\n",
        "      if display_roc_pr:\n",
        "        # plot ROC curve with partial data\n",
        "        label = \"{}{}\".format(label_prefix,col_v_name)\n",
        "        metrics.plot_roc_curve(model, X_data[filter], y_data[filter], \\\n",
        "                              sample_weight=None, response_method='auto', \\\n",
        "                              name=label, ax=ax_roc,  linestyle=linestyle)\n",
        "\n",
        "        # plot precision-recall curve with partial data\n",
        "        metrics.plot_precision_recall_curve(model, X_data[filter], y_data[filter], \\\n",
        "                                            sample_weight=None, response_method='auto', \\\n",
        "                                            name=label, ax=ax_pr, linestyle=linestyle)\n",
        "    else:\n",
        "      print('Feature {} has no values equal to {}. Will omit that category.'.format(feature,v))\n",
        "\n",
        "  return results, ax_roc, ax_pr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiuWktTa8CUv",
        "cellView": "form"
      },
      "source": [
        "#@title **Let's predict in the entire <font color=red>training data</font> and display a few samples**\n",
        "all_results = predict_all_methods_full_dataset(all_models, X_train, y_train, \\\n",
        "                                              dataset=\"train\", store_df=True)\n",
        "\n",
        "# join the results for all samples into a single dataframe, including the training data, predictions and errors\n",
        "all_results[\"all_samples_and_data_df\"] = pd.concat( [ train_copy[included_features_with_target].copy(), \\\n",
        "                                                 all_results[\"all_samples_df\"] ], axis=1)\n",
        "\n",
        "#@markdown <br>Display a few samples to compare the methods: select or type how many samples to display:\n",
        "num_samp = \"9\" #@param [5, 10, 20] {allow-input:true}\n",
        "\n",
        "#@markdown <br>Interpretation of what you are seeing:\n",
        "#@markdown - **probability** : the predicted probability that the target is 1\n",
        "#@markdown - **prediction** : the predicted target, based on thresholding the probability at level 0.5\n",
        "all_results[\"all_samples_and_data_df\"].head(np.int(num_samp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHTfz8ZrsjPF"
      },
      "source": [
        "### How to Summarize Performance for a Binary Classifier?\n",
        "*If you are not that familiar with classification tasks or hypothesis testing, you may want to briefly expand this section for a description of the **many ways** used to quantify theirperformance, and a brief discussion of the (potentially confusing) **terminology**.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9yo4A_KRH-S"
      },
      "source": [
        "The next step is to determine the aggregate performance of our ML model in the entire training data. Since **binary classification tasks** like the one here appear in many different application domains (e.g., healthcare, military, engineering, etc.), different fields have come up with slightly different performance metrics and even different terminology for the same metric, so things can get quite confusing. In a nutshell, we can evaluate performance either by looking at the **predicted labels** or by looking at the **predicted probabilities**. To make sure everyone is on the same page, we will try to include a brief description below for both, which should be more than sufficient for our purposes here. \n",
        "\n",
        "<br><font color=black>**Evaluating performance based on predicted labels.**</font><br>\n",
        "Since there are two possible values for the ground truth ($y=0$ or $y=1$) and our ML model can predict two possible labels ($\\hat{y}=0$ or $\\hat{y}=1$), four cases can arise. To refer to these four cases quickly, statisticians have come up with a simple \"encoding\": \n",
        "> - use **true** for cases where the prediction matches the ground truth, and **false** for cases where the prediction is wrong;\n",
        "> - use **positive** for cases where the predicted label is $\\hat{y}=1$, and **negative** for cases where the predicted label is $\\hat{y}=0$.\n",
        "\n",
        "This gives rise to the following four cases and associated terminology:\n",
        "\n",
        "> * <font color=green>**True Positive (TP)**</font>: the model predicts a positive label ($\\hat{y}=1$), and that matches the ground truth ($y=1$).\n",
        "> * <font color=green>**True Negative (TN)**</font>: the model predicts a negative label ($\\hat{y}=0$), and that matches the ground truth ($y=0$).\n",
        "> * <font color=red>**False Positive (FP)**</font>: the model predicts a positive label ($\\hat{y}=1$), but in reality the label is negative ($y=0$).\n",
        "> * <font color=red>**False Negative (FN)**</font>: the model predicts a negative label ($\\hat{y}=0$), but in reality the label is positive ($y=1$).\n",
        "\n",
        "It is common to list these four values in a (hopefully not too confusing!) 2x2 matrix called a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix), which is a very concise way to summarize the performance of a binary classifier. Additionally, these can be used as a starting point to calculate various other performance metrics, as follows:\n",
        "- **True Positive Rate (TPR)**: the fraction of positive labels correctly identified as positive **TP/(TP+FN)**\n",
        "- **True Negative Rate (TNR)**: the fraction of negative labels correctly identified as negative, **TN/(TN+FP)** \n",
        "- **False Positive Rate (FPR)**: the fraction of negative labels incorrectly identified as positive, **FP/(FP+TN) = 1 - TNR**\n",
        "- **False Negative Rate (FNR)**: the fraction of positive labels incorrectly identified as negative, **FN/(FN+TP) = 1 - TPR**\n",
        "- **Accuracy**: the fraction of labels correctly identified, **(TP+TN)/(TP+TN+FP+TN)**\n",
        "- [**Sensitivity**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity): another term for **TPR**\n",
        "- [**Specificity**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity): another term for **TNR**\n",
        "- [**Precision**](https://en.wikipedia.org/wiki/Precision_and_recall): the fraction of labels identified as positive that are actually positive, **TP/(TP+FP)**.\n",
        "- [**Recall**](https://en.wikipedia.org/wiki/Precision_and_recall): yet another term for **TPR**.\n",
        "\n",
        "In addition to these, there are several terms used for the same metrics, and also others metrics that can be computed (see some of the links above for examples).\n",
        "\n",
        "> **Quick historical note.** *Positive* and *negative* should not be interpreted in any way as value judgments! They are standard terms that came about because of the historical context and the motivating applications: predictions were often made as part of some kind of **test** (for instance, to check if a disease is present or whether an enemy submarine is nearby), and  a **positive  outcome in the test** amounted to conditions that were statistically surprising and needed follow-up action.\n",
        "\n",
        "<br>**Evaluating performance based on predicted probabilities.**<br>\n",
        "The predicted labels are actually defined by thresholding the predicted probabilities at a value of 0.5. However, it is natural to ask how the predictions and errors would change if this threshold took a different value, as this would present a much better picture of the overall performance of the binary classifier, without overly relying on the (potentially ad-hoc) thresholding at 0.5. This is commonly done by varying the threshold from 0 to 1 and plotting two of the metrics above as a function of the threshold.  This gives rise to two well known plots:\n",
        "\n",
        "- A [receiver operating characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve is created by plotting the true positive rate (**TPR**) against the false positive rate (**FPR**) at various threshold settings. The curves in these kinds of plots always start at (0,0), end at (1,1), and are monotonically increasing. If the classified is \"good\", we would hope to see curves that quickly go up to the value 1. Therefore, a good normalized measure of the overall performance of the classifier is given by the **area under the ROC curve (AUC)**: a perfect classifier would have a value 1, and values closer to 1 indicate better classifiers.\n",
        "- A [precision-recall (PCR)](https://en.wikipedia.org/wiki/Precision_and_recall) curve plots the **precision** against the **recall**. These plots are not necessarily monotonic, but we would hope to see points that are as close to (1,1) as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGLZcIFptDqc"
      },
      "source": [
        "### Summarize Performance in Entire Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNngFuQdyyxv",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@title **Let's evaluate several <font color=red>aggregate performance metrics</font> in the entire training set**\n",
        "\n",
        "#@markdown Select which performance metrics you want to see displayed. If you are not sure what these are, expand the previous section and read the brief description.\n",
        "display_all_performance_numbers = True #@param {type:\"boolean\"}\n",
        "display_confusion_matrix = True #@param {type:\"boolean\"}\n",
        "display_ROC_and_precision_recall_curves = False #@param {type:\"boolean\"}\n",
        "\n",
        "if display_all_performance_numbers:\n",
        "    print(\"The aggregate performance in the training set:\")\n",
        "    display(all_results[\"summary_df\"])\n",
        "\n",
        "#@markdown <br>**You can further customize some aspects of the plot**\n",
        "#@markdown - **`fig_width`** : the width of each figure in inches (leave as **Auto** if unsure)\n",
        "#@markdown - **`fig_height`** : the height of each figure in inches (leave as **Auto** if unsure)\n",
        "\n",
        "fig_width = 'Auto' #@param ['Auto']{allow-input : true}\n",
        "fig_height = 'Auto' #@param ['Auto']{allow-input : true}\n",
        "if fig_width=='Auto':\n",
        "  fig_width=8.0\n",
        "else:\n",
        "  fig_width=np.float(fig_width)\n",
        "\n",
        "if fig_height=='Auto':\n",
        "  fig_height=6.0\n",
        "else:\n",
        "  fig_height=np.float(fig_height)\n",
        "\n",
        "all_model_names = list(all_models.keys())\n",
        "num_models = len(all_model_names)\n",
        "\n",
        "# confusion matrix\n",
        "if display_confusion_matrix:\n",
        "    # separate plots for each method\n",
        "    fig_width = 8.0\n",
        "    fig_heigth = 6.0\n",
        "    fig = plt.figure(figsize=(fig_width*num_models,fig_heigth))  # create a figure of the desired size\n",
        "    for i in range(num_models):\n",
        "      model_name = all_model_names[i]\n",
        "      plt.subplot(1,num_models,1+i)\n",
        "      ax=plt.gca()\n",
        "      aux = our_plot_confusion_matrix(y_true=y_train, y_pred=all_results[model_name][\"y_hat\"],\\\n",
        "                                      class_names=None, \\\n",
        "                                      true_on_row=True, positive_first=True, \\\n",
        "                                      prefix_title=\"\", \\\n",
        "                                      figsize=(fig_width,fig_heigth), ax=ax)\n",
        "      ax.set_title(\"Confusion Matrix with {}\".format(model_name),fontsize=14)\n",
        "\n",
        "# ROC and precision-recall \n",
        "if display_ROC_and_precision_recall_curves :\n",
        "\n",
        "    # create a single figure: left for ROC, right for PRC (if both plots needed)\n",
        "    roc_pr_fig = plt.figure(figsize=(fig_width*2,fig_height))  # create a figure of the desired size\n",
        "    ax_roc, ax_pr = roc_pr_fig.subplots(1,2)\n",
        "\n",
        "    # set the colors based on jet map\n",
        "    cmap = mpl.cm.get_cmap(\"jet\")\n",
        "    ax_roc.set_prop_cycle('color',[cmap(i) for i in np.linspace(0, 1, num_models)])\n",
        "    ax_pr.set_prop_cycle('color',[cmap(i) for i in np.linspace(0, 1, num_models)])\n",
        "\n",
        "    ax_roc.set_title(\"ROC Curves with Each Model\",fontsize=14)\n",
        "    for i in range(num_models):\n",
        "      model_name = all_model_names[i]\n",
        "      metrics.plot_roc_curve(all_models[model_name][\"model\"], X_train, y_train, \\\n",
        "                            sample_weight=None, response_method='auto', \\\n",
        "                            name=model_name, ax=ax_roc)\n",
        "\n",
        "    ax_pr.set_title(\"Precision-Recall Curves with Each Model\",fontsize=14)\n",
        "    for i in range(num_models):\n",
        "      model_name = all_model_names[i]\n",
        "      metrics.plot_precision_recall_curve(all_models[model_name][\"model\"], X_train, y_train, \\\n",
        "                                          sample_weight=None, response_method='auto', \\\n",
        "                                          name=all_model_names[i], ax=ax_pr)\n",
        "\n",
        "    #plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojryixiZyyxz"
      },
      "source": [
        "# **5. Evaluating Bias**\n",
        "The next step is to understand whether the model is able to predict \"uniformly well\". This is the main task that we are interested in here: if our classifier  delivers substantially different performance for different categories (and particularly for protected characteristics), then that provides evidence that it is biased/unfair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biJfTX_nRa6O",
        "cellView": "form"
      },
      "source": [
        "#@title **Determine whether predictive performance depends on the features**\n",
        "\n",
        "#@markdown Select **one of the models** we trained:\n",
        "model_name = \"tree\" #@param [\"tree\", \"forest\"]\n",
        "\n",
        "#@markdown Select a **feature** to examine:\n",
        "feature = \"race\" #@param ['age', 'c_charge_degree', 'race', 'sex', 'priors_count']\n",
        "\n",
        "#@markdown (Numerical features will be discretized into bins. You can select how many bins to use below:)\n",
        "num_bins = \"3\" #@param [2,3,4,5]{allow-input:true}\n",
        "num_bins = np.int(num_bins)\n",
        "\n",
        "print(\"Your selection:\")\n",
        "print(\"Model:        {}\".format(model_name))\n",
        "print(\"Feature(s):   {}\".format(feature))\n",
        "\n",
        "#@markdown <br>**Feel free to customize what information is displayed:**\n",
        "display_all_performance_numbers = True #@param {type:\"boolean\"}\n",
        "display_confusion_matrix = True #@param {type:\"boolean\"}\n",
        "display_ROC_and_precision_recall_curves = False #@param {type:\"boolean\"}\n",
        "\n",
        "# if the dictionary to store results by feature for that model does not exist, create it\n",
        "if \"by_feature\" not in all_results[model_name].keys():\n",
        "  all_results[model_name][\"by_feature\"] = {} \n",
        "\n",
        "all_results[model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "evaluate_performance_with_feature_values(model_name=model_name, \\\n",
        "                                         model=all_models[model_name][\"model\"],\\\n",
        "                                         feature=feature, X_data=X_train, \\\n",
        "                                         y_data=y_train, \n",
        "                                         full_data_training=train_set, \\\n",
        "                                         results_full_data=all_results[model_name],\\\n",
        "                                         num_bins=num_bins, \\\n",
        "                                         prefix_title=\"\", class_names=None,\\\n",
        "                                         display_conf_mat=display_confusion_matrix,\\\n",
        "                                         display_roc_pr=display_ROC_and_precision_recall_curves, \\\n",
        "                                         plot_together=False,\\\n",
        "                                         ax_roc=None, ax_pr=None,\n",
        "                                         figsize=(8,6))\n",
        "\n",
        "if display_all_performance_numbers:\n",
        "    print(\"\\nDisplaying detailed performance metrics:\")\n",
        "    display(all_results[model_name][\"by_feature\"][feature])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2kNE0kbmd43"
      },
      "source": [
        "# **6. Potential Fixes to Reduce Bias**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34k4xVw1BtGk"
      },
      "source": [
        "## Change the Features Included in Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9H0LWL1mv6A"
      },
      "source": [
        "One of the first things to try is to change the features/data included when training the ML algorithm. For instance, we could try to eliminate the protected features from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQdER0NtBsFa",
        "cellView": "form"
      },
      "source": [
        "#@title **Select a new model and the features to use in training**\n",
        "\n",
        "#@markdown Pick a model and the **maximum depth** depth for trees:\n",
        "new_model_name = \"tree\" #@param [\"tree\", \"forest\"]\n",
        "max_depth_to_train = \"5\" #@param [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {allow-input: true}\n",
        "max_depth_to_train = np.int(max_depth_to_train)\n",
        "\n",
        "#@markdown Pick **which features** to include in the training by ticking the box:\n",
        "age = True #@param {type:\"boolean\"}\n",
        "#age_cat = True #@param {type:\"boolean\"}\n",
        "c_charge_degree = True #@param {type:\"boolean\"}\n",
        "race = False #@param {type:\"boolean\"}\n",
        "sex = False #@param {type:\"boolean\"}\n",
        "priors_count = True #@param {type:\"boolean\"}\n",
        "\n",
        "# mapping between the variables and the names\n",
        "var_names = [\"age\", \"c_charge_degree\", \"race\", \"sex\", \"priors_count\"]\n",
        "#var_names = [\"age_cat\", \"c_charge_degree\", \"race\", \"sex\", \"priors_count\"]\n",
        "selection = [age, c_charge_degree, race, sex, priors_count]\n",
        "\n",
        "new_X_train = None\n",
        "\n",
        "if new_model_name==\"tree\":\n",
        "  new_model = sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=max_depth_to_train, \\\n",
        "                                    random_state=123)\n",
        "elif new_model_name==\"forest\":\n",
        "  new_model = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth_to_train,\n",
        "                                  criterion='gini', random_state=42, oob_score=True)\n",
        "\n",
        "print(\"Your selection of model and variables to include:\")\n",
        "print(\"Model:        {}\".format(new_model_name))\n",
        "print(\"Variables:\")\n",
        "\n",
        "for i in range(len(var_names)):\n",
        "\n",
        "  if selection[i] == True:\n",
        "    col = var_names[i]\n",
        "    if (train_set[col].dtype==int or train_set[col].dtype==float):\n",
        "      # numeric variable\n",
        "      new_X_train = pd.concat( [new_X_train, train_set[col]], axis=1 )\n",
        "      print(\"{:20s} : including as is (numeric)\".format(col))\n",
        "    else:\n",
        "      # categorical variable\n",
        "      new_X_train = pd.concat( [new_X_train, pd.get_dummies(train_set[col], prefix=col)], axis=1 )\n",
        "      print(\"{:20s} : turning into dummies and removing last level '{}'\".format(col,new_X_train.columns[-1]))\n",
        "      new_X_train.drop(columns=new_X_train.columns[-1], inplace=True)\n",
        "\n",
        "# the target variables\n",
        "new_y_train = train_set[\"two_year_recid\"]\n",
        "\n",
        "# train and evaluate the model\n",
        "#new_models, new_results = train_and_evaluate(new_model, new_model_name, new_X_train, new_y_train)\n",
        "\n",
        "# train the model\n",
        "new_model.fit(new_X_train, new_y_train)\n",
        "\n",
        "# set up a dictionary with the new model\n",
        "new_models = {}\n",
        "new_models[new_model_name] = {}\n",
        "new_models[new_model_name][\"model\"] = new_model\n",
        "\n",
        "# predict with the new model in the entire training data\n",
        "new_results = predict_all_methods_full_dataset(new_models, new_X_train, new_y_train, \\\n",
        "                                              dataset=\"train\", store_df=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdojkvJvazLD",
        "cellView": "form"
      },
      "source": [
        "#@title **Examine the bias in the new model**\n",
        "\n",
        "#@markdown For the **new model**, select a specific **feature** to examine for bias. (Numerical features will be discretized; select how many bins to use below:)\n",
        "feature = \"race\" #@param ['age', 'c_charge_degree', 'race', 'sex', 'priors_count']\n",
        "num_bins = \"3\" #@param [2,3,4,5]{allow-input:true}\n",
        "num_bins = np.int(num_bins)\n",
        "\n",
        "#@markdown <br>Customize what information is displayed:\n",
        "display_all_performance_numbers = False #@param {type:\"boolean\"}\n",
        "display_confusion_matrix = True #@param {type:\"boolean\"}\n",
        "display_precision_recall = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <br>Compare the **new model** with the **old model** for the specific feature? (Picking **\"No\"** will display performance for the new model only)\n",
        "compare = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <br>Customize your plot:<br>\n",
        "figure_width = \"6.5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=7.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=7.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "print(\"Your selection:\")\n",
        "print(\"Feature(s):          {}\".format(feature))\n",
        "print(\"Compare with old:    {}\".format(compare))\n",
        "\n",
        "if (\"by_feature\" not in new_results[new_model_name].keys() ):\n",
        "  new_results[new_model_name][\"by_feature\"] = {} # reset the dictionary for results by feature\n",
        "\n",
        "new_results[new_model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "evaluate_performance_with_feature_values(model_name=new_model_name, \\\n",
        "                                        model=new_models[new_model_name][\"model\"], \\\n",
        "                                        feature=feature, X_data=new_X_train, \\\n",
        "                                        y_data=new_y_train, \\\n",
        "                                        full_data_training=train_set, \\\n",
        "                                        results_full_data=new_results[model_name],\\\n",
        "                                        num_bins=num_bins, \\\n",
        "                                        prefix_title=\"NEW: \", \\\n",
        "                                        class_names=None,\\\n",
        "                                        display_conf_mat=display_confusion_matrix,\\\n",
        "                                        display_roc_pr=display_precision_recall, \\\n",
        "                                        plot_together=compare, \\\n",
        "                                        figsize=(figure_width,figure_height))\n",
        "\n",
        "if compare:\n",
        "    # check to see if we have results from the \"old\" model for the same feature\n",
        "    if (\"by_feature\" not in all_results[new_model_name].keys() ):\n",
        "      all_results[new_model_name][\"by_feature\"] = {}\n",
        "\n",
        "    all_results[new_model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "    evaluate_performance_with_feature_values(model_name=new_model_name, \\\n",
        "                                        model=all_models[new_model_name][\"model\"], \\\n",
        "                                        feature=feature, X_data=X_train, \\\n",
        "                                        y_data=y_train, \\\n",
        "                                        full_data_training=train_set, \\\n",
        "                                        results_full_data=all_results[model_name],\\\n",
        "                                        num_bins=num_bins, \\\n",
        "                                        prefix_title=\"OLD: \", \\\n",
        "                                        class_names=None,\\\n",
        "                                        display_conf_mat=display_confusion_matrix,\\\n",
        "                                        display_roc_pr=display_precision_recall, \\\n",
        "                                        plot_together=compare, \\\n",
        "                                        figsize=(figure_width,figure_height))\n",
        "\n",
        "    results_to_display = pd.merge( left=new_results[new_model_name][\"by_feature\"][feature], \\\n",
        "                                  right=all_results[new_model_name][\"by_feature\"][feature], \\\n",
        "                                  left_index=True, right_index=True,\\\n",
        "                                  how=\"inner\", suffixes=(\"_NEW\", \"_OLD\") )\n",
        "\n",
        "else:\n",
        "    results_to_display = new_results[new_model_name][\"by_feature\"][feature]\n",
        "\n",
        "if display_all_performance_numbers:\n",
        "    display(results_to_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enz9e84Hb2yk"
      },
      "source": [
        "## Use Different Thresholds for Each Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTiuPGVIcJcY",
        "cellView": "form"
      },
      "source": [
        "#@title Allow the algorithm to use different thresholds for predicting\n",
        "\n",
        "#@markdown Select one of the models we trained:\n",
        "model_name = \"tree\" #@param [\"tree\", \"forest\"]\n",
        "\n",
        "feature=\"race\"\n",
        "\n",
        "#@markdown **Type a threshold in [0,1] to use when predicting recidivism.** The prediction will be 1 if and only if probabilities exceed the threshold.\n",
        "threshold_African_American = 0.65 #@param {type:\"number\"}\n",
        "threshold_Caucasian = 0.5 #@param {type:\"number\"}\n",
        "\n",
        "threshold_African_American = max(0,min(threshold_African_American,1))\n",
        "threshold_Caucasian = max(0,min(threshold_Caucasian,1))\n",
        "\n",
        "y_thresh_dict = {}\n",
        "y_thresh_dict[\"African-American\"] = threshold_African_American\n",
        "y_thresh_dict[\"Caucasian\"] = threshold_Caucasian\n",
        "\n",
        "#@markdown <br>Customize what information is displayed:\n",
        "display_all_performance_numbers = False #@param {type:\"boolean\"}\n",
        "display_confusion_matrix = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <br>Compare the **new prediction** with the **old one** for the specific feature? (Picking **\"No\"** will display performance for the new prediction)\n",
        "compare = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <br>Customize some aspects of the plot:\n",
        "#@markdown Choose or type the width and height for each figure in inches:\n",
        "figure_width = \"6.5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=7.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=7.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "print(\"Your selection:\")\n",
        "print(\"Model:               {}\".format(model_name))\n",
        "print(\"Feature(s):          {}\".format(feature))\n",
        "print(\"Compare with old:    {}\".format(compare))\n",
        "\n",
        "# predict with the model using different thresholds \n",
        "new_results = {}\n",
        "new_results = predict_different_thresholds(model_name,\\\n",
        "                                           all_models[model_name][\"model\"], \\\n",
        "                                           X_train, y_train, feature=feature,\\\n",
        "                                           full_data_training=train_set, \\\n",
        "                                           y_thresh_dict=y_thresh_dict,\\\n",
        "                                           dataset=\"train\")\n",
        "\n",
        "if (\"by_feature\" not in new_results[new_model_name].keys() ):\n",
        "  new_results[new_model_name][\"by_feature\"] = {} # reset the dictionary for results by feature\n",
        "\n",
        "new_results[new_model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "evaluate_performance_with_feature_values(model_name=new_model_name, \\\n",
        "                                        model=new_models[new_model_name][\"model\"], \\\n",
        "                                        feature=feature, X_data=new_X_train, \\\n",
        "                                        y_data=new_y_train, \\\n",
        "                                        full_data_training=train_set, \\\n",
        "                                        results_full_data=new_results[model_name],\\\n",
        "                                        num_bins=num_bins, \\\n",
        "                                        prefix_title=\"NEW: \", \\\n",
        "                                        class_names=None,\\\n",
        "                                        display_conf_mat=display_confusion_matrix,\\\n",
        "                                        display_roc_pr=display_precision_recall, \\\n",
        "                                        plot_together=compare, \\\n",
        "                                        figsize=(figure_width,figure_height))\n",
        "\n",
        "if compare:\n",
        "    # check to see if we have results from the \"old\" model for the same feature\n",
        "    if (\"by_feature\" not in all_results[new_model_name].keys() ):\n",
        "      all_results[new_model_name][\"by_feature\"] = {}\n",
        "\n",
        "    all_results[new_model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "    evaluate_performance_with_feature_values(model_name=new_model_name, \\\n",
        "                                        model=all_models[new_model_name][\"model\"], \\\n",
        "                                        feature=feature, X_data=X_train, \\\n",
        "                                        y_data=y_train, \\\n",
        "                                        full_data_training=train_set, \\\n",
        "                                        results_full_data=all_results[model_name],\\\n",
        "                                        num_bins=num_bins, \\\n",
        "                                        prefix_title=\"OLD: \", \\\n",
        "                                        class_names=None,\\\n",
        "                                        display_conf_mat=display_confusion_matrix,\\\n",
        "                                        display_roc_pr=display_precision_recall, \\\n",
        "                                        plot_together=compare, \\\n",
        "                                        figsize=(figure_width,figure_height))\n",
        "\n",
        "    results_to_display = pd.merge( left=new_results[new_model_name][\"by_feature\"][feature], \\\n",
        "                                  right=all_results[new_model_name][\"by_feature\"][feature], \\\n",
        "                                  left_index=True, right_index=True,\\\n",
        "                                  how=\"inner\", suffixes=(\"_NEW\", \"_OLD\") )\n",
        "\n",
        "else:\n",
        "    results_to_display = new_results[new_model_name][\"by_feature\"][feature]\n",
        "\n",
        "if display_all_performance_numbers:\n",
        "  display(results_to_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VA47Ha8BU2Zb"
      },
      "source": [
        "#@title **Automatically determine the thresholds so the false-positive/false-negative rates are close**\n",
        "\n",
        "#@markdown Select one of the models we trained:\n",
        "model_name = \"tree\" #@param [\"tree\", \"forest\"]\n",
        "\n",
        "# #@markdown Type a threshold in [0,1] to use for Caucasian. We will automatically determine a threshold for African-American so the FPR/FNR are with a specified tolerance.\n",
        "# threshold_Caucasian = 0.5 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Select or type a **tolerance** (in %) for the allowed difference in false-positive and false-negative rates:\n",
        "tolerance = \"5\" #@param [1, 2, 3, 4, 5]{allow-input: true}\n",
        "tolerance = np.float(tolerance)/100\n",
        "\n",
        "#@markdown <br>Customize a few things for the display/plots:\n",
        "display_all_performance_numbers = True #@param {type:\"boolean\"}\n",
        "display_confusion_matrix = True #@param {type:\"boolean\"}\n",
        "compare_with_old = True #@param {type:\"boolean\"}\n",
        "figure_width = \"6.5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=7.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=7.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "feature = \"race\"\n",
        "\n",
        "# set up potential thresholds\n",
        "thresh_AA_vals = np.arange(0,1.05,0.05)\n",
        "thresh_C_vals = np.arange(0,1.05,0.05)\n",
        "#thresh_C_vals = [threshold_Caucasian]\n",
        "\n",
        "y_thresh_dict = {}\n",
        "AA_col = \"race=African-American\"\n",
        "C_col = \"race=Caucasian\"\n",
        "FPR_label = \"False Positive Rate FPR=FP/(TN+FP)=1-TNR\"\n",
        "FNR_label = \"False Negative Rate FNR=FN/(TP+FN)=1-TPR\"\n",
        "accuracy_label = \"Accuracy =(TP+TN)/(TP+FP+TN+FN)\"\n",
        "precision_label = \"Precision = TP/(TP+FP)\"\n",
        "recall_label = \"Recall = TPR\"\n",
        "all_options = {}\n",
        "all_options[\"thresh\"] = []\n",
        "all_options[\"accuracy\"] = []\n",
        "all_options[\"precision\"] = []\n",
        "all_options[\"recall\"] = []\n",
        "\n",
        "for threshold_African_American in thresh_AA_vals:\n",
        "  for threshold_Caucasian in thresh_C_vals:\n",
        "\n",
        "      # update the dictionary with thresholds\n",
        "      y_thresh_dict[\"African-American\"] = threshold_African_American\n",
        "      y_thresh_dict[\"Caucasian\"] = threshold_Caucasian\n",
        "\n",
        "      # predict with the model using different thresholds \n",
        "      new_results = predict_different_thresholds(model_name,\\\n",
        "                                                all_models[model_name][\"model\"], \\\n",
        "                                                X_train, y_train, feature=feature,\\\n",
        "                                                full_data_training=train_set, \\\n",
        "                                                y_thresh_dict=y_thresh_dict,\\\n",
        "                                                dataset=\"train\")\n",
        "      \n",
        "      # reset the dictionary for results by feature\n",
        "      new_results[new_model_name][\"by_feature\"] = {}\n",
        "\n",
        "      # evaluate performance by feature\n",
        "      new_results[new_model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "      evaluate_performance_with_feature_values(model_name=new_model_name, \\\n",
        "                                              model=new_models[new_model_name][\"model\"], \\\n",
        "                                              feature=feature, X_data=new_X_train, \\\n",
        "                                              y_data=new_y_train, \\\n",
        "                                              full_data_training=train_set, \\\n",
        "                                              results_full_data=new_results[model_name],\\\n",
        "                                              num_bins=num_bins, \\\n",
        "                                              prefix_title=\"NEW: \", \\\n",
        "                                              class_names=None,\\\n",
        "                                              display_conf_mat=False,\\\n",
        "                                              display_roc_pr=False, \\\n",
        "                                              plot_together=False, \\\n",
        "                                              figsize=(figure_width,figure_height))\n",
        "      \n",
        "      # check if performance is within tolerance\n",
        "      df = new_results[new_model_name][\"by_feature\"][feature]\n",
        "      if ( max( np.abs(df[AA_col][FPR_label] - df[C_col][FPR_label]), \\\n",
        "                  np.abs(df[AA_col][FNR_label] - df[C_col][FNR_label]) ) < tolerance ):\n",
        "        pair = (threshold_African_American,threshold_Caucasian)\n",
        "        all_options[\"thresh\"] += [pair]\n",
        "        all_options[\"accuracy\"] += [df[new_model_name][accuracy_label]]\n",
        "        all_options[\"precision\"] += [df[new_model_name][precision_label]]\n",
        "        all_options[\"recall\"] += [df[new_model_name][recall_label]]\n",
        "\n",
        "# determine the optimal threshold\n",
        "if len(all_options[\"accuracy\"]) == 0 :\n",
        "    print(\"Unfortunately could not find values within the specified tolerance. Please make the tolerance parameter larger.\")\n",
        "\n",
        "else:\n",
        "    idx_max = all_options[\"accuracy\"].index( max(all_options[\"accuracy\"]) )\n",
        "    print(\"Highest accuracy achieved at thresholds :\\n\\t AA:{:.2f}\\n\\t C:{:.2f}\".\\\n",
        "          format(all_options[\"thresh\"][idx_max][0],all_options[\"thresh\"][idx_max][1]))\n",
        "\n",
        "    # re-evaluate at the optimal threshold and display the results this time\n",
        "    y_thresh_dict[\"African-American\"] = all_options[\"thresh\"][idx_max][0]\n",
        "    y_thresh_dict[\"Caucasian\"] = all_options[\"thresh\"][idx_max][1]\n",
        "\n",
        "    # predict with the model using different thresholds \n",
        "    new_results = predict_different_thresholds(model_name,\\\n",
        "                                              all_models[model_name][\"model\"], \\\n",
        "                                              X_train, y_train, feature=feature,\\\n",
        "                                              full_data_training=train_set, \\\n",
        "                                              y_thresh_dict=y_thresh_dict,\\\n",
        "                                              dataset=\"train\")\n",
        "\n",
        "    # evaluate performance by feature\n",
        "    new_results[new_model_name][\"by_feature\"] = {}\n",
        "    new_results[new_model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "    evaluate_performance_with_feature_values(model_name=new_model_name, \\\n",
        "                                            model=new_models[new_model_name][\"model\"], \\\n",
        "                                            feature=feature, X_data=new_X_train, \\\n",
        "                                            y_data=new_y_train, \\\n",
        "                                            full_data_training=train_set, \\\n",
        "                                            results_full_data=new_results[model_name],\\\n",
        "                                            num_bins=num_bins, \\\n",
        "                                            prefix_title=\"NEW: \", \\\n",
        "                                            class_names=None,\\\n",
        "                                            display_conf_mat=display_confusion_matrix,\\\n",
        "                                            display_roc_pr=False, \\\n",
        "                                            plot_together=False, \\\n",
        "                                            figsize=(figure_width,figure_height))\n",
        "\n",
        "    if compare_with_old:\n",
        "        # check to see if we have results from the \"old\" model for the same feature\n",
        "        if (\"by_feature\" not in all_results[new_model_name].keys() ):\n",
        "          all_results[new_model_name][\"by_feature\"] = {}\n",
        "\n",
        "        all_results[new_model_name][\"by_feature\"][feature], ax_roc, ax_pr = \\\n",
        "        evaluate_performance_with_feature_values(model_name=new_model_name, \\\n",
        "                                            model=all_models[new_model_name][\"model\"], \\\n",
        "                                            feature=feature, X_data=X_train, \\\n",
        "                                            y_data=y_train, \\\n",
        "                                            full_data_training=train_set, \\\n",
        "                                            results_full_data=all_results[model_name],\\\n",
        "                                            num_bins=num_bins, \\\n",
        "                                            prefix_title=\"OLD: \", \\\n",
        "                                            class_names=None,\\\n",
        "                                            display_conf_mat=display_confusion_matrix,\\\n",
        "                                            display_roc_pr=display_precision_recall, \\\n",
        "                                            plot_together=compare_with_old, \\\n",
        "                                            figsize=(figure_width,figure_height))\n",
        "\n",
        "        results_to_display = pd.merge( left=new_results[new_model_name][\"by_feature\"][feature], \\\n",
        "                                      right=all_results[new_model_name][\"by_feature\"][feature], \\\n",
        "                                      left_index=True, right_index=True,\\\n",
        "                                      how=\"inner\", suffixes=(\"_NEW\", \"_OLD\") )\n",
        "\n",
        "    else:\n",
        "        results_to_display = new_results[new_model_name][\"by_feature\"][feature]\n",
        "\n",
        "    if display_all_performance_numbers:\n",
        "      display(results_to_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkRrcEo7yyx0"
      },
      "source": [
        "**TASK. Go back to the step where we initially created our decision tree and change the `max_depth` parameter, and repeat the various steps above. What do you observe?** Some things to look out for:\n",
        "1. **How does the predictive accuracy change with `max_depth`?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What happens to the errors in the training set?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What about the errors in the test set?\n",
        "2. **How does the overall decision tree change?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do the same features remain important?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do the splits change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYRivVtrJCZ5"
      },
      "source": [
        "# **7. Things We Are Leaving Out**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoAsVq3mziqx"
      },
      "source": [
        "### Evaluating Bias Using **Google's What-If Tool**\n",
        "The researchers at Google [PAIR](https://research.google/teams/brain/pair/) have developed a powerful open-source tool called [What-If](https://pair-code.github.io/what-if-tool/get-started/) that allows inspecting classification or regression models to detect discrimination or bias.\n",
        "\n",
        "*Run and expand this entire sub-section. No need to understand any of the code - we will only rely on the output.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZW7FTkFzdFL"
      },
      "source": [
        "#@title Install the What-If Tool widget if running in colab {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  !pip install --upgrade witwidget &> /dev/null\n",
        "  from witwidget.notebook.visualization import WitConfigBuilder\n",
        "  from witwidget.notebook.visualization import WitWidget\n",
        "  print(\"Success!\")\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BzLqe9Q0LTn",
        "cellView": "form"
      },
      "source": [
        "#@title Use **What-If Tool** to inspect the model for bias/discrimination\n",
        "\n",
        "#@markdown Select **one of the models** we trained:\n",
        "model_name = \"tree\" #@param [\"tree\", \"forest\"]\n",
        "\n",
        "#@markdown (Optional) Adjust a few parameters for the What-If tool:\n",
        "num_datapoints = 3000  #@param {type: \"number\"}\n",
        "tool_height_in_pixels = 1000  #@param {type: \"number\"}\n",
        "\n",
        "model = all_models[model_name][\"model\"]\n",
        "\n",
        "# define a function for prediction\n",
        "def adjust_prediction(X_train):\n",
        "  return model.predict(X_train)\n",
        "\n",
        "# Return model predictions and SHAP values for each inference.\n",
        "def custom_predictions(examples_to_infer):\n",
        "  # Get the class predictions from the model.\n",
        "  preds = model.predict(examples_to_infer)\n",
        "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
        "  return preds\n",
        "\n",
        "test_examples = np.hstack((X_train.values[0:num_datapoints,:],\\\n",
        "                           np.reshape( np.array(y_train[0:num_datapoints]),(num_datapoints,1)) ))\n",
        "config_builder = (WitConfigBuilder(test_examples.tolist(), X_train.columns.tolist() + [y_train.name])\n",
        "    .set_custom_predict_fn(all_models[\"tree\"][\"model\"].predict_proba)\n",
        "    .set_target_feature(y_train.name)\n",
        "    .set_label_vocab(['No Recid', 'Recid']))\n",
        "\n",
        "a = WitWidget(config_builder, height=tool_height_in_pixels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxgPnN8zyyyN"
      },
      "source": [
        "# **8. References**\n",
        "If you would like to learn more about some of the topics covered here, the following are a great set of resources:\n",
        " 1. [An Introduction to Statistical Learning with Applications in R](https://www.springer.com/gp/book/9781461471370) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani. Excellent and very accesible reference for conceptual elements + implementation in R.\n",
        " 2. [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291) by Aurélien Géron. Great reference for Machine Learning in Python, primarily on the coding and implementation side.\n",
        " 3. [Interpretable ML Book: A Guide to Making Black-Box Models Interpretable](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar. Great reference for going deeper on the topic of Interpretable Machine Learning."
      ]
    }
  ]
}