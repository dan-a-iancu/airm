{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S03-04_Descriptive_to_Predictive_Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gZ7HjwudVBO8",
        "2GOd-up_VBO_",
        "krD-m80kyyv1",
        "NK0jRm-Uyyvs",
        "FOsd8P1Vyyv9",
        "JoudwDaryywD",
        "Cu9Pwx-IyywE",
        "gtQqi9hOyywG",
        "J2glFCOZyywJ",
        "qvJahKKOyywR",
        "rUTWELBAyywT",
        "S9lXUqWGyywo",
        "4zZjAry5My-a",
        "1JHpvSQvWsLZ",
        "_GvxO9UVpc4c",
        "bOe3bM8Jyyw5",
        "lhndoFO4xdsF",
        "R_COgWGwyyw6",
        "PEgXpRvyyyxS",
        "DC-9H-veCyzK",
        "ZH7YFei5CyzO",
        "VYNQnzV9CyzR",
        "0LloIrYmrVGg",
        "4T1f-Erxi44U",
        "pDR1145E3Dtw",
        "oU7yaTCs05TD",
        "Dge8dygVW_y6",
        "eO0vJVnQyyxn",
        "5A4qX8C2FUGE",
        "veYk5xv7NdUF",
        "jedkr34Jyyxq",
        "-v13PC2Wyyxx",
        "uj75pA48yyx0",
        "ojryixiZyyxz",
        "SQHisC3qyyxp",
        "wzwP2k1aTpSo",
        "QySOMRaOl6Fp",
        "HBQBiqOBXdPw",
        "zkr91lP0yyx8",
        "MXc_oHbtyyxB",
        "TMHehu7Sl-x4",
        "hNYn5sn7mlzs",
        "yqKhhmH-uluI",
        "xjfvYgf4yyx_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dan-a-iancu/airm/blob/master/ML_Pipeline/S03_04_Descriptive_to_Predictive_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxhGE4RiVBO6"
      },
      "source": [
        "**A Typical Machine Learning Pipeline: From Descriptive to Predictive**\n",
        "\n",
        "This exercise will walk you through some of the typical steps that you would need to follow in a typical **Machine Learning Prediction** problem. We will be using the [California Housing Dataset](https://github.com/ageron/handson-ml2/tree/master/datasets/housing) to illustrate this: our task will be to **predict median house values in Californian districts** given a number of features from these districts. The dataset is a bit older and smaller than what you might encounter nowadays, but it serves as a very good pedagogical device. It has actually been featured in an excellent [recent book](https://github.com/ageron/handson-ml2) on machine learning in Python, and our notebook follows some of the developments there, but with a lot more guidelines to ensure a responsible implementation of the end-to-end ML pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk_jQKT5yyvs"
      },
      "source": [
        "<a id='top'></a>\n",
        "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
        "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Navigation</h3>\n",
        "\n",
        "[1. Download the Data](#1)   \n",
        "[2. Load the Data and Take a Quick Look](#2)  \n",
        "[3. Understand the Context](#3)   \n",
        "[4. Exploratory Data Analysis](#4)    \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Stats and Visualizations with Google's Facets Overview](#4a)   \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[b. Stats and Visualizations: Nuts & Bolts](#4b)       \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Correlation Analysis with Google's Facets Dive](#4c)   \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Correlation Analysis Nuts & Bolts](#4d)   \n",
        "[5. Split the Data into a Training and Test Set](#5)     \n",
        "[6. Preparing Data for Machine Learning Algorithms](#6)    \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Data Cleaning](#6a)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[b. Handling Text and Categorical Features](#6b)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[c. Scaling the Features](#6c)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[d. All Steps in Our Case](#6d)    \n",
        "[7. Train the Machine Learning Models](#7)   \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Our Three Models in a Nutshell](#7a)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[b. Train/Fit the Models](#7b)     \n",
        "[8. Evaluate and Interpret the Model](#8)   \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Predict and Evaluate in the Training Set](#8a)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Predict and Evaluate in the Test Set](#8b)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[b. Evaluate Bias](#8c)    \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[b. Interpret the Model](#8c)    \n",
        "[9. Things We Are Leaving Out](#9)  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[a. Avoiding Sampling Bias with Stratified Sampling](#9a)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[b. Feature Engineering](#9b)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[c. Calibrating Hyperparameters](#9c)     \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[d. Changing the Measure of Accuracy to Reflect other KPIs](#9d)    \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[e. Advanced Tools for Interpretability](#9e)     \n",
        "[10. References](#10)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ7HjwudVBO8"
      },
      "source": [
        "______\n",
        "<a id=\"basic_setup\"></a>\n",
        "# **Basic Setup**\n",
        "Most Python code relies on specific packages that define useful data-structures, objects and functions to manipulate these. The code below does exactly that, and also defines some useful routines for plotting figures or saving them to files. \n",
        "\n",
        "_**Please run this entire section by clicking the \"Play\" button.** No need to worry about understanding any of this code._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sicBV7d0VBO8",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@markdown Import revelant modules and define some useful functions\n",
        "import os\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)   # Python ≥3.5 is required\n",
        "import urllib\n",
        "\n",
        "import numpy as np   # numpy for numerical linear algebra\n",
        "import pandas as pd  # pandas for managing dataframes\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from tensorflow.keras import layers\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn version ≥0.20 required\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# adjust reporting in pandas: max 10 rows and two-digit precision\n",
        "#pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "\n",
        "# import matplotlib and pyplot: critical packages for plotting \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "# Make sure Matplotlib runs inline, for nice figures\n",
        "%matplotlib inline  \n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# install the latest version of seaborn for nicer graphics\n",
        "!pip install --prefix {sys.prefix} seaborn==0.11.0  &> /dev/null\n",
        "import seaborn as sns\n",
        "\n",
        "# install facets overview\n",
        "!pip install facets-overview &> /dev/null\n",
        "# import facets overview + some other relevant packages\n",
        "from IPython.core.display import display, HTML\n",
        "import base64\n",
        "from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator\n",
        "\n",
        "# install graphviz for visualizing decision trees\n",
        "!pip install graphviz  &> /dev/null\n",
        "import graphviz\n",
        "\n",
        "# install pdpbox for partial dependency visualization\n",
        "!pip install pdpbox &> /dev/null\n",
        "from pdpbox import info_plots, pdp\n",
        "\n",
        "# Ignore useless some warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action=\"ignore\")\n",
        "\n",
        "# Create a function to save figures to a desired local folder\n",
        "FIGURE_FOLDER = \"Figures\"\n",
        "FIGURE_PATH = os.path.join(\".\",FIGURE_FOLDER)\n",
        "os.makedirs(FIGURE_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(FIGURE_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# A function to append dummies for specified variables -- this is done here because of some plotting routines\n",
        "# returns the new dataframe and a dictionary with the categories for each categorical variable turned into a dummy\n",
        "def append_dummies(data, columns):\n",
        "  categories = {}  # the dictionary with categories\n",
        "  data_with_dummies = data.copy() # the changed dataframe\n",
        "  for col in columns:\n",
        "    if col not in data.columns:\n",
        "      print(\"WARNING. Column '{}' not among the columns in the dataframe. Skipping it.\")\n",
        "    elif (data[col].dtype!=int and data[col].dtype!=float):\n",
        "      dummy_df = pd.get_dummies(data[col], prefix=col)\n",
        "      categories[col] = list(dummy_df.columns)  # keep all the categories\n",
        "      data_with_dummies = pd.merge( left=data_with_dummies, \\\n",
        "                                    right=dummy_df, \\\n",
        "                                   left_index=True, right_index=True,\\\n",
        "                                   how=\"inner\", suffixes=(\"\", \"\") )\n",
        "  return data_with_dummies, categories\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GOd-up_VBO_"
      },
      "source": [
        "_________\n",
        "<a id=\"1\"></a>\n",
        "# **1. Download the Data**\n",
        "You may sometimes need to download your data from some remote website repository or database such as Kaggle, Github, UCI, etc. Here, we will download our dataset as a CSV (comma separated value) file from a Github site, and save it to a local folder.\n",
        "\n",
        "_**Please run this entire section.** In practice, all you would need to do is change the URL below to point to the right website. If you already have your data stored locally in a suitable format (e.g., as a CSV or Excel file), you can skip this step._\n",
        "\n",
        "<font color=green>**EXCEL:** You can also do this in MS Excel: from the **Data** menu, select **\"Get Data\"**.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHsLmnszyyvy",
        "cellView": "form"
      },
      "source": [
        "#@markdown Download the data from the web and save it to a local CSV file\n",
        "url = \"https://raw.githubusercontent.com/dan-a-iancu/airm/master/ML_Pipeline/housing.csv\"  # full URL to the dataset\n",
        "local_csv = \"housing.csv\"   # name of local file where you want to store the downloaded file\n",
        "aux = urllib.request.urlretrieve(url, local_csv)    # download from website and save it locally\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krD-m80kyyv1"
      },
      "source": [
        "___________\n",
        "<a id=\"2\"></a>\n",
        "# **2. Load the Data and Take a Quick Look**\n",
        "Next, we read the dataset from the local CSV file into Python, and store it into a `pandas` DataFrame, which is very convenient for manipulating data. A way to think of a DataFrame is as an Excel table with data organized by columns, where each row corresponds to a different **sample**.  \n",
        "\n",
        "_**Please run this entire section.** We suggest expanding the cells to see how a DataFrame looks and to read some brief comments._\n",
        "\n",
        "<font color=green>**EXCEL:** Even if your data is stored in a local file, it is good to import it from the **Data** menu (**Data > Get Data > From File**), as this defines it as a table and already includes a Filter, which will make later steps a lot easier. Check if your data include a unique identifier for each row/sample. If not, you will need to include one yourself; for instance, you can label each sample using an integer from 1 to (number of rows).</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIG2Ll1BVBO_",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@markdown Read the data from a CSV file into a `pandas` DataFrame\n",
        "data = pd.read_csv(local_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POqCE_O-VBPD",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@title Examine a few rows at the top \n",
        "\n",
        "#@markdown Select or type how many rows to display:\n",
        "num_samp = \"5\" #@param [5, 10, 20] {allow-input:true}\n",
        "num_samp = np.int(num_samp)\n",
        "\n",
        "data.head(num_samp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3G-UrIYSyyv6",
        "cellView": "form"
      },
      "source": [
        "#@title Examine a few rows at the bottom\n",
        "#@markdown Select or type how many rows to display:\n",
        "num_samp = \"5\" #@param [5, 10, 20] {allow-input:true}\n",
        "num_samp = np.int(num_samp)\n",
        "\n",
        "data.tail(num_samp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1-snT_5yyv8"
      },
      "source": [
        "Note that each sample (i.e. row) in our data contains information on one district in California. The very first column in the table above is not an actual data field, but rather a unique index to identify the sample/row -- this was automatically introduced when we read our data in Python, and it makes it easy to refer to each sample. Our actual dataset has 10 columns, corresponding to the columns above that have headers. The first 9 seem to have **numeric** values, and `ocean_proximity` looks like a text and seems to have repeated values, so it is likely **categorical**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcptHFO06cus"
      },
      "source": [
        "# <font color = red>**TASK #1**</font>\n",
        "**Conduct the following tasks in Excel:**\n",
        "\n",
        "1. Load a copy of the data directly into Excel from the following web link:\n",
        "> [https://raw.githubusercontent.com/dan-a-iancu/airm/master/ML_Pipeline/housing.csv]()  \n",
        "  \n",
        "  and save it as an **Excel** file.\n",
        "\n",
        "2. Add an initial column with a **unique index** for each sample. Here, please use an integer running from **0** to **(number of samples-1)**, so you see the same thing as in Python. \n",
        "  - You can do this manually after your copied the data, but the best way is to select **Transform** when you are importing your data, and choose to add a column with an index starting from 0.\n",
        "\n",
        "3. Take a quick look at the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK0jRm-Uyyvs"
      },
      "source": [
        "__________\n",
        "<a id=\"3\"></a>\n",
        "# **3. Understand the Context**\n",
        "Before diving in, it is very important to gain as much clarity about the context as possible. Some critical questions to consider:\n",
        "  * **What type of data is available?** (Specifically, obtain a complete **data dictionary** that explains each field, and check you data against it.)\n",
        "  * **How was the data collected and processed?**  (By whom, what methods, was there any pre-processing done?)\n",
        "  * **What is the immediate prediction problem to solve?** (What is the precise **predicted quantity** and what kind of prediction problem is this?)\n",
        "  * **How are the model's predictions going to be used?**  (Context is critical to avoid pitfalls related to privacy, legal issues, biases, etc.)\n",
        "\n",
        "_To see more details about these issues and answers in our specific case, expand the hidden cells._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfGvGkr8-dIj"
      },
      "source": [
        "Before starting out, one needs as much clarity as possible about the  data and the (end) purpose of the ML project, which will also help with planning some of the subsequent steps. A few of the things to be aware of:  \n",
        "  * **What data are available?**  \n",
        "      - Make sure to obtain a **data dictionary** that describes all the available data fields.  \n",
        "    <font color=blue>Our data fields here have the following meaning:\n",
        "    *   `longitude`: A measure of how far west a district is (higher value is farther west)\n",
        "    *   `latitude`: How far north a district is (a higher value is farther north)\n",
        "    *   `housingMedianAge`: Median age of houses within the district (lower number is a newer building)\n",
        "    *   `totalRooms`: Total number of rooms within a district\n",
        "    *   `totalBedrooms`: Total number of bedrooms within a district\n",
        "    *    `population`: Total number of people residing within a district\n",
        "    *    `households`: Total number of households within a district\n",
        "    *    `medianIncome`: Median income for households within a district (measured in tens of thousands of US Dollars)\n",
        "    *    `medianHouseValue`: Median house value in a district (measured in US Dollars). This is our **predicted variable** of interest.\n",
        "    *    `ocean_proximity`: A variable that characterizes the location of the district with respect to the ocean/bay.</font>  \n",
        "\n",
        "    <font color=red>**GOOD PRACTICE: you may want to check a few samples in your data against the data dictionary, to spot any inconsistencies.**</font>\n",
        "\n",
        "  * **How was the data collected and processed?**  \n",
        "     - Try to find out how the data was collected (by whom, what methods, etc.), and if any **censoring or pre-processing** occurred before you obtained it.  \n",
        "     <font color=blue>Our dataset is based on information collected in the 1990 California census. Aurélien Géron (the author of an ML textbook) modified it by removing some entries and defining the variable `ocean_proximity`. For more details, you can read [here](https://github.com/ageron/handson-ml2/tree/master/datasets/housing).</font>\n",
        "\n",
        "  * **What is the precise prediction problem to solve?**  \n",
        "      - What is the **predicted quantity**?  \n",
        "      <font color=blue>Our task here is to predict **median house values in districts in California** based on features of those districts.</font>  \n",
        "      - Does the prediction require solving a **regression** or a **classification** problem?  \n",
        "      <font color=blue>We want to predict the precise house value as a continuous number, so we are dealing with a **regression task**.</font>  \n",
        "\n",
        "  * **How is the ML model going to be used?**  \n",
        "    - Predictive ML models are often used as inputs into other quantitative models -- for instance, prescriptive models, like we saw with the Starbux minicase last time. So it is important to understand as much as possible regarding **how** your ML model will be used. This is critical for the design, (e.g., it can inform what features might be relevant to add, and also help you tweak your model's \"goodness of fit\" so it better matches the end purpose), and also to make sure you avoid any privacy or legal concerns (e.g., using features that may be protected)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvrGby0Fyyv8"
      },
      "source": [
        "**Vocabulary.** Different disciplines like to use different terms in the context of prediction tasks. For instance, you may see the **columns** referred to as **attributes**, **fields** or **variables** interchangeably. Similarly, you may see the **predicted variable** (i.e., the `median_house_value`) also referred to as the **output**, **target**, **label** or **dependent variable**, and the other attributes referred to as **predictors**, **features** or **independent variables**.\n",
        "\n",
        "**Notation.** We may have to write a few formulas later, so it would be useful to have a bit of notation defined:\n",
        "   - We will use $i$ to denote a sample/row in our data. The easiest way is to think of $i$ as the index of the row in the table above. For instance, $i=0$ refers to the first sample, $i=1$ refers to the second row, etc.\n",
        "   - We will use $y_i$ to denote the value that our **target** (i.e., the `median_house_value`) takes in the $i$-th sample. For instance, $y_0 = 452,600$.\n",
        "   - We will use $x_i$ to denote the value of a particular **feature $x$** in the $i$-th sample. For instance, if $x$ is `total_rooms`, then $x_{20635} = 1,665$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZR2rHaPyyv9"
      },
      "source": [
        "_________________\n",
        "<a id=\"4\"></a>\n",
        "# **4. Exploratory Data Analysis**\n",
        "The next step is to start exploring your data through **summary statistics**, **visualizations**, and a **correlation analysis**. These kinds of **descriptive analytics** help you understand your data and how it relates to your prediction task, and also enable you to detect potentially problematic issues like missing or incorrect entries, censoring problems, protected or private data, etc.\n",
        "\n",
        "*This is an important section, and we will be replicating some parts of the analysis in Excel. We suggest expanding the hidden cells and then choosing which sub-section to expand further.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80EN6wfyyvt"
      },
      "source": [
        "<font color=red>Some important questions and issues to bear in mind when auditing your dataset to ensure a **responsible** implementation:</font>\n",
        "* **Do you notice any pattern for some of your variables?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are there missing values?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are there unexpectedly high or low values?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do some values occur more or less often than you would expect?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do you see signs of skew?\n",
        "* **Is the pattern you are observing \"robust\"?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Does it depend on the way in which you are visualizing the data?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is it different than what you might expect / does it represent reality?  \n",
        "* **Does the pattern have meaningful implications in the given prediction problem?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Could it lead to over-representing or under-representing some parts of the \"real\" data?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Could it generate bias in the predicted values?\n",
        "* **Could using some variables in the given prediction task raise legal or privacy concerns?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do the variables directly or indirectly speak to [**protected characteristics**](https://www.equalityhumanrights.com/en/equality-act/protected-characteristics) such as age, gender, race/ethnicity, etc.?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Could using some data lead to privacy violations?  \n",
        "* **How are the variables related to the predicted variable (target) or other variables**?<br></font>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is there strong correlation?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are the values related in some other way?   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOsd8P1Vyyv9"
      },
      "source": [
        "<a id=\"5-EDA1-Facets\"></a>\n",
        "## **Statistics and Visualizations** with Google's **Facets Overview**\n",
        "The researchers at Google [PAIR](https://research.google/teams/brain/pair/) have developed a powerful open-source tool called [Facets](https://pair-code.github.io/facets/) that allows exploring the data through an easy-to-use graphical user interface. We can try using it on our dataset.\n",
        "\n",
        "*Run and expand this entire sub-section. No need to understand any of the code - we will only rely on the output.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3BHbdWZSyywA",
        "cellView": "form"
      },
      "source": [
        "#@title Visualize the Data in Facets\n",
        "\n",
        "# generate the visualization\n",
        "gfsg = FeatureStatisticsGenerator()\n",
        "\n",
        "proto = gfsg.ProtoFromDataFrames([{'name': 'original_data', 'table': data}])\n",
        "# uncomment next two lines if you want to visualize training & test sets\n",
        "#proto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': train_set},\n",
        "#                                  {'name': 'test', 'table': test_set}])\n",
        "protostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-overview id=\"elem\"></facets-overview>\n",
        "        <script>\n",
        "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(protostr=protostr)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiFxScMMGelK"
      },
      "source": [
        "The tool should hopefully be self-explanatory. Some small things to note:\n",
        "  * using **Sort by**, you can sort features based on other criteria; specifically:\n",
        "      - **Non-uniformity** can be useful to seeing which features are very skewed\n",
        "      - **Amount missing/zero** can allow you to see the features with missing values\n",
        "  * underneath **Chart to show**, there are two useful buttons:\n",
        "      - you can see a histogram of the logarithm of the values \n",
        "      - you can see larger histograms by clicking on **expand** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoudwDaryywD"
      },
      "source": [
        "<a id=\"5-EDA-NutsBolts\"></a>\n",
        "## **Statistics and Visualizations** : Nuts & Bolts\n",
        "Although the Facets tool is powerful and easy to use, it is important to understand at a deeper level some of the routine processes. This enables you to replicate the functionality (useful if you don't have access to Google's `tensorflow` package or even Python), scale it up (useful if your dataset has thousands of variables), and **avoid certain pitfalls** that can emerge in the process.\n",
        "\n",
        "*There are many details here. As you start conducting <font color=red>**TASK #2**</font> below, you may want to expand certain sections here to compare outputs.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpGn1HGPm6P",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Define various functions to help with visualizations**\n",
        "\n",
        "#@markdown A function for **simple visualizations** of one or more features with histograms/boxplots/countplots\n",
        "def visualize_features(all_data, features, figsize=(6,6), num_plot='histogram', \\\n",
        "                       num_bins='auto', num_cols=3):\n",
        "\n",
        "    # calculate how many figures are needed\n",
        "    num_figs = len(features)\n",
        "    num_columns = (1 if num_figs==1 else num_cols)\n",
        "    num_rows = int(np.ceil(num_figs/num_columns))\n",
        "    \n",
        "    # one large figure with a common title\n",
        "    fig = plt.figure(figsize=(figsize[0]*num_columns,figsize[1]*num_rows))\n",
        "    \n",
        "    # title for plot\n",
        "    title = ''\n",
        "\n",
        "    for i in range(num_figs):\n",
        "\n",
        "      # Create figure\n",
        "      plt.subplot(num_rows,num_columns,i+1)\n",
        "      ax1 = plt.gca()\n",
        "      ax1.grid(alpha=.5, linewidth=1)\n",
        "\n",
        "      if( (all_data[features[i]].dtype == 'float') or (all_data[features[i]].dtype == 'int') ):\n",
        "        # numeric feature \n",
        "        \n",
        "        # plot the feature according to the option\n",
        "        if( num_plot == 'histogram'):\n",
        "          #title = \"Histogram of {}\".format(features[i])\n",
        "          sns.histplot(data = all_data[features[i]], bins=num_bins, color=\"skyblue\", ax=ax1).set_title(features[i])\n",
        "\n",
        "        elif( num_plot == 'boxplot' ):\n",
        "          #title = \"Boxplot of {}\".format(features[i])\n",
        "          sns.boxplot(data = all_data[features[i]], color=\"skyblue\", ax=ax1).set_title(features[i])\n",
        "        plt.tight_layout()\n",
        "\n",
        "      else:\n",
        "        # categorical feature\n",
        "        sns.countplot(data = all_data, x=features[i], palette=\"Blues\", ax=ax1).set_title(features[i])\n",
        "        plt.tight_layout()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # determine the title\n",
        "    #tar_ylabel = '{}'.format(target_name) if target_name else 'target'\n",
        "    #fig.suptitle(\"Predicted value of {} as a function of feature(s).\".format(tar_ylabel), fontsize=16, color=color1)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#\n",
        "#\n",
        "#@markdown A function to visualize **average dependency** of one numeric feature as a function of two other features\n",
        "def visualize_average_dependency(all_data_with_dummies, all_categories, \\\n",
        "                                 feature_y, feature_x1, feature_x2=None,\\\n",
        "                                 num_bins=10, figsize=(10,9)):\n",
        "   \n",
        "    # append to all categories a string for numeric categories\n",
        "    all_feats = [feature_y, feature_x1]\n",
        "    if feature_x2 != None:\n",
        "      all_feats += [feature_x2]\n",
        "\n",
        "    categ_for_infoplot = all_categories.copy()\n",
        "    for feat_ in all_feats:\n",
        "      if feat_ not in all_categories.keys():\n",
        "        # just the names for numeric features\n",
        "        categ_for_infoplot[feat_] = feat_\n",
        "\n",
        "    # do all the infoplots\n",
        "    if feature_x2==None:\n",
        "      # visualizing just two features, so a simple target_plot will do\n",
        "      title = \"Relation between target '{}' and feature '{}'\".format(feature_y,feature_x1)\n",
        "      subtitle = \"The count/height of bars shows the number of samples where {} takes that value\".format(feature_x1) +\\\n",
        "        \"\\n and the line plot shows the average value of the target.\"\n",
        "\n",
        "      fig, axes, summary_df = \\\n",
        "      info_plots.target_plot(df=all_data_with_dummies, \\\n",
        "                            feature=categ_for_infoplot[feature_x1], \\\n",
        "                            feature_name=feature_x1, \\\n",
        "                            target=categ_for_infoplot[feature_y],\\\n",
        "                            num_grid_points=num_bins, grid_type='percentile', \\\n",
        "                            percentile_range=None, \\\n",
        "                            grid_range=None, cust_grid_points=None, \\\n",
        "                            show_percentile=False, \\\n",
        "                            show_outliers=False, endpoint=True, \\\n",
        "                            figsize=figsize, ncols=2, \\\n",
        "                            plot_params={\"title\" : title, \"subtitle\": subtitle})\n",
        "    else:\n",
        "      # plotting y as color-coded, as a function of x1 and x2\n",
        "      title = \"Relation between target '{}' and features '{}' and '{}'\".format(feature_y,feature_x1,feature_x2)\n",
        "      subtitle = \"The circle color shows the average value of the target\" +\\\n",
        "        \"\\n and the circle size shows the number of samples.\"\n",
        "\n",
        "      fig, axes, summary_df = \\\n",
        "      info_plots.target_plot_interact(df=data_with_dummies, \\\n",
        "                                      features=[categ_for_infoplot[feature_x1],categ_for_infoplot[feature_x2]], \\\n",
        "                                      feature_names=[feature_x1,feature_x2], \\\n",
        "                                      target=categ_for_infoplot[feature_y], \\\n",
        "                                      num_grid_points=num_bins, \\\n",
        "                                      grid_types=None, percentile_ranges=None, \\\n",
        "                                      grid_ranges=None, cust_grid_points=None, \\\n",
        "                                      show_percentile=False, \\\n",
        "                                      show_outliers=False, endpoint=True, \\\n",
        "                                      figsize=figsize, ncols=2, annotate=True, \\\n",
        "                                      plot_params={\"title\" : title, \\\n",
        "                                                    \"subtitle\" : subtitle})    \n",
        "#\n",
        "#\n",
        "#\n",
        "#@markdown A complex function to visualize **relationships** among several features\n",
        "def visualize_detailed_dependencies(all_data_with_dummies, all_categories, \\\n",
        "                                    target, feature, feature_color=None,\\\n",
        "                                    feature_breakdown=None, \\\n",
        "                                    plot_type='scatter', min_categ=5, orient=None,\\\n",
        "                                    figs_per_row=3, figsize=(10,9)):\n",
        "\n",
        "    # check to see if there's a need to change the plot type\n",
        "    if plot_type=='boxen':\n",
        "        # check to make sure that either the target or one of the features is numerical with enough values\n",
        "        if (target in data_categories.keys() \\\n",
        "            and feature in data_categories.keys() ) or \\\n",
        "            ( len(all_data_with_dummies[target].unique()) <= min_categ and \\\n",
        "             len(all_data_with_dummies[feature].unique()) <= min_categ ):\n",
        "            # target and first feature are categorical or have very few values\n",
        "            if feature_color in data_categories.keys() or \\\n",
        "            ( len(all_data_with_dummies[feature_color].unique()) <= min_categ):\n",
        "              # second feature is also categorical or with very few values \n",
        "              # it means all features are categorical -> switch to countplot\n",
        "              print('Target and all selected features are categorical or have very few values. Switching to a countplot.')\n",
        "              plot_type = 'count'\n",
        "            else:\n",
        "              # third feature is numeric with enough values --> switch with first feature\n",
        "              aux=feature\n",
        "              feature=feature_color\n",
        "              feature_color=aux\n",
        "\n",
        "    if plot_type=='scatter':\n",
        "        # simple scatter plot\n",
        "        sns.relplot(data=data_with_dummies, x=feature, y=target, \\\n",
        "                    hue=feature_color, col=feature_breakdown, \\\n",
        "                    row=None, style=None, col_wrap=None, row_order=None, \\\n",
        "                    col_order=None, palette=None, hue_order=None, hue_norm=None, \\\n",
        "                    sizes=None, size_order=None, size_norm=None, markers=None, \\\n",
        "                    dashes=None, style_order=None, legend='auto', \\\n",
        "                    kind='scatter', height=figsize[1], \\\n",
        "                    aspect=figsize[0]/figsize[1], facet_kws=None, units=None)\n",
        "\n",
        "    if plot_type=='count':\n",
        "        sns.catplot(data=data_with_dummies, x=target, y=None,\\\n",
        "                    hue=feature, col=feature_breakdown, \\\n",
        "                    kind=plot_type, row=None, col_wrap=None, \\\n",
        "                    row_order=None, col_order=None, \\\n",
        "                    ci=95, n_boot=1000, units=None, seed=None, order=None, \\\n",
        "                    hue_order=None, height=figsize[1], aspect=figsize[0]/figsize[1], \\\n",
        "                    orient=orient, color=None, palette=None, legend=True, \\\n",
        "                    legend_out=True, sharex=True, sharey=True, \\\n",
        "                    margin_titles=False, facet_kws=None)\n",
        "\n",
        "    if plot_type=='boxen':\n",
        "        # check if the number of categories for the target is smaller than for the feature\n",
        "        if( len(all_data_with_dummies[target].unique()) < \\\n",
        "           len(all_data_with_dummies[feature].unique()) ):\n",
        "          orient='h'\n",
        "        sns.catplot(data=data_with_dummies, x=feature, y=target, \\\n",
        "                    hue=feature_color, col=feature_breakdown, \\\n",
        "                    kind=plot_type, row=None, col_wrap=None, \\\n",
        "                    row_order=None, col_order=None, \\\n",
        "                    ci=95, n_boot=1000, units=None, seed=None, order=None, \\\n",
        "                    hue_order=None, height=figsize[1], aspect=figsize[0]/figsize[1], \\\n",
        "                    orient=orient, color=None, palette=None, legend=True, \\\n",
        "                    legend_out=True, sharex=True, sharey=True, \\\n",
        "                    margin_titles=False, facet_kws=None)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu9Pwx-IyywE"
      },
      "source": [
        "#### Examine the Data Values\n",
        "A good start is to check more carefully the kind of values that each attribute takes in the dataset.<br>\n",
        "<font color=green>**EXCEL:** You can add a [**Filter**](https://support.microsoft.com/en-us/office/filter-data-in-a-range-or-table-01832226-31b5-4568-8806-38c37dcc180e#:~:text=Select%20any%20cell%20within%20the,filter%20criteria%20and%20select%20OK.) from the **Data** menu, and glance at the values for each field.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7mKMvdVBPH",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@title Information on data types\n",
        "\n",
        "# take a look at the type of data\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8pTuQqyywG"
      },
      "source": [
        "The output above confirms how many samples/rows (under `RangeIndex`) and how many columns are in the dataset. From `Dtype`, you can see the **type** of data: columns 0-8 are `float64` values, which means they are _numeric_. `ocean_proximity` is reported as a Python `object`, which could mean a lot of things; but we already know from our data dictionary and from earlier output that it is a _categorical_ attribute. From `Non-Null Count`, you can see how many non-empty values there are for each feature. This reveals that the **`total_bedrooms`** feature has <font color=red>**missing values**</font> in some samples, which is very important information in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYBemFH_yywY"
      },
      "source": [
        "You may also want to see particular samples/rows of your dataset to check or flag errors. For instance, you can use the code below to display all the rows where the value of a specific feature (i.e., column) satisfies some condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aiBnRU6yywY",
        "cellView": "form"
      },
      "source": [
        "#@markdown Display the samples where a feature satisfies a certain condition\n",
        "\n",
        "#@markdown Select a numeric feature and specify a minimum/maximum range of values\n",
        "\n",
        "feature = \"total_bedrooms\" #@param [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\"]\n",
        "\n",
        "minimum_value = 1 #@param {type:\"number\"}\n",
        "maximum_value = 1 #@param {type:\"number\"}\n",
        "\n",
        "if data[feature].dtype=='int' or data[feature].dtype=='float':\n",
        "  filter = ( data[feature]>=minimum_value ) & (data[feature]<=maximum_value )\n",
        "else:\n",
        "  filter = []\n",
        "\n",
        "# locate all the rows with households = 1\n",
        "data.loc[ filter ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85MpUzdni9ZF",
        "cellView": "form"
      },
      "source": [
        "#@markdown Display all samples with missing values for `total_bedroom`\n",
        "data.loc[ data['total_bedrooms'].isna() ] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtQqi9hOyywG"
      },
      "source": [
        "#### Summary Statistics\n",
        "Next, let's obtain some quick summary statistics.  \n",
        "<font color=green>**EXCEL:** Recall that you can do this using [**Descriptive Statistics**](https://www.excel-easy.com/examples/descriptive-statistics.html) from the **Data Analysis** TookPak, under the **Data** menu.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcjRS80nVBPM",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@markdown  Take a look at summary statistics for numeric attributes\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2glFCOZyywJ"
      },
      "source": [
        "### Histograms and Boxplots for Numeric Features\n",
        "You can often gain a lot more insight about the data by visualizing the entire distribution of values through **histograms** or **boxplots**.\n",
        "\n",
        "<font color=green>**EXCEL:** You can create histograms and boxplots from  **Insert > Insert Statistic Chart** ([click here](https://support.microsoft.com/en-us/office/create-a-histogram-85680173-064b-4024-b39d-80f17ff2f4e8) for detailed instructions on histograms).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7mpx_ETyywL"
      },
      "source": [
        "<font color=red>**When building histograms, always remember to try different values for the number of bins or bin width**.</font><br> \n",
        "This is actually quite critical: a wrong choice can mislead by obscuring important patterns or by creating patterns out of pure randomness. Better packages try to come up with good default choices, but **you should never be over-reliant on automatic approaches, because they depend on particular assumptions about your data**. It is always best to check that whatever pattern you observe remains reasonably consistent as you change the bin number/width. Try changing this parameter below to see how the plots change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieUpdDQvkowS",
        "cellView": "form"
      },
      "source": [
        "#@title **Plot histograms or boxplots for numeric features**\n",
        "\n",
        "#@markdown Select which feature to visualize (picking **\"All\"** will plot histograms for all numeric features):\n",
        "feature = \"median_house_value\" #@param [\"All\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\"]\n",
        "\n",
        "if(feature==\"All\"):\n",
        "  # all features except ocean proximity (which is categorical)\n",
        "  feature = list(data.columns)  \n",
        "  feature.remove(\"ocean_proximity\")\n",
        "else:\n",
        "  feature = [feature]\n",
        "\n",
        "#@markdown Choose what type of plot you want to see:\n",
        "plot_type = \"histogram\" #@param [\"histogram\",\"boxplot\"]\n",
        "\n",
        "#@markdown <br><br>**Feel free to customize some elements of the plot (leave as \"Auto\" if unsure)**\n",
        "\n",
        "#@markdown Choose or type how many figures to display per row:\n",
        "figures_per_row = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figures_per_row==\"Auto\":\n",
        "  figures_per_row=4\n",
        "else:\n",
        "  figures_per_row=np.int(figures_per_row)\n",
        "\n",
        "#@markdown Choose or type how many bins to use for the histogram:\n",
        "num_bins = \"50\" #@param [\"Auto\",5,10,20,50,100]{allow-input: true}\n",
        "if num_bins==\"Auto\":\n",
        "  num_bins=10\n",
        "else:\n",
        "  num_bins=np.int(num_bins)\n",
        "\n",
        "#@markdown Choose or type the width and height for each figure in inches:\n",
        "figure_width = \"4\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=5.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"4\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=5.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "# plot the feature(s)\n",
        "visualize_features(all_data=data, features=feature, \\\n",
        "                   figsize=(figure_width,figure_height), num_plot=plot_type, \\\n",
        "                   num_bins=num_bins, num_cols=figures_per_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvJahKKOyywR"
      },
      "source": [
        "### Visualizing Geographic Data\n",
        "\n",
        "If your data contains numeric attributes with geographic meaning, it may be very useful to try a different visualization  overlaid on top of actual maps. For instance, we can do a scatter plot of `latitude` and `longitude`, and overlay it on a map of California.\n",
        "\n",
        "<font color=green>**EXCEL:** Apart from a scatter plot, you can also insert a Map in Excel from **Insert > 3D Map** ([click here](https://support.microsoft.com/en-us/office/get-started-with-3d-maps-6b56a50d-3c3e-4a9e-a527-eea62a387030) for detailed instructions).</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6-zgzwdyywR",
        "cellView": "form"
      },
      "source": [
        "#@title Scatter plot of `latitude` and `longitude` on top of a map\n",
        "import matplotlib.image as mpimg\n",
        "map_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/California_Locator_Map.PNG/280px-California_Locator_Map.PNG\"\n",
        "california_img = mpimg.imread(map_url)\n",
        "\n",
        "fig = plt.figure(dpi = 100,figsize = (5,5))\n",
        "ax = fig.add_axes([1,1,1,1])\n",
        "\n",
        "data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize = (8,4), ax=ax, alpha=0.1)\n",
        "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5, cmap=plt.get_cmap(\"jet\"))\n",
        "plt.ylabel(\"Latitude\", fontsize=12)\n",
        "plt.xlabel(\"Longitude\", fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSIse2RRF78u"
      },
      "source": [
        "To be able to get useful information from these scatter plots, you may want to play with the transparency of the markers, setting this to a lower value so that you can see areas with agglomerations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUTWELBAyywT"
      },
      "source": [
        "<a id=\"5d\"></a>\n",
        "### Categorical Attributes\n",
        "Recall that our data also has a categorical field, `ocean_proximity`. For such fields, it is good to tabulate the values and create a count plot.\n",
        "\n",
        "<font color=green>**EXCEL:** The most effective way to do this in Excel is by creating a **Pivot Chart** from the **Insert** menu ([click here](https://support.microsoft.com/en-us/office/create-a-pivotchart-c1b1e057-6990-4c38-b52b-8255538e7b1c) for detailed instructions).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkM7UbA7Ggch"
      },
      "source": [
        "You can start by tabulating the values of the feature. This gives information similar to a histogram, recording all the values and their frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk5qgLKIVBPK",
        "scrolled": false,
        "cellView": "form"
      },
      "source": [
        "#@title Tabulate the values of a categorical attribute\n",
        "data[\"ocean_proximity\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX0oqyDByywV"
      },
      "source": [
        "For visualization, countplots can be very effective (they act essentially like histograms)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ahPrfqSwyywV",
        "cellView": "form"
      },
      "source": [
        "#@title Build a histogram / count plot for each categorical attribute\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=data, x=\"ocean_proximity\", orient=\"v\").set_title('Ocean Proximity Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeh5mJMiyywa"
      },
      "source": [
        "<a id=\"5-EDA-Facets\"></a>\n",
        "## **Correlation Analysis** with Google's **Facets Dive**\n",
        "We can again rely on Google PAIR's [Facets](https://pair-code.github.io/facets/) package, which provides an interactive tool called **Facets Dive** for visualizing **relationships** among features and even individual data points. When the dataset is large, it is best to use a **random sample** for the tool, as below.\n",
        "\n",
        "*Run and expand this entire sub-section. No need to understand any of the code - we will only rely on the output.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gECiKXGpyywb",
        "cellView": "form"
      },
      "source": [
        "#@title Set the Number of Data Points to Visualize in Facets Dive\n",
        "\n",
        "SAMPLE_SIZE = 5000 #@param\n",
        "  \n",
        "data_dive = data.sample(SAMPLE_SIZE).to_json(orient='records')\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
        "        <script>\n",
        "          var data = {jsonstr};\n",
        "          document.querySelector(\"#elem\").data = data;\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(jsonstr=data_dive)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mD_b_iQyywd"
      },
      "source": [
        "<a id=\"5B\"></a>\n",
        "## **Correlation Analysis** : Nuts & Bolts\n",
        "For more flexibility and a deeper understanding of the processes, you could also take a step-by-step, do-it-yourself approach.\n",
        "\n",
        "*There are many details here. As you conduct <font color=red>**TASK #2**</font>, you may want to expand some sections here to compare outputs.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsa9KpzlaFvw"
      },
      "source": [
        "#### Correlation Coefficients\n",
        "The starting point is to compute and visualize the **correlation coefficients** between the various **numeric** attributes. \n",
        "\n",
        "<font color=green>**EXCEL:** Recall that you can do this using [**Correlation**](https://www.excel-easy.com/examples/correlation.html) from **Data > Data Analysis**.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjqksnIDaUHy"
      },
      "source": [
        "Since the dataset is not large, we can obtain the entire matrix of correlation coefficients in Python in one shot with the `corr` method, and we can visualize it using the [`heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function in the `seaborn` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k0xPu-2yywe",
        "cellView": "form"
      },
      "source": [
        "#@title Calculate and visualize the entire correlation matrix\n",
        "# compute the matrix\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "# plot the matrix as a heatmap\n",
        "plt.figure(figsize = (10,5));\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h43-hWYQyywk"
      },
      "source": [
        "A critical thing to examine at this step is how each attribute correlates with the target `median_house_value`. You can read that information from above, but it's easier to display a sorted list of all the correlation coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iuA94nhryywk",
        "cellView": "form"
      },
      "source": [
        "#@markdown Display all the sorted correlation coefficients for `median_house_value`\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Gn6JJZkhEV"
      },
      "source": [
        "#### Scatter Plots\n",
        "A very effective way to understand relationships between features is by visualizing them through **scatter plots** combined with color maps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31PZQtmF2Pwq",
        "cellView": "form"
      },
      "source": [
        "#@title Examine the relationships between features\n",
        "\n",
        "# append dummies and store categories for all data (useful for plotting routines)\n",
        "data_with_dummies, data_categories = append_dummies(data, data.columns)\n",
        "\n",
        "#@markdown **Select the features to examine**\n",
        "\n",
        "#@markdown The main feature for which you want to visualize dependencies:\n",
        "main_feature = 'median_house_value' #@param [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\", \"ocean_proximity\"]\n",
        "\n",
        "#@markdown Select another feature:\n",
        "feature_1 = 'households' #@param [\"None\", \"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\", \"ocean_proximity\"]\n",
        "\n",
        "#@markdown (Optional) Select a third feature to use for color-coding:\n",
        "feature_2 = 'None' #@param [\"None\", \"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\", \"ocean_proximity\"]\n",
        "\n",
        "#@markdown <br>**Select the type of plot to see**\n",
        "#@markdown - **scatter** is the usual scatter plot you are familiar with\n",
        "#@markdown - **average_dependency** displays the average value of the **`main_feature`**\n",
        "plot_type = 'scatter' #@param ['average_dependency', 'scatter']\n",
        "\n",
        "#@markdown **You can further customize some aspects of the plot**\n",
        "#@markdown - **`fig_width`** : the width of each figure in inches (leave as **Auto** if unsure)\n",
        "#@markdown - **`fig_height`** : the height of each figure in inches (leave as **Auto** if unsure)\n",
        "#@markdown - **`num_bins`** : the number of bins to use for an **average dependency plot** (leave as **Auto** if unsure)\n",
        "fig_width = '8' #@param ['Auto']{allow-input : true}\n",
        "fig_height = '8' #@param ['Auto']{allow-input : true}\n",
        "num_bins = '10' #@param ['Auto']{allow-input : true}\n",
        "\n",
        "# print the selections\n",
        "print(\"Your selections:\")\n",
        "print(\"Main feature/target :  \", main_feature)\n",
        "print(\"Feature_1 :            \", feature_1)\n",
        "print(\"Feature_2 :            \", feature_2)\n",
        "print(\"Plot type:             \", plot_type)\n",
        "\n",
        "# set up parameters properly\n",
        "if feature_2==\"None\":\n",
        "  feature_2 = None\n",
        "\n",
        "if fig_width=='Auto':\n",
        "  fig_width=7.0\n",
        "else:\n",
        "  fig_width=np.float(fig_width)\n",
        "\n",
        "if fig_height=='Auto':\n",
        "  fig_height=7.0\n",
        "else:\n",
        "  fig_height=np.float(fig_height)\n",
        "\n",
        "if num_bins=='Auto':\n",
        "  num_bins=10\n",
        "else:\n",
        "  num_bins=np.int(num_bins)\n",
        "\n",
        "if plot_type=='average_dependency':\n",
        "    visualize_average_dependency(all_data_with_dummies=data_with_dummies,\\\n",
        "                                  all_categories=data_categories, \\\n",
        "                                  feature_y=main_feature, feature_x1=feature_1,\\\n",
        "                                  feature_x2=feature_2, num_bins=num_bins,\\\n",
        "                                  figsize=(fig_width,fig_height))\n",
        "\n",
        "else:\n",
        "    if plot_type=='categorical':\n",
        "        # check to make sure that either the target or one of the features is numerical\n",
        "        if main_feature in data_categories.keys() and feature in data_categories.keys():\n",
        "            # target and first feature are categorical\n",
        "            if feature_2 in data_categories.keys():\n",
        "              # second feature is also categorical; since the third feature \n",
        "              # is also categorical by choice, all are categorical -> switch to countplot\n",
        "              print('Target and all selected features are categorical. Switching to a countplot of target.')\n",
        "              plot_type = 'count'\n",
        "            else:\n",
        "              # feature_2 is numeric, will switch with feature_1\n",
        "              aux=feature_1\n",
        "              feature_1=feature_2\n",
        "              feature_2=aux\n",
        "\n",
        "    visualize_detailed_dependencies(all_data_with_dummies=data_with_dummies, \\\n",
        "                          all_categories=data_categories, \\\n",
        "                          target=main_feature, \\\n",
        "                          feature=feature_1, \\\n",
        "                          feature_color=feature_2,\\\n",
        "                          feature_breakdown=None,\\\n",
        "                          plot_type=plot_type,\\\n",
        "                          figs_per_row=3, figsize=(fig_width,fig_height))\n",
        "\n",
        "# uncomment next line to save\n",
        "#save_fig(\"scatterplot_of_{}_and_{}\".format(feature1,feature2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9lXUqWGyywo"
      },
      "source": [
        "#### Geographical Features\n",
        "For geographical attributes, we may want to rely on a different visualization by plotting feature values on top of a map.\n",
        "\n",
        "<font color=green>**EXCEL:** Inserting maps in Excel from **Insert > 3D Map** can be **extremely** effective for this (click [here](https://support.microsoft.com/en-us/office/get-started-with-3d-maps-6b56a50d-3c3e-4a9e-a527-eea62a387030) for detailed instructions).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9YyGwE0rJrh"
      },
      "source": [
        "We can plot two different numeric attributes on top of a map of California, with each data point given by a circle whose color captures one feature and whose width captures the other feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oLrBCFm8ar1E",
        "cellView": "form"
      },
      "source": [
        "#@title Visualize two features on a map\n",
        "\n",
        "#@markdown Pick the main feature to visualize using a color map:\n",
        "main_feature = 'median_house_value' #@param [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\"]\n",
        "\n",
        "#@markdown Pick an additional feature to visualize by changing the size of bubbles:\n",
        "extra_feature = 'population' #@param [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\"]\n",
        "\n",
        "#@markdown Optional: you can use **`min_value`** and **`max_value`** to filter which rows are shown based on the value of **`extra_feature`**:<br>\n",
        "#@markdown (leave as **None** if you do not want to filter anything)\n",
        "\n",
        "min_value = None #@param {type:\"raw\"}\n",
        "max_value = None #@param {type:\"raw\"}\n",
        "\n",
        "# adjust min and max if needed\n",
        "minv = data[extra_feature].min()\n",
        "maxv = data[extra_feature].max()\n",
        "\n",
        "if( min_value is None):\n",
        "  min_value = minv\n",
        "elif( min_value > maxv ):\n",
        "  print('Specified min_value of {} is above the maximum value in the data, which is {}. Setting min_value to minimum.'.\\\n",
        "        format(min_value,maxv))\n",
        "  min_value = minv\n",
        "\n",
        "if( max_value is None):\n",
        "  max_value = maxv\n",
        "elif( max_value < minv ):\n",
        "  print('Specified max_value of {} is below the minimum value in the data, which is {}. Setting max_value to maximum.'.\\\n",
        "        format(max_value,minv))\n",
        "  max_value = maxv\n",
        "\n",
        "# filter based on the criterion\n",
        "filtered_data = data[ (data[extra_feature] >= min_value) & (data[extra_feature] <= max_value) ]\n",
        "if( len(filtered_data.index) == 0):\n",
        "  print(\"Filter is too restrictive - there is no data to display. Removing filter.\")\n",
        "  filtered_data = data\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "map_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/California_Locator_Map.PNG/280px-California_Locator_Map.PNG\"\n",
        "california_img = mpimg.imread(map_url)\n",
        "\n",
        "ax = filtered_data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(15,7),\n",
        "                        s=filtered_data[extra_feature]/(filtered_data[extra_feature].median())*50, \\\n",
        "                        label=extra_feature, c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\\\n",
        "                        colorbar=False, alpha=0.2)\n",
        "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], \n",
        "           alpha=0.5, cmap=plt.get_cmap(\"jet\"))\n",
        "plt.ylabel(\"Latitude\", fontsize=14)\n",
        "plt.xlabel(\"Longitude\", fontsize=14)\n",
        "\n",
        "main_feature_vals = filtered_data[main_feature]\n",
        "tick_values = np.linspace(main_feature_vals.min(), main_feature_vals.max(), 11)\n",
        "cbar = plt.colorbar(ticks=tick_values/main_feature_vals.max())\n",
        "cbar.ax.set_yticklabels([\"{:,.1f}\".format(v) for v in tick_values], fontsize=14)\n",
        "cbar.set_label(main_feature, fontsize=16)\n",
        "\n",
        "plt.legend(fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# uncomment next line to save\n",
        "# save_fig(\"california_housing_prices_plot\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sdtj_O_yywq"
      },
      "source": [
        "#### Categorical Features\n",
        "For categorical features we cannot compute correlation coefficients, but we can rely on visualizations to understand the relationships. \n",
        "\n",
        "<font color=green>**EXCEL:** The most effective way to do this in Excel is by creating a **Pivot Chart** from the **Insert** menu ([click here](https://support.microsoft.com/en-us/office/create-a-pivotchart-c1b1e057-6990-4c38-b52b-8255538e7b1c) for detailed instructions).</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uQ4vwV7Dyyws",
        "cellView": "form"
      },
      "source": [
        "#@title Create boxenplots as a function of the categorical feature `ocean_proximity`\n",
        "\n",
        "#@markdown **Select which numeric feature to visualize** (**All** will display all):\n",
        "feature = \"median_house_value\" #@param [\"All\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\"]\n",
        "\n",
        "#@markdown <br>**Feel free to customize some other elements of the plot** (leave as \"Auto\" if unsure)\n",
        "\n",
        "#@markdown Choose or type how many figures to display per row:\n",
        "figures_per_row = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figures_per_row==\"Auto\":\n",
        "  figures_per_row=4\n",
        "else:\n",
        "  figures_per_row=np.int(figures_per_row)\n",
        "\n",
        "#@markdown Choose or type the width and height for each figure in inches:\n",
        "figure_width = \"7\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=7.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"7\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=7.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "if feature==\"All\":\n",
        "  feature = list(data.columns)\n",
        "  feature.remove('ocean_proximity')\n",
        "else:\n",
        "  feature = [feature]\n",
        "\n",
        "num_figs = len(feature)\n",
        "num_columns = figures_per_row\n",
        "num_rows = int(np.ceil(num_figs/num_columns))\n",
        "\n",
        "fig = plt.figure(figsize=(figure_width*num_columns,figure_height*num_rows))\n",
        "\n",
        "for i in range(num_figs):\n",
        "    plt.subplot(num_rows,num_columns,i+1)\n",
        "    sns.boxenplot( x=data[\"ocean_proximity\"], y=data[feature[i]], palette=\"Blues\")  # a boxenplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7x4UjLPyywX"
      },
      "source": [
        "# <font color = red>**TASK #2**</font>\n",
        "<font color=purple>**A.** Conduct the following Exploratory Data Analysis **in EXCEL:**</font>\n",
        "1. Build a **histogram** for **`median_house_value`** and **`housing_median_age`**. \n",
        "2. Use a **filter** to visualize all the samples that have **`total_bedroom`** equal to 1 or missing.\n",
        "  * To create a filter, just select a cell in your data and click on **Data > Filter** (click [here](https://support.microsoft.com/en-us/office/filter-data-in-a-range-or-table-01832226-31b5-4568-8806-38c37dcc180e#:~:text=Select%20any%20cell%20within%20the,filter%20criteria%20and%20select%20OK.) for more details on filters)\n",
        "  * To visualize specific samples, click on the **down arrow** next to the attribute you are interested in, and select a filtering criterion\n",
        "  * Missing values may be recorded in several ways, e.g., as **Blanks**, **N/A**, **n/a**, etc.\n",
        "4. Draw a **scatter plot** of `latitude` and `longitude`. \n",
        "   * You can also try creating a **3D Map** (click [here](https://support.microsoft.com/en-us/office/get-started-with-3d-maps-6b56a50d-3c3e-4a9e-a527-eea62a387030) for instructions), but do not waste more than 3 minutes on it. \n",
        "5. Build a **Pivot Chart** to visualize the distribution of values for `ocean_proximity`.\n",
        "   * From **Insert > PivotChart**, select **PivotChart**\n",
        "   * In the dialog box, make sure the cell range appearing under \"Select a table or range\" properly reflects your dataset\n",
        "   * In the sheet where the PivotChart was created, drag **`ocean_proximity`** to **Axis (Categories)** and to **Values** (click [here]() for a picture)\n",
        "\n",
        "<font color=purple>**B.** Using the information in the Notebook above and in your Excel analysis, answer the following questions:</font>\n",
        "1. **Do you notice any pattern in the values of some of the variables?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are there missing values?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are there unexpectedly high or low values?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do some values occur more or less often than you would expect?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do you see signs of skew?\n",
        "2. **Is the pattern you are observing \"robust\"?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Does it depend on the way in which you are visualizing the data?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is it different than what you might expect? Do you think it represents reality?  \n",
        "3. **Does the pattern have meaningful implications in the given prediction problem?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Could it lead to over-representing or under-representing something?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Could it generate bias in the predicted values?\n",
        "4. **Could using specific variables for the given prediction task raise legal or privacy concerns?**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do some variables directly or indirectly speak to [**protected characteristics**](https://www.equalityhumanrights.com/en/equality-act/protected-characteristics) such as age, gender, race/ethnicity, etc.?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Could it lead to privacy violations?  \n",
        "5. **What can you say about the relationship of variables with the target (`median_house_value`) or other variables**?<br></font>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is there strong correlation?  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are the values related in some other way?   \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *As you answer this question, bear questions 2-4 in mind as well*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBcFbDdJTEvI"
      },
      "source": [
        "# Solution to <font color=red>**TASK#2**</font>\n",
        "\n",
        "Click below to see a few of our insights and some additional observations emerging from this step of EDA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpv-h64BVh9d"
      },
      "source": [
        "#### **`longitude`** and **`latitude`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxmlmsYWVvjm"
      },
      "source": [
        "**Regarding values**\n",
        "- The histogram for each of these features seems to exhibit two peaks. Presumably these correspond to some densely populated areas, but it is hard to tell just from the histograms. A map visualization is more useful here, as it allows identifying locations with a higher density of points: you can see that these correspond to roughly 3 areas, and if you are familiar with California you may even identify these (the Bay Area to the north-west, the Los Angeles and San Diego areas to the south, and a long line east, near Sacramento and Fresno) \n",
        "\n",
        "- The map also reveals areas of California where your dataset has **very few samples**. This may reflect reality (there are fewer households there), but it could also reflect a <font color=black>**bias in data sampling**</font>, e.g., maybe the folks who collected the data never visited those areas! <br> <font color=red>**Always try to ensure that your data is as representative as possible of important classes that may be affected by the prediction.**</font> This is critical with protected characteristics, as it could lead to illegal discrimination. But it is important even with non-protected ones, because it can lead your ML algorithms to perform poorly for those classes. For instance, here, we may get very inaccurate predictions for house values drawn from such areas. \n",
        "\n",
        "- Additionally, some districts in our data also turn out to be very small, i.e., with very few households (we will talk about this more in our discussion **`total_bedrooms`**, **`total_rooms`**, and **`households`**). This means that using the location may generate <font color=red>**potential privacy concerns**</font>, as we may be able to **identify individual houses**. The same issue also arises with other features that may speak to private information, such as **median_income**. A potential way to correct for this is to aggregate such small districts together with other (nearby) districts into a single sample, but that obviously means there will be some loss of information.\n",
        "\n",
        "**Regarding relationships**\n",
        "- The features seem to be weakly correlated with our target **`median_house_value`**: correlation coefficients are -0.05 and -0.14, respectively. However, the scatter plots of house value against these features reveal important patterns, as there seem to be peaks in house values occuring at particular latitudes and longitude. Looking at the map figure makes this more clear: the peaks seem to occur exactly in some of the densely populated urban centers that we identified earlier, specifically the Bay Area in the North and the Los Angeles/San Diego area in the south. This makes intuitive sense, as densely populated areas may also command higher prices (also see the comments below for other features like `households` or `population`). So clearly geographical features are relevant for prices! But this also means that latitude and longitude by themselves may be poor predictors of median house values, but certain features derived from them may be valuable predictors; for instance, we could instead include a categorical variable capturing proximity to major urban areas. \n",
        "\n",
        "- Although the correlation coefficients are weak, `latitude` and `longitude` are also quite strongly related to other variables in our data, such as `median_income`, `households` and `population`. You may have noticed this when comparing the scatter plots of these variables against `latitude` and `longitude` (the location of the peaks was roughly **the same** as what we saw for `median_house_value`), or when plotting the various features on the map. This makes intuitive sense, as we might expect that in real life there is a concentration of wealth as well as population in certain areas, such as in densely populated urban centers. This also means that the information conveyed by the geographical features may already be captured by some of other variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zZjAry5My-a"
      },
      "source": [
        "#### **`housing_median_age`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyNCWA7YNQRH"
      },
      "source": [
        "**Regarding values**\n",
        "- The **`housing_median_age`** variable seems to take a maximum value of 52, and the histogram exhibits a sharp peak exactly at the value 52. This suggests that **the values for this feature may have been capped/censored**, i.e., any values exceeding 52 may have been recorded as 52. Having to work with pre-processed values is quite common and is not problematic in itself, but the context is critical to make a responsible determination. <font color=red>**Always try to understand how your data was collected and pre-processed, and whether that poses a problem.**</font> For instance, in our data we can observe the same censoring happening for our predicted variable **`median_house_value`**, which can be very problematic (read our discussion there). <br>\n",
        " **Note:** You may not have seen the sharp peaks in the histograms if you only used the Facets Tool, which draws a default histogram with 10 bins. This is why **drawing your own histograms is important!**\n",
        "\n",
        "**Regarding relationships**\n",
        "- This attribute seems to be weakly positively correlated with our target **`median_house_value`**: the correlation coefficient is 0.11. This may be a bit surprising, as one may expect newer constructions to be more expensive (holding everything else equal). But in reality, older houses may also be located in more desirable neighborhoods or may have certain design features that newer constructions don't have (e.g., high ceilings, charm), which may push prices higher. So the relationship between age and house value may be mixed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIaf686xENyq"
      },
      "source": [
        "#### **`total_rooms`,`total_bedrooms`,`households`, `population`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuI6XsioENyu"
      },
      "source": [
        "**Regarding values**\n",
        "- The **minimum values of these features are very small**, 1, 2 or 3. Either this is incorrect data or some districts are extremely small. To see what is happening more clearly, it is useful to filter the data based on a condition, as you did in <font color=black>**TASK 2** (A.2)</font>, where you probably noticed that indeed there is a district with just 1 household. This can lead to <font color=red>**potential privacy issues**</font>, because it means that our data may not be sufficiently annonymized, so our ML algorithms may be able to identify and discriminate based on information on unique households. <font color=red>**Always try to check if your data allows identifying individual protected characteristics.**</font> Whether this becomes a legal concern is context-dependent, and we will revisit these issues at length in a future class.\n",
        "\n",
        "- You probably saw in <font color=black>**TASK 2** (A.2) that some districts are missing the value for **`total_bedrooms`**. <font color=red>**Missing or incorrect data**</font> issues occur very often in real-world datasets. Apart from revisiting the collection and recording process, there are a few other things you can do here, which we will discuss briefly in the section on preparing and cleaning your data.\n",
        "\n",
        "- The histograms for all these features have a \"heavy right tail\", i.e., the values extend much farther to the right of the median than to the left. This kind of <font color=red>**skewed data**</font> may make it harder for some ML algorithms to detect patterns, so it may be useful to transform or rescale the features to have more \"bell-shaped\" distributions. This is something you would do at a later stage, when preparing your data.\n",
        "\n",
        "**Regarding relationships**\n",
        "- The target **`median_house_value`**  is weakly positively correlated with **`total_rooms`** and **`total_bedrooms`** (coefficients 0.13 and 0.05, respectively). This probably makes some sense if we expect \"larger houses\" to command larger prices; but remember that these features are recorded as total values **per district**, so it becomes less clear whether the relationship should be one of positive or negative correlation (also see the discussion below, for `households`). It would probably make more sense to consider new features corresponding to **rooms per household** for instance, as those would likely better predict house value. This is something you would usually do at a later stage of the ML pipeline, during **Feature Engineering**.<br>\n",
        "\n",
        "- The target **`median_house_value`**  is very weakly correlated with **`households`** and **`population`** (coefficients 0.07 and -0.02, respectively). Here, it is not clear what one might expect. On the one hand, more densely populated areas such as large urban centers may command higher prices, which also seems to be the case in the map of California that we plotted. However, a larger population **in a district** may not necessarily be correlated with larger house prices (e.g., such neighborhoods may also be dirtier or have a higher crime rate). \n",
        "\n",
        "- These features are **very correlated among themselves**. This makes sense given that they are all related to total quantities per district. Strong correlation between predictive features may be undesirable as it can lead to multi-collinearity and it can make it hard to interpret the effects of individual features. It is good to be aware of these issues, and during the **Feature Engineering** stage you can try to define features that are less related among themselves but still correlated to the target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JHpvSQvWsLZ"
      },
      "source": [
        "#### **`median_income`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZ7YZi8WsLc"
      },
      "source": [
        "**Regarding values**\n",
        "- The values of this variable seem suspiciously small; in particular, it does not seem like the income is expressed in tens of thousands of US dollars, as our data dictionary claims. At this stage, it is probably useful to go back and check with the team that collected the data regarding any **processing or censoring** that may have taken place. For our purposes here, suppose that you are told that the data was scaled and capped at 15 (actually 15.0001) for higher incomes, and at 0.5 (actually 0.4999) for lower incomes.\n",
        "\n",
        "- Somewhat relatedly, it is worth noting here that our numeric attributes have <font color=red>**very different value scales**</font>: some are expressed in thousands, some are less than 15. This can be problematic for certain ML algorithms, but can be easily fixed by re-scaling all the data.\n",
        "\n",
        "- Since our dataset has districts with just one household, using the income may also result in <font color=red>**potential privacy concerns**</font>, as we can identify the income of a specific family. (In fact, this is one of the reasons why income values are censored both below and above for census collection purposes, so as to make it more difficult to identify potential outliers.)\n",
        "\n",
        "**Regarding relationships**\n",
        "- This variable is strongly positively correlated with the **`median_house_value`** target (coefficient 0.69), which can also be clearly seen in a scatter plot. This means that the `median_income` will likely be a good predictor, which also makes some sense in reality.\n",
        "\n",
        "- Although income by itself is not likely a protected characteristic, it may heavily correlate with protected characteristics. So by using the income feature, the house value predictions from our ML algorithms may inadvertently be correlated with such protected characteristics, leading to <font color=red>**potential concerns of bias/discrimination**</font>. Whether that also results in legal or ethical issues is context-dependent, but it is important to be aware of the correlations in order to correct any issues, should they arise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GvxO9UVpc4c"
      },
      "source": [
        "#### **`median_house_value`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPNND93npc4f"
      },
      "source": [
        "**Regarding values**\n",
        "- Our target variable **`median_house_value`** seems to take a maximum value of \\$500,001, and the histogram exhibits a sharp peak exactly at that value. This suggests that **the values for this feature may have been capped/censored**, i.e., any values exceeding \\$500,001 may have been recorded as \\$500,001. We saw this issue arising with the **`housing_median_age`** as well, and all the comments there also apply here.\n",
        "\n",
        "This <font color=red>**censoring or pre-processing is particularly problematic**</font> here, because `median_house_value` is our predicted variable. Our ML algorithms may learn that prices never exceed \\$500,001, but that is certainly not the case in reality, and would simply reflect poor data. You would need to check with your clients or the user of your system to see if this is a problem. If intended use cases require precise predictions beyond \\$500,001, then you have two options: **(a)** collect proper values for the districts whose labels were capped, or **(b)** remove those districts from your data and clearly document that your system is only intended to predict house values below \\$500K."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD6RofwWeDBq"
      },
      "source": [
        "#### **`ocean_proximity`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbCzi1zmpc4g"
      },
      "source": [
        "**Regarding values**\n",
        "- `ocean_proximity` has a **very uneven distribution**: the vast majority of the data is either `<1H OCEAN` or `INLAND`, a few records are `NEAR OCEAN` or `NEAR BAY`, and extremely few records are listed as `ISLAND` (a histogram may actually be deceiving here as it could suggest zero records, so it is better to look at the counts). This may be a case of <font color=red>**biased data sampling**</font>, and it means that your ML algorithms are not likely to perform well for districts on islands. In addition to trying to collect more data or removing those districts altogether, we could also try to re-define the categories, e.g., by aggregating `ISLAND` and `NEAR OCEAN` into a single group.\n",
        "\n",
        "**Regarding relationships**\n",
        "- this feature seems to matter for house values: houses that are on an island seem to have higher value than those that are near the bay/ocean, and houses that are `INLAND` seem to have the lowest value. However, this feature is likely also very correlated with the geographical information in latitude and longitude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOe3bM8Jyyw5"
      },
      "source": [
        "__________\n",
        "<a id=\"5\"></a>\n",
        "# **5. Split the Data into a Training Set (80%) and a Test Set (20%)**\n",
        "The next step is to split our data into two: a **training set** (typically 80% of the data), which we will use to estimate and calibrate our ML algorithms, and a separate **test set** (the remaining 20%), which we will use to assess performance after calibration. This practice is called **out-of-sample testing** or **cross validation**, and it is critical in a responsible evaluation of your system's performance. For those of you seeing it for the first time, it may see bizarre: *why would we voluntarily set data aside and not use it to train our algorithms?!* The reason is that we want to avoid having our algorithms learn to predict really well on our own data but fail miserably on new data -- a phenomenon known as **\"overfitting\"**. This is why from the get-go, we reserve and set aside some data for testing.\n",
        "\n",
        "_**Please run this entire section.** If you would like to understand the issues at a deeper level, you can expand the sections and read through, particularly regarding how to avoid potential pitfalls like **sampling bias**._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhndoFO4xdsF"
      },
      "source": [
        "### An 80-20 Random Split in Excel\n",
        "<font color=green>**EXCEL:** You can do a random split in MS Excel using the [**Random Number Generation**](https://bettersolutions.com/excel/add-ins/analysis-toolpak-random-number-generation.htm) feature from **Data > Data Analysis**. We will do this together in class, but for complete instructions you can expand the hidden cells below and read the first sub-section.</font>\n",
        "\n",
        "*Expand this section for detailed instructions and steps.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPPKq-4qxBX2"
      },
      "source": [
        "1. Use [**Random Number Generation**](https://bettersolutions.com/excel/add-ins/analysis-toolpak-random-number-generation.htm) to produce a uniformly distributed random value for each sample in your data.  \n",
        " * Navigate to **Data > Data Analysis** and select **Random Number Generation**  \n",
        " * Under **Number of Variables**, type 1  \n",
        " * Under **Number of Random Numbers**, type the number of samples in your data (20640 for the California Housing Set)  \n",
        " * Under **Distribution**, choose **Uniform**, and you can leave the **Parameters** to the default values (0 and 1)  \n",
        " * Under **Random Seed**, type a specific value. <font color=red>For our class purposes, use a seed of **123** to ensure that you obtain the same random split in Excel as we use subsequently in our Python code.</font>\n",
        " * Under **Output Range**, select **New Worksheet Ply**, and give the new sheet a good name such as *training_set*.\n",
        " * Hit **Enter** or press **OK**.\n",
        " * Go to the new sheet, where you will see all the random values generated in column A, and insert one additional row just above the values (i.e., row 1). The values should now be stored in cells A2:A20641.\n",
        "\n",
        "2. **Copy** your entire dataset (including the column with indexes for the samples, the header, and all the rows), and **paste** the values in the new sheet where you have the random values, starting with column **B**, i.e., immediately to the right of the column with random values.<br>_In our example, this means copying the data from cells `A1:K20641` or the original sheet and pasting it into cells `B1:L20641` of the new sheet._\n",
        "\n",
        "3. **Add a filter** to the entire table of data in the new sheet.\n",
        " * From **Data**, select **Filter**; you should see buttons with down arrows added to each column. \n",
        " * It may happen that Excel refuses to include column A (with the random values) into the filter together with the new columns. If that is the case, you can always extend the table from **Table Design > Resize Table**.\n",
        "\n",
        "4. Sort the entire table based on the random values in column A.\n",
        " * Click on the down arrow for column **A**, and select a sorting method. _Use **Sort Smallest to Largest** to get the same values as we did._\n",
        "\n",
        "5. Remove the filter and delete column **A**. \n",
        " * Go to **Data** and click on **Filter** again; you should no longer see the buttons with down arrows.\n",
        "\n",
        "6. You now have a dataset that contains exactly the same samples/rows as our original dataset, but with the samples in a permuted order. You can simply keep the first 80% of the rows (i.e., rows 2-16513) in the new sheet you generated, and **cut and paste** the last 20% of the samples (i.e., rows 16514-20641) into a separate sheet for the **test set**. You may also want to copy the headers with the column names into this new sheet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_COgWGwyyw6"
      },
      "source": [
        "### An 80-20 Random Split in Python\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjYVmuqRpsQZ"
      },
      "source": [
        "We will use the `Scikit-Learn` package to automate this, and we will set a \"seed\" for the random number generator (here, the value 42) to ensure that we would get the same train/test split if we repeated the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOzPhbBQVBPk",
        "cellView": "form"
      },
      "source": [
        "#@markdown Create an 80-20 random split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSvIuvokyyw9"
      },
      "source": [
        "Let's have a quick look at the two datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCyRw4a_yyw9",
        "cellView": "form"
      },
      "source": [
        "#@markdown Print a few samples from the top of the training set\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30a3V-8lVBPl",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@markdown Print a few samples from the top of the training set\n",
        "test_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt8Llu1fyyxB"
      },
      "source": [
        "The simple procedure that we described here should work reasonably well if our dataset is large enough, particularly relative to the number of attributes. But in practice, that may not always be the case, and two potential pitfalls could occur:\n",
        "  - When the data is not **very large**, we may run the risk of introducing **sampling bias**, which is a form of **selection bias**. This means that the training or test set may not adequately represent our full data, and can be a very serious issue in practice. It is important to avoid it by using **stratified sampling**, which we discuss next.\n",
        "  - We may want to ensure that even if we append new data to our dataset and do a new split (on the bigger dataset), the current training (respectively, test) set continues to be a part of the future training (respectively, test) set. This can be done by creating a unique ID for each data point and using those IDs to decide if a point goes in the training or test set. We omit further details here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEgXpRvyyyxS"
      },
      "source": [
        "### Use the Same Split as in Excel\n",
        "For the rest of our exercise, we would like to ensure that the results in Python are the same as those obtained in Excel, so we will read the random sample from an Excel file where we used the same random seed of **123**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6AxJwh8VBP8",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@markdown Read the train-test split obtained in Excel\n",
        "url_train = 'https://raw.githubusercontent.com/dan-a-iancu/airm/master/ML_Pipeline/housing_training_set.csv'\n",
        "url_test = 'https://raw.githubusercontent.com/dan-a-iancu/airm/master/ML_Pipeline/housing_test_set.csv'\n",
        "\n",
        "train_set = pd.read_csv(url_train, index_col=0)\n",
        "test_set = pd.read_csv(url_test, index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA9J44kzyHZp",
        "cellView": "form"
      },
      "source": [
        "#@markdown Check the first few samples in the training set\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWUer6pByPWr",
        "cellView": "form"
      },
      "source": [
        "#@markdown Check the first few samples in the test set\n",
        "test_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOZLuIguVBQX"
      },
      "source": [
        "______________\n",
        "# **6. Prepare the Data for ML Algorithms**\n",
        "\n",
        "The next step is to prepare our data to ensure it is compatible with Machine Learning models. This includes several typical steps such as:\n",
        "1. **data cleaning**: dealing with missing or incorrect entries, removing outliers, etc.\n",
        "2. **handling text and categorical variables**: transforming them into a numeric representation\n",
        "3. **feature scaling**: re-scaling some of the numeric features to ensure numeric stability and equal \"importance\".\n",
        "\n",
        "These steps must be applied to both the **training** and the **test** set, with some care to avoid \"data snooping bias\", i.e., using values from the test set to influence the computations. There are several options possible for each step; if you are interested in details, expand the section for more.\n",
        "\n",
        "_**Please run this entire section.** If you would like to see the details, you can expand the sections and read through._\n",
        "\n",
        "<font color=green>**EXCEL:** These steps are relatively straightforward to conduct in Excel as well using a combination of **Filters** and **IF** statements, although this process is a bit harder to automate. We include a few details for each step below, so you can expand the section if you are interested in more.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzz5WDLnOkHh"
      },
      "source": [
        "## **Data Cleaning** : Nuts & Bolts\n",
        "*Expand for details on the options available and an explanation with examples in the context of our dataset.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2st2DprZJMg"
      },
      "source": [
        "Missing values are unfortunately a common occurrence in real-world datasets, and might appear in several forms: as an empty string, as NA, N/A, None, as infeasible values (such as -1 for features with positive values), etc. How to best deal with them depends on the context and the pattern of missing values, but generally there are 3 options:\n",
        "1. **remove all the rows** with missing data\n",
        "2. **remove all the columns/features** with missing data\n",
        "3. **impute missing values**: set missing values to a particular \"well-chosen\" value.\n",
        "\n",
        "We discuss each option briefly below and highlight when it is most appropriate. This is actually a pretty important topic and our treatment here will be limited, but if you want more details and links to relevant methods, check out [this Wikipedia Page](https://en.wikipedia.org/wiki/Missing_data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhnDZeqBJWDW",
        "cellView": "form"
      },
      "source": [
        "#@markdown To show examples using our data, we make copies of the training and test sets so we don't change them subsequently.\n",
        "train_copy = train_set.copy()\n",
        "test_copy = test_set.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC-9H-veCyzK"
      },
      "source": [
        "### Option 1: Remove all the rows with missing data\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhJSAnZHV3lb"
      },
      "source": [
        "This is reasonable if:\n",
        "  - only a few rows have missing values (typically less than 1%), since otherwise, you might be reducing your dataset too much;\n",
        "  - the values are **missing at random**, i.e., they have no relationship with values of other features; otherwise, deleting the rows may miss important patterns in the data.\n",
        "\n",
        "<font color=green>**EXCEL:** You can use a **Filter** to display all the rows where a feature has a missing value, and then you can delete those rows.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H4XAW0DmfTB"
      },
      "source": [
        "To see how this would work in our dataset, let's first see how many entries are missing in our training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tgoW4klEdD",
        "cellView": "form"
      },
      "source": [
        "#@markdown Count the missing entries in our training data\n",
        "train_copy.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "o9wbRQPICyzL",
        "cellView": "form"
      },
      "source": [
        "#@markdown Option 1: use `dropna()` to remove all rows with missing data \n",
        "new_data = train_copy.dropna(subset=[\"total_bedrooms\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_CPm-Z5CyzM"
      },
      "source": [
        "The method `dropna()` does not actually change the dataframe from which it is called (i.e., `train_set`), but instead returns a new dataframe with the changes implemented. This is why we are creating the new variable `new_data` above; this has no more missing values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KIx_E99CyzM",
        "cellView": "form"
      },
      "source": [
        "#@markdown Check the new dataset for missing entries\n",
        "new_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH7YFei5CyzO"
      },
      "source": [
        "### Option 2: Remove all the columns with missing data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc0ccaqsWSy5"
      },
      "source": [
        "This is reasonable for features that have **a high percentage** of missing values (say, in more than 60-70% of the rows). \n",
        "\n",
        "<font color=green>**EXCEL:** You could use a **Filter** to see which features have missing values, and then you can delete all those features/columns.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7pcspOsCyzO",
        "cellView": "form"
      },
      "source": [
        "#@markdown Use `drop()` to remove a feature with missing values \n",
        "new_data = train_copy.drop(\"total_bedrooms\", axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9VlKzYJCyzQ"
      },
      "source": [
        "The method `drop()` leaves the original dataframe unchanged, so we save the result of the transformation in a new dataframe `new_data`. This no longer has the `total_bedrooms` feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYLeESFgCyzQ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Print information on new set\n",
        "new_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYNQnzV9CyzR"
      },
      "source": [
        "### Option 3: Replace missing entries with specific values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZJsdeBeCyzR"
      },
      "source": [
        "This is reasonable in most cases, and we also adopt this approach for our dataset. The precise implementation depends a bit on the context, and there are several options:\n",
        "   - **Mean, Median, Mode imputation.** This involves replacing the missing values with the mean or the median for a continuous variable, and with the mode for a categorical variable. The **mean** is the most common choice, but it is better to use the **median** if the values are skewed or if there are outliers.The approach is fast and easy to execute, but reduces the variance of the data and may introduce some bias.\n",
        "   - **Predictive methods.** This involves predicting the missing values based on other existing features with ML/estimation algorithms such as expectation-maximization, stochastic regression, k-nearest neighbors, [deep neural networks](https://datawig.readthedocs.io/en/latest/#) or [Multivariate Imputation by Chained Equations (MICE)](https://amices.org/mice/). These more sophisticated approaches tend to work better than simple methods, but are also more complex and computationally expensive, and depending on the specific implementation, they may also introduce bias.\n",
        "   - **Encode values as missing and use ML methods that accept missing entries**. Certain Machine Learning methods (most notably, tree-based methods) can actually accept missing data inputs. If you plan on using these and a software implementation that allows it, you can encode the missing values systematically (e.g., as NaN) and then skip any additional imputation. This  is simple and does not introduce bias, but it limits the range of ML algorithms we can rely on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LloIrYmrVGg"
      },
      "source": [
        "### Our implementation explained\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0y9rrMHrpWr"
      },
      "source": [
        "We choose to implement the first method, i.e., impute missing values for our dataset. Since we know from our exploratory data analysis that `total_bedroom` indeed has skewed values, we will use the median **calculated in the training set**. Note that this means that even the missing values in the test set are replaced with the median from the **training set**, which is done in order to avoid any \"data snooping\" bias.\n",
        "\n",
        "Let's display a few samples with missing data in the training and the test set, so you can see the implementation working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY0DBs7UqTOF",
        "cellView": "form"
      },
      "source": [
        "#@markdown Show some samples with missing values in the training set:\n",
        "missing_entries_training = train_copy[\"total_bedrooms\"].isnull()\n",
        "train_copy[missing_entries_training].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prU6_xW-qxgP",
        "cellView": "form"
      },
      "source": [
        "#@markdown Show some samples with missing entries in the test set:\n",
        "missing_entries_test = test_copy[\"total_bedrooms\"].isnull()\n",
        "test_copy[missing_entries_test].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dWu6_e8noIc",
        "cellView": "form"
      },
      "source": [
        "#@markdown Now let's calculate the median **in the training set** and replace all missing values with it\n",
        "median_val = train_copy[\"total_bedrooms\"].median()             # calculate median from the training set\n",
        "print(\"The median value in the training set is {}.\".format(median_val))\n",
        "\n",
        "train_copy[\"total_bedrooms\"].fillna(median_val, inplace=True)  # replace all missing values in training set\n",
        "test_copy[\"total_bedrooms\"].fillna(median_val, inplace=True)    # ... and test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJvdjOZAtQAP"
      },
      "source": [
        "Let's check what happened in both the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "y_HE56EhnoIi",
        "cellView": "form"
      },
      "source": [
        "#@markdown Show the samples as above in the training set:\n",
        "train_copy[missing_entries_training].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6TW4y6TuWho",
        "cellView": "form"
      },
      "source": [
        "#@markdown Show the same samples as above in the test set:\n",
        "test_copy[missing_entries_test].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HEahF6HLyyxX",
        "cellView": "form"
      },
      "source": [
        "#@markdown We can also count the missing entries in the entire train set to check that none remain.\n",
        "train_copy.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMTTuY3gVBRL"
      },
      "source": [
        "## **Handling Text and Categorical Features** : Nuts & Bolts\n",
        "*Expand for details on the options available and an explanation with examples in the context of our dataset.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anmhti4o4QXS"
      },
      "source": [
        "Most Machine Learning algorithms can only work with **numerical data**, so any text or categorical features must be preprocessed and transformed into numerical values. There are usually two ways to do this:\n",
        " - **ordinal encoding**: transform each value of the categorical feature into a different numeric value. This is suitable when the categorical values have a particular ordering, so that values that are closer correspond to more similarity in the feature. For instance, we could encode \"bad\", \"average\", \"good\" and \"excellent\" as 0, 1, 2, 3. This works well when there is a precise and obvious mapping to use, but otherwise may introduce bias in our data.\n",
        " - **dummy variables** (also known as **one-hot-encoding**): this approach is much more general, and involves creating new **binary** features (i.e., with values 0 or 1) for each possible value of a categorical feature, to indicate whether the feature takes that value. For instance, if we had a categorical feature **`child`** that takes values \"girl\" or \"boy\" in all our samples, we would create two binary features called for instance **`girl`** and **`boy`**, and **`girl`** would be equal to 1 in every sample where **`child`** = \"girl\" (and would equal 0 otherwise), whereas **`boy`** would be equal to 1 in every sample where **`child`** = \"boy\" (and would equal 0 otherwise). \n",
        "\n",
        "<font color=green>**EXCEL:** Both options could be implemented in Excel using  **IF** statements possibly complemented with some other transformations (e.g., to handle text). When creating \"dummy variables\"  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yHh2paD_8ZP"
      },
      "source": [
        "### Our implementation explained: create dummy variables for `ocean_proximity`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0fPmyHFTPDB"
      },
      "source": [
        "`ocean_proximity` is our only categorical feature, with 5 possible values: \"$<$1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\".\n",
        "\n",
        "You may be tempted to think that **ordinal encoding** could work here, because the values speak to how close the district is to the ocean, so there should be an ordering. However, it is not at all clear how to do the mapping here. For instance, if we assigned the values 1-5 to the categories in the order above, then districts where `ocean_proximity` takes the value 1 in our encoding (\"$<$1H OCEAN\") would arguably be more similar to districts with value 4 (\"NEAR BAY\") than to districts with value 2 (\"INLAND\"). Additionally, assigning values to maintain what we perceive as proximity might lead to very ad-hoc choices, and might risk biasing our data.\n",
        "\n",
        "In such cases, creating **dummy variables** in preferrable. Since `ocean_proximity` has 5 values, we need to create 5 binary features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IGCXBSDG_rL",
        "cellView": "form"
      },
      "source": [
        "#@markdown Create dummy variables and add them to the data\n",
        "from pandas import get_dummies\n",
        "train_copy.join(get_dummies(train_copy[\"ocean_proximity\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwmCJgN5VBRb"
      },
      "source": [
        "<font color=red>**Multicollinearity warning.**</font> It is important to note that **the sum of all the dummy variables derived from the same feature is always exactly 1** (i.e., for each row of our dataset). Thus, using all dummy variables as predictors is guaranteed to generate multicollinearity. The problem can be easily avoided by omitting to include **exactly one** of the dummy variables, which will then become the \"benchmark\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQdAg7sNCyzh"
      },
      "source": [
        "## **Scaling the Features** : Nuts & Bolts\n",
        "*Expand for details on the options available and an explanation with examples in the context of our dataset.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FOPpjy1Cyzi"
      },
      "source": [
        "The last step is to scale some of the features. This is relevant when some numerical features take very different values (it helps algorithms to avoid giving features with larger values more importance, and it can also significantly improve the numerical stability, which is relevant when working with huge datasets). Several ways to scale are used in practice, and the most common are:\n",
        "1. **Min-max scaling**: scale each feature so it takes values between 0 and 1, by subtracting the minimum value of the feature and dividing by the max minus the min.\n",
        "2. **Standardization**: subtract the mean value from each feature and divide the result by the standard deviation of the feature.\n",
        "\n",
        "In practice, several other scaling methods exist that may perform better (e.g., by reducing the impact of outliers), but no approach is really without flaws. If you want to read more about the various options and how they compare, check out [this detailed SciKit post](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py), which actually relies on the same dataset on California House prices.\n",
        "\n",
        "<font color=green>**EXCEL:** All options could be implemented in Excel using standard functions. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzoiqdE0iNjo"
      },
      "source": [
        "### Option 1: Min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wpMJqOUj9UV"
      },
      "source": [
        "#@markdown Perform min-max scaling on the `total_rooms` feature \n",
        "min_val = train_copy[\"total_rooms\"].min()  # calculate the minimum value\n",
        "max_val = train_copy[\"total_rooms\"].max()  # and the max value\n",
        "total_rooms_mm_scaled = (train_copy[\"total_rooms\"]-min_val)/(max_val-min_val)   # scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STpePYnfEZtt"
      },
      "source": [
        "#@markdown Plot two histograms to compare the effect of min-max scaling\n",
        "plt.figure(figsize=(14,5)) \n",
        "\n",
        "# original unscaled data\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(train_copy[\"total_rooms\"], bins=50)\n",
        "plt.title( \"Histogram of original `total_rooms` feature\")\n",
        "\n",
        "# min-max scaling / normalization\n",
        "plt.subplot(1,2,2) \n",
        "plt.hist(total_rooms_mm_scaled, bins=50)   # histogram with 50 bins\n",
        "plt.title( \"Histogram of `total_rooms` with min-max scaling\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T1f-Erxi44U"
      },
      "source": [
        "### Option 2: Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn_75pcNFz1N"
      },
      "source": [
        "#@markdown Perform standardization on the `total_rooms` feature \n",
        "feature_mean = train_copy[\"total_rooms\"].mean()\n",
        "feature_stdev = train_copy[\"total_rooms\"].std()\n",
        "total_rooms_standardized = (train_copy[\"total_rooms\"]-feature_mean)/feature_stdev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTFkOIqFz1W"
      },
      "source": [
        "#@markdown Plot two histograms to compare the effect of standarization\n",
        "plt.figure(figsize=(14,5)) \n",
        "\n",
        "# original unscaled data\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(train_copy[\"total_rooms\"], bins=50)\n",
        "plt.title( \"Histogram of original `total_rooms` feature\")\n",
        "\n",
        "# min-max scaling / normalization\n",
        "plt.subplot(1,2,2) \n",
        "plt.hist(total_rooms_standardized, bins=50)   # histogram with 50 bins\n",
        "plt.title( \"Histogram of `total_rooms` with standardization\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdOzYZMDyyxW"
      },
      "source": [
        "## **All steps** applied in our dataset\n",
        "\n",
        "*Expand to see the steps we follow for our dataset.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdh5YCClQUTP"
      },
      "source": [
        "We implement the following choices for our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MEMR7I1yyxY",
        "cellView": "form"
      },
      "source": [
        "#@title 1. **Data cleaning**\n",
        "\n",
        "#@markdown We replace all missing values of `total_bedroom` with the **median value** calculated in the <font color=red>**training set**</font>.\n",
        "median_val = train_set[\"total_bedrooms\"].median()             # calculate median from the training set\n",
        "train_set[\"total_bedrooms\"].fillna(median_val, inplace=True)  # replace all missing values in training set\n",
        "test_set[\"total_bedrooms\"].fillna(median_val, inplace=True)    # ... and test set\n",
        "\n",
        "# make a copy of the training and testing here, before defining the categorical features\n",
        "train_copy = train_set.copy()\n",
        "test_copy = test_set.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydJhE10Lyyxd",
        "cellView": "form"
      },
      "source": [
        "#@title 2. **Handling Text and Categorical Features**\n",
        "\n",
        "#@markdown We transform the categorical feature `ocean_proximity` into binary indicators for each value (\"dummy variables\")\n",
        "\n",
        "# add dummy variables for \"ocean_proximity\" to both training and test sets \n",
        "train_set = train_set.join(pd.get_dummies(train_set[\"ocean_proximity\"]))\n",
        "test_set = test_set.join(pd.get_dummies(test_set[\"ocean_proximity\"]))\n",
        "\n",
        "# remove the original categorical 'ocean_proximity' feature, as it's no longer needed\n",
        "train_set = train_set.drop(columns=[\"ocean_proximity\"])\n",
        "test_set = test_set.drop(columns=[\"ocean_proximity\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6M8LhO8PQUy",
        "cellView": "form"
      },
      "source": [
        "#@title 3. **Feature scaling**\n",
        "\n",
        "#@markdown We do **not** re-scale any features since we will be using ML methods that are not sensitive to scaling.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b384uuARyyxg",
        "cellView": "form"
      },
      "source": [
        "#@markdown Let's separate the predicted variable from the rest of the features\n",
        "X_train = train_set.drop(\"median_house_value\", axis=1)  # drop the target for the training set    \n",
        "y_train = train_set[\"median_house_value\"]\n",
        "\n",
        "X_test = test_set.drop(\"median_house_value\", axis=1)  # drop the target for the test set \n",
        "y_test = test_set[\"median_house_value\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "W4ZXmg91MJ9o"
      },
      "source": [
        "#@markdown We remove the **`INLAND`** dummy variable to avoid multicollinearity\n",
        "X_train.drop(\"INLAND\", axis=1, inplace=True)\n",
        "X_test.drop(\"INLAND\", axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYWlfTtLajY"
      },
      "source": [
        "# <font color = red>**TASK #3**</font>\n",
        "<font color=purple>**Conduct the following Analysis in EXCEL:**</font>\n",
        "1. Create a **random split** of your dataset into 80% for training and 20% for testing, using a **seed of 123**.\n",
        "2. Replace all the missing values of `total_bedrooms` with the median **calculated in the training set**. \n",
        "3. Transform the categorical variable `ocean_proximity` into **dummy variables** and delete the column corresponding to `INLAND`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WENSYHnvyyxi"
      },
      "source": [
        "# **7. Train the Machine Learning Model**\n",
        "The next step is to train machine learning models using the training data. \n",
        "We will train (and subsequently test) three different ML models: a **linear model**, a **decision tree**, and a **random forest**.\n",
        "\n",
        "_**Run this entire section.** We will be replicating some parts of the analysis in Excel, so we suggest expanding the hidden cells and then choosing which sub-section to expand further._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDR1145E3Dtw"
      },
      "source": [
        "## Our Three ML Models in a Nutshell \n",
        "_If you have never seen some of these models, you may want to uncomment this and read for a very quick description._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWOtZnsaMEx"
      },
      "source": [
        "The three models that we are training can be quickly summarized as follows:\n",
        "* In a **linear model**, the predicted value of the target is given by a linear function of the predictors $X_1,\\dots,X_m$. That is:\n",
        "> $\\beta_0 + \\beta_1 \\cdot X_1 + \\dots + \\beta_m \\cdot X_m,$\n",
        "\n",
        " where $\\beta_0$ is a constant term (intercept), and $\\beta_1,\\dots,\\beta_m$ are the coefficients that determine the dependency.\n",
        "\n",
        "* In a **Decision Tree** (or more precisely, a **Regression Tree** in our case), the predicted value is given by the **average** of the target values for all the samples that satisfy a nested sequence of **IF/THEN** statements. These nested **IF/THEN** statements can be visualized on a tree, which gives the method its name. If you have never seen this before, it will become very clear once we actually visualize a tree. (For details, you can check out [this](https://www.youtube.com/watch?v=g9c66TUylZ4) video tutorial).\n",
        "\n",
        "* A **Random Forest** is just an ensemble of many decision trees. The predicted value is given by the **average** of the predictions of all trees. Random Forests were invented by Leo Breiman, and are based on a few smart tweaks that are aimed to make the trees in the ensemble \"work well\" together. (For details, check out [this](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&ab_channel=StatQuestwithJoshStarmer) video tutorial.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiuauOI1gthF"
      },
      "source": [
        "## **Train** or **Fit** the Models\n",
        "This is the critical step where each ML model is **fitted** (or **trained**) using the data in our **training set**. \n",
        "\n",
        "_**Run the entire section.** We recommend expanding this and reading the sub-section that describes **what** this process actually means, with examples focused on the three ML models we are considering._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU7yaTCs05TD"
      },
      "source": [
        "### What Does it Mean to **Fit** (or **Train** or **Estimate**) an ML Model? \n",
        "*Expand and read this if you would like to understand more about what actually happens when we train an ML algorithm.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otm9CGcEum9W"
      },
      "source": [
        "To make the notions precise, it is helpful to define a tiny bit of **notation**.  \n",
        "> Suppose we denote the value of the **target** with $y$, and the value of the **predictors** with $X_1, X_2,\\dots,X_m$. For instance, in our case, $y$ would be the `median_house_value`, and $X_1$ is the value of `longitude`, $X_2$ is the value of `latitude`, etc. Since these take different values in each of the samples in our data, we would like to have a way to refer to the values in each sample. So if we use an index $i$ to refer to the $i$-th sample, we will denote by $y^{(i)}$ the value of the target in the $i$-th sample, and with $X_1^{(i)}, X_2^{(i)},\\dots,X_m^{(i)}$ the values of the predictors in the $i$-th sample. Lastly, let's assume that the **training set** consists of the $N$ samples indexed $1,2,\\dots,N$.\n",
        "\n",
        "**What <font color=red>is</font> a predictive ML model, and what does it actually <font color=red>do</font> when predicting?<br>**\n",
        "> Each of the ML models we work with has a set of internal **parameters** $\\beta$ that characterize it, and a certain **functional way** $F(\\cdot)$ of predicting the values of the target given values of the predictors and parameters. Here, $F(\\cdot)$ is really just a function that is specific to the kind of ML model we use, and that determines **how predictions are done**: given the internal parameters $\\beta$ and predictors with values $X_1, X_2, \\dots, X_m$, our ML model would **predict a value** $\\hat{y} = F(\\beta, X_1, X_2,\\dots,X_m)$ for the target.\n",
        "\n",
        "**Some Specific Examples.**\n",
        "* In a **linear model**, the predicted value of the target $\\hat{y}$ is given by a linear function of the predictors $X_1,\\dots,X_m$:\n",
        "> $ \\hat{y} = \\beta_0 + \\beta_1 \\cdot X_1 + \\dots + \\beta_m \\cdot X_m.$\n",
        "\n",
        " So here, the **internal parameters** are the intercept $\\beta_0$ and the coefficients $\\beta_1,\\dots,\\beta_m$, and the function $F(\\cdot)$ is the linear mapping above.\n",
        "\n",
        "* In a **decision tree**, the predicted value $\\hat{y}$ is given by a nested sequence of **IF/THEN** statements on the predictors, of the form:\n",
        "> $ \\hat{y} = \\texttt{IF}( X_{p_1} \\leq \\beta_{p_1}, \\texttt{IF} (X_{p_2} \\leq \\beta_{p_2}, (\\dots), ) \\dots ).$\n",
        "\n",
        " Here, $X_{p_1}, X_{p_2}$, etc. denote the predictor used in each node of the tree (chosen from the predictors $X_1,\\dots,X_m$, with the same predictor possibly occuring several times), and the coefficients $\\beta_{p_1}, \\beta_{p_2}$, etc. determine the splits in each node of the tree. The **internal parameters** are the choice of predictor to use at each node of the tree ($p_1,p_2$, etc.) and the corresponding values $\\beta_{p_1}, \\beta_{p_2}$. (In addition to these, there are also several so-called \"hyper-parameters\" that control other aspects of the tree, such as how deep the tree can become.) The function $F(\\cdot)$ is given by the nested sequence of **IF/THEN** statements.\n",
        "\n",
        "* A **random forest** is simply a collection/ensemble of several trees, so the **internal parameters** correspond to all the internal parameters we described above for each tree. (In addition, there are also hyper-parameters that determine, e.g., how many trees to use.) The function $F(\\cdot)$ for a random forest is given by the average of the functions $F(\\cdot)$ for the trees in the forest.\n",
        "\n",
        "**What does it mean to <font color=red>train</font> a predictive ML algorithm?**<br>\n",
        "> When we **estimate** (or **fit** or **train**) an ML model, we are effectively trying to find values for the internal parameters $\\beta$ of the model so that the predictions are \"as accurate as possible\" in the training set. More formally, we are solving an **optimization problem** that seeks values for the parameters $\\beta$ that would **minimize the prediction error calculated in the training set**.\n",
        "\n",
        "**What <font color=red>is</font> the prediction error and how do we <font color=red>measure</font> it?**<br>\n",
        "> There are several ways to quantify/measure the prediction error in regression tasks. By far the most popular way is to use the **mean squared error**, but it is also possible to use **mean absolute error** or even some other custom error metric. <br>\n",
        " To understand these, let us denote with $\\hat{y}^{(i)} = F(\\beta,X_1^{(i)}, X_2^{(i)},\\dots,X_m^{(i)})$ the prediction that our algorithm would make for each sample $i$ in our training set. Then, the two ways to measure error above are:\n",
        " * the **mean squared error**: $\\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - \\hat{y}^{(i)})^2 $\n",
        " * the **mean absolute error**: $\\frac{1}{N}\\sum_{i=1}^N |y^{(i)} - \\hat{y}^{(i)}| $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dge8dygVW_y6"
      },
      "source": [
        "### Model #1: A **Linear Regression**\n",
        "\n",
        "<font color=green>**EXCEL:** As you already know, this can be done in Excel using the `LINEST()` function or the **Regression** function under **Data > Data Analysis**.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A13MQojMnKpW"
      },
      "source": [
        "Recall that in a **linear model**, the predicted value of the target $\\hat{y}$ is given by a linear function of the predictors $X_1,\\dots,X_m$, that is:\n",
        "> $ \\hat{y} = \\beta_0 + \\beta_1 \\cdot X_1 + \\dots + \\beta_m \\cdot X_m,$\n",
        "\n",
        "where $\\beta_0$ is an intercept, and $\\beta_1,\\dots,\\beta_m$ are the coefficients that determine the dependency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeaN3jtLW1EB",
        "cellView": "form"
      },
      "source": [
        "#@title Train a **Linear** Model\n",
        "from sklearn import linear_model\n",
        "\n",
        "# fit the model\n",
        "linear_model = linear_model.LinearRegression().fit(X_train,y_train)\n",
        "\n",
        "#@markdown Visualize the results of the training by displaying the coefficients\n",
        "print(\"{:25s}: {:,.2f}\".format(\"Intercept:\",linear_model.intercept_))\n",
        "for k in range(len(X_train.columns)):\n",
        "  print(\"{:25s}: {:,.2f}\".format(X_train.columns[k],linear_model.coef_[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO0vJVnQyyxn"
      },
      "source": [
        "### Model #2: A **Decision Tree**\n",
        "\n",
        "<font color=green>**EXCEL:** Unfortunately there is no built-in function for this in Excel (yet), but many add-ins are available, e.g., [Analytic Solver](https://www.solver.com/xlminer-data-mining).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY92Af0vyyxo"
      },
      "source": [
        "With more complex ML models, there are always a few **\"hyper-parameters\"** we can set when constructing the model. These typically control how complex the model is allowed to become, with more complex models fitting the data better, but requiring more computational time.\n",
        "\n",
        "For instance, when creating a regression tree we can specify a **maximum depth** that controls how many layers the tree can have. Larger trees fit the training data better, but take longer time to fit and are harder to visualize. (There are several other \"hyper-parameters\" that can be set as well, but we will not discuss them for simplicity; see [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) for more details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9ncQFPHyyyxn",
        "cellView": "form"
      },
      "source": [
        "#@title Train a **Decision Tree**\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "#@markdown Select or type the **maximum depth** depth your tree is allowed to have (selecting **`None`** will allow any depth):\n",
        "max_depth = \"3\" #@param [1, 2, 3, 4, 5, 6, \"None\"] {allow-input: true}\n",
        "\n",
        "if max_depth == \"None\":\n",
        "  max_depth = None\n",
        "else:\n",
        "  max_depth = np.int(max_depth)\n",
        "\n",
        "tree_model = DecisionTreeRegressor(criterion='mse', max_depth=max_depth, \\\n",
        "                                   random_state=123)\n",
        "aux = tree_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POh3UorsRQDg"
      },
      "source": [
        "As the name suggests, these models can be conveniently visualized on a tree. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7-xf6Glqyyxp",
        "cellView": "form"
      },
      "source": [
        "#@title **Visualize** the fitted tree\n",
        "from sklearn import tree\n",
        "\n",
        "#@markdown You can adjust how many levels to display with a **maximum depth**. A large depth may generate a picture that is harder to see. \n",
        "\n",
        "#@markdown Select or type the depth you want to visualize (selecting **`None`** will try printing the full tree):\n",
        "max_depth_to_see = \"4\" #@param [1, 2, 3, 4, 5, 6, \"None\"] {allow-input: true}\n",
        "\n",
        "#@markdown The **LEFT** branch corresponds to the **IF** condition being **TRUE**, and the **RIGHT** branch corresponds to it being **FALSE**.\n",
        "\n",
        "if max_depth_to_see == \"None\":\n",
        "  max_depth_to_see = None\n",
        "else:\n",
        "  max_depth_to_see = np.int(max_depth_to_see)\n",
        "\n",
        "dot_data = tree.export_graphviz(tree_model, max_depth=max_depth_to_see, \\\n",
        "                                feature_names=X_train.columns, class_names=None,\n",
        "                                label='all', filled=True, impurity=True, node_ids=True, \\\n",
        "                                rounded=True, rotate=False, proportion=False)\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc2idoVJyyxq"
      },
      "source": [
        "When interpreting the tree above, the main thing to note is the logical condition in each node. This is the **split condition**, and it always involves one of the features in our model. The two subtrees correspond to the two possible outcomes: **the left subtree for the condition being true**, and the **right subtree for the condition being false**. The **color** of each node is indicative of the average value of all the samples that fall in that node: the darker the color is, the higher the value.\n",
        "\n",
        "When the tree is **trained/fitted**, the ML algorithm determines **which feature to use at each node** and **what threshold to use for the split**, and its goal in doing this is to reduce the prediction error in the entire training set.\n",
        "\n",
        "**Predictions** for a new sample are done by following all the conditions based on the predictors until a final leaf is reached, and taking the **average** of the target variable for all the training samples in that leaf node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A4qX8C2FUGE"
      },
      "source": [
        "### Model #3: A **Random Forest**\n",
        "\n",
        "<font color=green>**EXCEL:** Unfortunately there is no built-in function for this in Excel (yet), but many add-ins are available, e.g., [Analytic Solver](https://www.solver.com/xlminer-data-mining).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYVA-yRDLpDG"
      },
      "source": [
        "Just as in the case of the decision tree, there are several \"hyper-parameters\" we can specify when training a random forest. For instance, an important one is the **number of trees** (or estimators) in the forest ensemble, and we can also give a **maximum depth** for each of the trees. Forests with more or deeper trees will fit data better, but also require more time to train. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6phTnolUFUGH",
        "cellView": "form"
      },
      "source": [
        "#@title Train a **Random Forest** with 50 Trees\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#@markdown Select the number of trees (estimators) to use:\n",
        "n_estimators = 20 #@param [5, 10, 20, 30, 40, 50]\n",
        "\n",
        "#@markdown Select or type the depth you want to use (selecting **`None`** will allow any depth):\n",
        "max_depth = \"5\" #@param [1, 2, 3, 4, 5, 6, \"None\"] {allow-input: true}\n",
        "\n",
        "if max_depth == \"None\":\n",
        "  max_depth = None\n",
        "else:\n",
        "  max_depth = np.int(max_depth)\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                 random_state=42, oob_score=True)\n",
        "aux = rf_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgzR8K-y4lG4"
      },
      "source": [
        "Since a random forest consists of many trees, it becomes significantly harder to visualize what the overall trained model looks like. One thing we could do is to plot a few of the trees in the ensemble, but these can actually look very different from each other! Random forests are actually an example of a **black-box** model, and as you can already see, it can be **hard to visualize** such models and **very hard to interpret** concisely how they arrive at their predictions **or to understand the impact of a feature** on the predicted values. We will devote the last section of our tutorial to discussing a set of practical tools that can help de-mistify the workings of such models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_TOSdGyyxq"
      },
      "source": [
        "# **8. Evaluate and Interpret the Model**\n",
        "The next step is perhaps one of the most important ones: actually evaluating and understanding our ML model. There are many important issues to bear in mind when it comes to this, and an exhaustive discussion would be difficult. But any responsible implementation should consider the following questions:\n",
        "\n",
        "1. **How accurate is the model?**  _(How well can it fit the data the **training set** and, more importantly, predict in the **test set**?)_\n",
        "\n",
        "2. **Is the model exhibiting any bias?** _(Does the quality of the prediction change with certain features or categories?)_\n",
        "\n",
        "3. **Can we interpret/explain the model?**  _(Can we **concisely** explain **how** it reaches its predictions and quantify the **impact** of features?)_\n",
        "\n",
        "This section tries to provide some processes and tools to answer such questions.\n",
        "\n",
        "_**Run this entire section.** We will be replicating some parts of the analysis in Excel, so we suggest expanding the hidden cells and then choosing which sub-section to expand further._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veYk5xv7NdUF"
      },
      "source": [
        "## Evaluating the **Predictive Accuracy**\n",
        "\n",
        "The first step is to evaluate the ML model at the task it is actually supposed to achieve, i.e., **prediction**. Specific things to look at are:\n",
        "* How well is the model predicting in the **training set**? _(Is it good at fitting the data it was trained on...)_  \n",
        "* Is the model predicting \"well\" under other performance metrics? _(We may want to calculate a few different metrics of accuracy.)_  \n",
        "* How well is the model predicting in the **test set**? _(The most critical test of all, as this is **new** data that the algorithm hasn't seen before.)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7nLU_6vYg0O",
        "cellView": "form"
      },
      "source": [
        "#@markdown Define a function to calculate and display useful performance metrics\n",
        "def calculate_performance_metrics(label_dataset, y_true, y_pred, display=False):\n",
        "\n",
        "    mse = metrics.mean_squared_error(y_true, y_pred) \n",
        "    rmse = np.sqrt(mse) \n",
        "    r2 = metrics.r2_score(y_true, y_pred)\n",
        "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    if display:\n",
        "      print(\"Results in the {} Set:\".format(label_dataset))\n",
        "      print('MSE:       ', round(mse,4))\n",
        "      print('RMSE:      ', round(rmse,4))\n",
        "      print('R-squared: ', round(r2,4))\n",
        "      print('MAE:       ', round(mae,4))\n",
        "    \n",
        "    return mse, rmse, r2, mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jedkr34Jyyxq"
      },
      "source": [
        "### Predictions in the **Training Set**\n",
        "The first thing is to see how well our ML models are able to fit the data that was used to train them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uXdWYNcyyxr",
        "cellView": "form"
      },
      "source": [
        "#@title Predict with each method for all the samples\n",
        "\n",
        "# predict with each method\n",
        "yhat_linear_train = linear_model.predict(X_train)\n",
        "yhat_tree_train = tree_model.predict(X_train)\n",
        "yhat_rf_train = rf_model.predict(X_train)\n",
        "\n",
        "# join all results into a single dataframe, including the training data, predictions and errors\n",
        "results_train = pd.concat([train_copy, \\\n",
        "                           pd.DataFrame( {'linear_prediction' : yhat_linear_train, \\\n",
        "                                          'tree_prediction' : yhat_tree_train, \\\n",
        "                                          'rf_prediction' : yhat_rf_train, \\\n",
        "                                          'linear_error(%)' : 100*(1.0-yhat_linear_train/y_train), \\\n",
        "                                          'tree_error(%)' : 100*(1.0-yhat_tree_train/y_train), \\\n",
        "                                          'rf_error(%)' : 100*(1.0-yhat_rf_train/y_train)}, \\\n",
        "                                        index = X_train.index ) ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hp3YpJSYD0H",
        "cellView": "form"
      },
      "source": [
        "#@title Display a few samples to compare the methods\n",
        "\n",
        "#@markdown Select or type how many samples to display:\n",
        "num_samp = \"9\" #@param [5, 10, 20] {allow-input:true}\n",
        "\n",
        "results_train.head(np.int(num_samp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qITcdu88yyxv"
      },
      "source": [
        "Next, we can evaluate and display a few different performance metrics in the **entire training set**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZNngFuQdyyxv",
        "cellView": "form"
      },
      "source": [
        "#@title Calculate and display the performance in the entire **training set**\n",
        "\n",
        "#@markdown Let's calculate and display the mean squared error (**MSE**), root mean squared error (**RMSE**), R-squared (**R2**), and mean absolute error (**MAE**).\n",
        "\n",
        "pf_index = [\"MSE\", \"RMSE\", \"R2\", \"MAE\"]\n",
        "linear_metrics = calculate_performance_metrics(\"Training\", y_train, yhat_linear_train, display=False)\n",
        "tree_metrics = calculate_performance_metrics(\"Training\", y_train, yhat_tree_train, display=False)\n",
        "rf_metrics = calculate_performance_metrics(\"Training\", y_train, yhat_rf_train, display=False)\n",
        "\n",
        "# join all dataframes into one\n",
        "summary_train_metrics = pd.DataFrame({\"linear_model\" : linear_metrics, \\\n",
        "                                      \"tree_model\" : tree_metrics, \\\n",
        "                                      \"rf_model\" : rf_metrics}, index=pf_index )\n",
        "\n",
        "# display all performance metrics \n",
        "summary_train_metrics.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v13PC2Wyyxx"
      },
      "source": [
        "### Predictions in the **Test Set**\n",
        "A critical step in the evaluation process is to assess performance in the **test set**, which contains data that is new to our ML algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewr0SCXgyyxx"
      },
      "source": [
        "We can apply the same steps that we took earlier for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGR2PreuPlsG",
        "cellView": "form"
      },
      "source": [
        "#@title Predict with each method for all the samples\n",
        "\n",
        "# predict with each method and save the results as a dataframe\n",
        "# predict with each method\n",
        "yhat_linear_test = linear_model.predict(X_test)\n",
        "yhat_tree_test = tree_model.predict(X_test)\n",
        "yhat_rf_test = rf_model.predict(X_test)\n",
        "\n",
        "# join all results into a single dataframe, including the training data, predictions and errors\n",
        "results_test = pd.concat([test_copy, \\\n",
        "                          pd.DataFrame( {'linear_prediction' : yhat_linear_test, \\\n",
        "                                         'tree_prediction' : yhat_tree_test, \\\n",
        "                                         'rf_prediction' : yhat_rf_test, \\\n",
        "                                         'linear_error(%)' : 100*(1.0-yhat_linear_test/y_test), \\\n",
        "                                         'tree_error(%)' : 100*(1.0-yhat_tree_test/y_test), \\\n",
        "                                         'rf_error(%)' : 100*(1.0-yhat_rf_test/y_test)}, \\\n",
        "                                       index = X_test.index ) ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4XVCox0XqRc",
        "cellView": "form"
      },
      "source": [
        "#@title Display a few samples to compare the methods\n",
        "\n",
        "#@markdown Select or type how many samples to display:\n",
        "num_samp = \"8\" #@param [5, 10, 20] {allow-input:true}\n",
        "\n",
        "results_test.head(np.int(num_samp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm1j7gYPlsK"
      },
      "source": [
        "And we can re-calculate all the performance metrics in the **entire test set**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cQMzgfoNPlsL",
        "cellView": "form"
      },
      "source": [
        "#@title Calculate and display the performance in the entire **test set**\n",
        "\n",
        "#@markdown We calculate and display the mean squared error (**MSE**), root mean squared error (**RMSE**), R-squared (**R2**), and mean absolute error (**MAE**).\n",
        "\n",
        "pf_index = [\"MSE\", \"RMSE\", \"R2\", \"MAE\"]\n",
        "linear_metrics = pd.DataFrame(calculate_performance_metrics(\"Test\", y_test, yhat_linear_test, display=False), \\\n",
        "                              index=pf_index, columns=['linear_model'])\n",
        "tree_metrics = pd.DataFrame(calculate_performance_metrics(\"Test\", y_test, yhat_tree_test, display=False), \\\n",
        "                            index=pf_index, columns=['tree_model'])\n",
        "rf_metrics = pd.DataFrame(calculate_performance_metrics(\"Test\", y_test, yhat_rf_test, display=False), \\\n",
        "                          index=pf_index, columns=['rf_model'])\n",
        "\n",
        "# join all dataframes into one\n",
        "summary_test_metrics = pd.concat([linear_metrics, tree_metrics, rf_metrics], axis=1)\n",
        "\n",
        "# display all performance metrics \n",
        "summary_test_metrics.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjvVHNgFyyxy"
      },
      "source": [
        "It is important to **compare** the performance in the **training** and the **test** data. If you are seeing a big degradation in the performance on the test set compared to the training set, it is likely because the model is **overfitting** the training data. Loosely speaking, overfitting occurs when the model has a lot of degrees of freedom and it uses these to perfectly match the training set. But this also means the model ends up fitting a lot of the noise in the training data, so it fails to perform well on new data (such as in the test set). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj75pA48yyx0"
      },
      "source": [
        "### Model Complexity and Overfitting\n",
        "To help you understand overfitting, we will try a small exercise below: we will systematically change the `max_depth` parameter that controls how deep our decision tree model can grow, train a tree for each depth level, and record its predictive accuracy in both the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SyRX742Hyyx1",
        "cellView": "form"
      },
      "source": [
        "#@title Train trees with depth values from 2 to 30 and plot their performance\n",
        "max_depth_values = np.arange(2,30,1)\n",
        "training_performance = np.zeros(len(max_depth_values))\n",
        "test_performance = np.zeros(len(max_depth_values))\n",
        "\n",
        "for i in range(len(max_depth_values)):\n",
        "    # train a new decision tree model with max_depth = i\n",
        "    new_tree_model = DecisionTreeRegressor(criterion='mse', max_depth=max_depth_values[i], random_state=123)\n",
        "    new_tree_model.fit(X_train, y_train)\n",
        "    \n",
        "    # record the MSE in training and test set\n",
        "    training_performance[i] = metrics.mean_squared_error(y_train, new_tree_model.predict(X_train))\n",
        "    test_performance[i] = metrics.mean_squared_error(y_test, new_tree_model.predict(X_test))\n",
        "\n",
        "# plot the performance as a function of max_depth\n",
        "fig = plt.figure(figsize=(15,6))  # create a figure of the desired size\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(x=max_depth_values, y=training_performance, label=\"training set\")   # a simple scatter plot\n",
        "plt.scatter(x=max_depth_values, y=test_performance, label=\"test set\")   # a simple scatter plot\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title( \"Performance in Training and Test Set\")\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.title( \"Performance in Test Set\")\n",
        "plt.scatter(x=max_depth_values, y=test_performance, label=\"test set\", color='red')         # a simple scatter plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp1IZc5Eyyx3"
      },
      "source": [
        "The plots above show a clear trade-off between the maximum tree depth and the performance: as we increase the depth, the algorithm performs better and better in the training set, until it eventually fits the data **perfectly** (at depths above 25 or so). In the test  however, there is initial improvement, but the errors then increase and eventually stabilize. What we are seeing is overfitting in action: as the tree becomes more complex, it fits the training data perfectly, but it stops being able to generalize. If we want our tree to work well on new data, we should pick a tree depth somewhere between 8 and 10. If you are wondering how one could proceed about finding this value in a systematic way, see our final section on **Limitations and Topics Left Out**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojryixiZyyxz"
      },
      "source": [
        "## Evaluating **Bias**\n",
        "The next step is to understand whether the model is able to predict \"uniformly well\" across the dataset. Specifically, does the quality of the prediction depend on certain feature values? Does our model exhibiting any bias in the way it predicts for different categories?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPYbmVHEPzM"
      },
      "source": [
        "A good starting point is to plot the prediction errors as a function of the features. This step is important, as it allows you to **inspect if the errors have a certain pattern**. Specifically, the main thing to look for is whether the magnitude of the prediction errors depends on the value of specific features. If that is indeed the case, it is evidence that your model may not be entirely adequate, and you may want to revisit the way the features are defined (for instance, you may want to define some new features). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92itZe7uF72a",
        "cellView": "form"
      },
      "source": [
        "#@title Define a few useful functions to examine the prediction errors\n",
        "\n",
        "#@markdown A function to see how errors depend on one feature at a time\n",
        "def visualize_errors(model_name, all_data, target_name, errors, features, \\\n",
        "                     figsize=(10,9)):\n",
        "\n",
        "    # calculate how many figures are needed\n",
        "    num_figs = len(features)\n",
        "    num_columns = (1 if num_figs==1 else 3)\n",
        "    num_rows = int(np.ceil(num_figs/num_columns))\n",
        "    \n",
        "    # one large figure with a common title\n",
        "    fig = plt.figure(figsize=(figsize[0]*num_columns,figsize[1]*num_rows))\n",
        "    \n",
        "    # title for plot\n",
        "    tar_ylabel = 'Errors in predictions for {}'.format(target_name)\n",
        "\n",
        "    # calculate the min/max errors\n",
        "    ymin = np.min(errors)\n",
        "    ymax = np.max(errors)\n",
        "\n",
        "    for i in range(num_figs):\n",
        "\n",
        "      if( (all_data[features[i]].dtype == 'float') or (all_data[features[i]].dtype == 'int') ):\n",
        "        # numeric feature \n",
        "\n",
        "        # Get min & max values\n",
        "        xmin = all_data[features[i]].min()\n",
        "        xmax = all_data[features[i]].min()\n",
        "        \n",
        "        # Create figure\n",
        "        plt.subplot(num_rows,num_columns,i+1)\n",
        "        ax1 = plt.gca()\n",
        "        #fig, ax1 = plt.subplots(figsize=figsize)\n",
        "        ax1.grid(alpha=.5, linewidth=1)\n",
        "        \n",
        "        # Scatter plot\n",
        "        color1 = 'tab:blue'\n",
        "        ax1.scatter(all_data[features[i]], errors, color=color1, alpha=0.2)\n",
        "        ax1.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "      else:\n",
        "        # categorical feature\n",
        "        plt.subplot(num_rows,num_columns,i+1)\n",
        "        ax1 = sns.boxenplot( data=all_data, x=features[i], y=errors, palette=\"Blues\")\n",
        "\n",
        "      ax1.set_xlabel(features[i], fontsize=12)\n",
        "      ax1.set_ylabel(tar_ylabel, color='black', fontsize=12)\n",
        "      ytickvals = ax1.get_yticks()\n",
        "      ax1.set_yticklabels([\"{:.0f}%\".format(v) for v in ytickvals], fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # determine the title\n",
        "    #tar_ylabel = '{}'.format(target_name) if target_name else 'target'\n",
        "    #fig.suptitle(\"Predicted value of {} as a function of feature(s).\".format(tar_ylabel), fontsize=16, color=color1)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#@markdown A function to see how errors depend on two features (color+size)\n",
        "def visualize_errors_two_features(all_data, feature_x, feature_y, errors,\\\n",
        "                                  model_name=\"Model\", dataset_name=\"Data\", figsize=(10,8)):\n",
        "\n",
        "    plt.figure(figsize=(figsize[0],figsize[1]))\n",
        "\n",
        "    vmax = errors.max()\n",
        "    vmin = errors.min()\n",
        "    maxlog = int(np.ceil(np.log10(vmax))) \n",
        "    minlog = int(np.ceil(np.log10(-vmin))) \n",
        "\n",
        "    # parameters for the logarithmic plot\n",
        "    logthresh = 0\n",
        "    logstep = 1\n",
        "    linscale = 1\n",
        "\n",
        "    # use matplotlib's symmetric normalizer\n",
        "    norm = mpl.colors.SymLogNorm(linthresh=10**-logthresh, linscale = linscale, \\\n",
        "                            vmin=vmin, vmax=vmax, base=10)\n",
        "\n",
        "    ax = plt.scatter(data=all_data, x=feature_x, y=feature_y, \\\n",
        "                    c=errors, s=errors.abs(),\\\n",
        "                    edgecolor='black',\n",
        "                    sizes=(100,20), cmap='RdBu_r', norm=norm, alpha=0.3)\n",
        "\n",
        "    # generate logarithmic ticks \n",
        "    tick_locations =([-(10**x) for x in range(-logthresh, \n",
        "                                              minlog + 1, \n",
        "                                              logstep)][::-1] \n",
        "                    +[0.0] \n",
        "                    +[(10**x) for x in range(logthresh, \n",
        "                                              maxlog + 1, \n",
        "                                              logstep)] )\n",
        "\n",
        "    cb = plt.colorbar(ticks = tick_locations, format = mpl.ticker.LogFormatter()) \n",
        "    log_ticks = cb.get_ticks()\n",
        "    cb.ax.set_yticklabels([\"{:,.0f}%\".format(v) for v in log_ticks], fontsize=14)\n",
        "    cb.set_label(\"Errors on logarithmic scale\", fontsize=14)\n",
        "\n",
        "    plt.ylabel(feature_y, fontsize=14)\n",
        "    plt.xlabel(feature_x, fontsize=14)\n",
        "    plt.title(\"Errors for {} in {} Set\".format(model_name,dataset_name))\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "QhABupYIRiJn"
      },
      "source": [
        "#@title Visualize how errors depend on **one feature** at a time\n",
        "\n",
        "#@markdown Select **one of the models** we trained:\n",
        "model = \"Linear Model\" #@param [\"Linear Model\", \"Decision Tree\", \"Random Forest\"]\n",
        "\n",
        "#@markdown Select a **feature** from the dropdown menu to visualize errors (**All** will display all features)\n",
        "\n",
        "feature = 'All' #@param [\"All\", \"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\", \"ISLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"<1H OCEAN\"]\n",
        "\n",
        "#@markdown Select **which dataset** to do the plot for:\n",
        "dataset = \"Training\" #@param ['Training', 'Test']\n",
        "\n",
        "#@markdown <br>**Feel free to customize some aspects of the plot**\n",
        "\n",
        "#@markdown Choose or type the width and height for the figure in inches:\n",
        "figure_width = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=5.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=5.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "#@markdown You can use **`max_abs_error`** to truncate errors above a certain threshold. This is useful if the errors for some samples are very large.<br>\n",
        "#@markdown Select or type a value (in %) where to truncate. Leave as **None** if you do not want to truncate any errors.\n",
        "\n",
        "max_abs_error = \"250\" #@param ['100', '200', '500', '1000', 'None'] {allow-input:true}\n",
        "\n",
        "# feature selection\n",
        "if feature==\"All\":\n",
        "  feature=list(X_train.columns)\n",
        "else:\n",
        "  feature=[feature]\n",
        "\n",
        "# select the right dataset\n",
        "if dataset==\"Training\":\n",
        "  results = results_train  # pass the original dataframes to be able to plot categorical features\n",
        "else :\n",
        "  results = results_test  # pass the original dataframes to be able to plot categorical features\n",
        "\n",
        "# select the right errors corresponding to the model\n",
        "if model == \"Linear Model\":\n",
        "  errors = results[\"linear_error(%)\"]\n",
        "elif model == \"Decision Tree\":\n",
        "  errors = results[\"tree_error(%)\"]\n",
        "else:\n",
        "  errors = results[\"rf_error(%)\"]\n",
        "\n",
        "# select the right dataset\n",
        "if dataset==\"Training\":\n",
        "  X_data = X_train\n",
        "  y_data = y_train\n",
        "  results = results_train\n",
        "else :\n",
        "  X_data = X_test\n",
        "  y_data = y_test\n",
        "  results = results_test\n",
        "\n",
        "# print the selections\n",
        "print(\"Your selection:\")\n",
        "print(\"Model:          {}\".format(model))\n",
        "print(\"Dataset:        {}\".format(dataset))\n",
        "print(\"Feature:        {}\".format(feature))\n",
        "\n",
        "# truncate errors if wanted\n",
        "if( max_abs_error != \"None\"):\n",
        "  max_abs_error = np.float(max_abs_error)\n",
        "  errors = errors.copy()\n",
        "  errors[errors>max_abs_error] = max_abs_error\n",
        "  errors[errors<-max_abs_error] = -max_abs_error\n",
        "  print(\"Truncating at:  {}%\".format(max_abs_error))\n",
        "\n",
        "# do the actual plot\n",
        "aux = visualize_errors(model_name=model, all_data=X_data, \\\n",
        "                       target_name='median_house_value', \\\n",
        "                       errors=errors, features=feature, \\\n",
        "                       figsize=(figure_width,figure_height))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBicDxVtOREx",
        "cellView": "form"
      },
      "source": [
        "#@title Visualize how errors depend on **two features** at a time\n",
        "\n",
        "#@markdown In this plot, the **circle size** corresponds to the (absolute) error magnitude, and the **colors** capture the direction of errors:<br> <font color=red>**red**</font> indicates large **positive** errors, and <font color=blue>**blue**</font> indicates large **negative** errors.\n",
        "\n",
        "#@markdown Select **one of the models** we trained:\n",
        "model = \"Random Forest\" #@param [\"Linear Model\", \"Decision Tree\", \"Random Forest\"]\n",
        "\n",
        "#@markdown Select two **features** from the dropdown menus to visualize errors\n",
        "\n",
        "feature_x = 'latitude' #@param [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\"]\n",
        "feature_y = 'longitude' #@param [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"median_house_value\"]\n",
        "\n",
        "#@markdown Select **which dataset** to do the plot for:\n",
        "dataset = \"Training\" #@param ['Training', 'Test']\n",
        "\n",
        "\n",
        "#@markdown <br><br>**Feel free to customize some aspects of the plot**\n",
        "\n",
        "#@markdown Choose or type the width and height for the figure in inches:\n",
        "figure_width = \"10\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=5.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"8\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=5.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "#@markdown You can use **`max_abs_error`** to truncate errors above a certain threshold. (This may be useful if the errors for some samples are extremely large.) \n",
        "#@markdown Select a value (in %) where to truncate. Leave as **None** if you do not want to truncate any errors.\n",
        "\n",
        "max_abs_error = \"None\" #@param ['100', '200', '500', '1000', 'None'] {allow-input:true}\n",
        "\n",
        "# select the right dataset\n",
        "if dataset==\"Training\":\n",
        "  results = results_train  # pass the original dataframes to be able to plot categorical features\n",
        "else :\n",
        "  results = results_test  # pass the original dataframes to be able to plot categorical features\n",
        "\n",
        "# select the right errors corresponding to the model\n",
        "if model == \"Linear Model\":\n",
        "  errors = results[\"linear_error(%)\"]\n",
        "elif model == \"Decision Tree\":\n",
        "  errors = results[\"tree_error(%)\"]\n",
        "else:\n",
        "  errors = results[\"rf_error(%)\"]\n",
        "\n",
        "# select the right dataset\n",
        "if dataset==\"Training\":\n",
        "  X_data = X_train\n",
        "  y_data = y_train\n",
        "  results = results_train\n",
        "else :\n",
        "  X_data = X_test\n",
        "  y_data = y_test\n",
        "  results = results_test\n",
        "\n",
        "# print the selections\n",
        "print(\"Your selection:\")\n",
        "print(\"Model:        {}\".format(model))\n",
        "print(\"Dataset:      {}\".format(dataset))\n",
        "print(\"Feature_x:    {}\".format(feature_x))\n",
        "print(\"Feature_y:    {}\".format(feature_y))\n",
        "\n",
        "# truncate errors if wanted\n",
        "if( max_abs_error != \"None\"):\n",
        "  max_abs_error = np.float(max_abs_error)\n",
        "  errors = errors.copy()\n",
        "  errors[errors>max_abs_error] = max_abs_error\n",
        "  errors[errors<-max_abs_error] = -max_abs_error\n",
        "  print(\"Truncating at: {}%\".format(max_abs_error))\n",
        "\n",
        "# do the actual plot\n",
        "aux = visualize_errors_two_features(all_data=results, feature_x=feature_x, \\\n",
        "                              feature_y=feature_y, errors=errors,\\\n",
        "                              model_name=model, dataset_name=dataset,\\\n",
        "                              figsize=(figure_width,figure_height))\n",
        "# uncomment next line to save\n",
        "#save_fig(\"scatterplot_of_{}_and_{}\".format(feature1,feature2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HruO0Ucjyyx0"
      },
      "source": [
        "This step is important for several reasons:\n",
        "   - it can allow you to detect if the errors have a certain pattern, which may inform the design of better features to use when training\n",
        "   - it can enable you to detect iif your ML algorithm exhibits some form of bias, such as predicting better for certain categories than others.\n",
        "   \n",
        "Additionally, having such variable predictive accuracy could also be evidence of systematic bias, which can be particularly problematic. such as predicting better for certain categories than others.\n",
        "\n",
        "We will return to these points in a future class, where we examine issues of bias and fairness in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQHisC3qyyxp"
      },
      "source": [
        "## **Interpreting** the Model\n",
        "\n",
        "The final question to address is **can we interpret/explain the model?**  Specifically,\n",
        " * Can we **concisely** explain **how** the model reaches its predictions?  \n",
        " * Can we quantify how the predictions **change** with the values of the features? \n",
        "\n",
        "Addressing these issues is critical if we want our models to be adopted in practice, so we a few processes that can help.\n",
        "\n",
        "_**Run the entire sub-section** and expand to see specific subsections._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzwP2k1aTpSo"
      },
      "source": [
        "### Direct **Visualization**\n",
        "If possible, the first step is to **visualize** the ML model, which can also help summarize its inner workings. Our ability to do this is unfortunately limited: **white-box** models like linear models or trees can be visualized, but this is significantly harder for **black-box** models like random forests. \n",
        "\n",
        "_We already visualized each model in the training step, so you can skip this section if everything made sense. Expand for a brief discussion._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QySOMRaOl6Fp"
      },
      "source": [
        "#### **Linear Models**\n",
        "Recall that in linear models, the predicted value of the target is a linear function of the predictors $X_1,\\dots,X_m$:\n",
        "> $ \\beta_0 + \\beta_1 \\cdot X_1 + \\dots + \\beta_m \\cdot X_m,$\n",
        "\n",
        "So all we need in order to understand the model are the parameters $\\beta_0, \\beta_1,\\dots,\\beta_m$. This achieves two things at once:\n",
        "1. it concisely summarizes the **mechanism** by which predictions are made: just apply the linear function to a new sample;\n",
        "2. it allows quantifying **the impact of each feature** on the prediction: this is given by the corresponding coefficient.\n",
        "\n",
        "**Caveat.** In practice, even linear models may be hard to interpret, particularly when they rely on **derived features** that depend in a complex way on the initial features. Such derived features are often added during the **feature engineering** step in order to improve the quality of the prediction, so this situation can easily arise in complex prediction problems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P00ftaG2OsHF",
        "cellView": "form"
      },
      "source": [
        "#@markdown Let's consider our linear model for an example. \n",
        "\n",
        "#@markdown First, let's print out the coefficients.\n",
        "print(\"intercept: {:<.3f}\".format(linear_model.intercept_))\n",
        "for k in range(len(X_train.columns)):\n",
        "  print(\"{}: {:<.3f}\".format(X_train.columns[k],linear_model.coef_[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAz0YEmOhHf"
      },
      "source": [
        "Access to the coefficients allows us to quickly summarize the impact of a feature in the original model. For instance, our model predicts that increasing the median house age by 1 (year) would increase the median house value by roughly \\$1,013, holding all else equal.\n",
        "\n",
        "However, this would no longer be the case with derived features. For instance, imagine that in order to improve the accuracy of our predictions, we decided to include among our predictors the squared value of the `total_rooms` feature or the ratio of `total_rooms` to `total_bedrooms`. Even if we had access to the coefficients for all these extra features (after fitting the larger model), we would have a much harder time in **concisely** summarizing the impact of `total_rooms`, as all the extra predictors would involve it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBQBiqOBXdPw"
      },
      "source": [
        "#### **Decision Trees**\n",
        "These lessons extend to the case of decision trees:\n",
        "* a tree with small depth can be easily visualized, and the mechanism by which it achieves its predictions can be summarized concisely through simple **IF**/**THEN** rules and **averaging**\n",
        "* the impact of a particular feature can be quantified **for small trees** (i.e., with low depth), but becomes harder for larger trees, where the same feature may appear in multiple nodes of the tree\n",
        "* as trees become more complex (e.g., deeper), our ability to understand their behavior is significantly hampered.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkr91lP0yyx8"
      },
      "source": [
        "### Plotting **Partial Dependencies**\n",
        "One of the best ways to understand the workings of complex ML models is through **partial dependence plots**. Let's see these in action.\n",
        "\n",
        "_**Run the entire sub-section** and expand for and explanation and the visualizations._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edlVmLiqLYhE",
        "cellView": "form"
      },
      "source": [
        "#@markdown Define a function to visualize partial dependence plots with histograms of features overlayed\n",
        "def partial_dependence_plots(dataset, model_name, model, X, features, target_name=False, show_hist=False, \\\n",
        "                             show_density=True, figsize=(10,9)):\n",
        "\n",
        "    from sklearn.inspection import partial_dependence\n",
        "\n",
        "    # calculate how many figures are needed\n",
        "    num_figs = len(features)\n",
        "    num_columns = (1 if num_figs==1 else 3)\n",
        "    num_rows = int(np.ceil(num_figs/num_columns))\n",
        "    \n",
        "    # one large figure with a common title\n",
        "    fig = plt.figure(figsize=(figsize[0]*num_columns,figsize[1]*num_rows))\n",
        "    \n",
        "    # title for plot\n",
        "    tar_ylabel = '{}'.format(target_name) if target_name else 'target'\n",
        "\n",
        "    for i in range(num_figs):\n",
        "\n",
        "      # Get partial dependence\n",
        "      pardep = partial_dependence(model, X, features[i], percentiles=(0, 1))\n",
        "\n",
        "      # Get min & max values\n",
        "      xmin = pardep[1][0].min()\n",
        "      xmax = pardep[1][0].max()\n",
        "      ymin = pardep[0][0].min()\n",
        "      ymax = pardep[0][0].max()\n",
        "      \n",
        "      # Create figure\n",
        "      plt.subplot(num_rows,num_columns,i+1)\n",
        "      ax1 = plt.gca()\n",
        "      #fig, ax1 = plt.subplots(figsize=figsize)\n",
        "      ax1.grid(alpha=.5, linewidth=1)\n",
        "      \n",
        "      # Plot partial dependence\n",
        "      color1 = 'tab:blue'\n",
        "      ax1.plot(pardep[1][0], pardep[0][0], color=color1)\n",
        "      ax1.tick_params(axis='y', labelcolor=color1)\n",
        "      ax1.set_xlabel(features[i], fontsize=12)     \n",
        "      ax1.set_ylabel('Average of predicted {}'.format(tar_ylabel), color=color1, fontsize=12)\n",
        "\n",
        "      ytickvals = ax1.get_yticks()\n",
        "      if( np.floor(np.max(np.abs(ytickvals))/1000) >= 1.0  ):\n",
        "        # y-tick values are all above 1000\n",
        "        ax1.set_yticklabels([\"${:.0f}K\".format(v/1000) for v in ytickvals], fontsize=12)\n",
        "      \n",
        "      #tar_title = target_name if target_name else 'Target Variable'\n",
        "      #ax1.set_title('Relationship between \"{}\" and \"{}\"'.format(feature[i], tar_title), fontsize=14)\n",
        "\n",
        "      if show_hist :\n",
        "          ax2 = ax1.twinx()\n",
        "          color2 = 'tab:red'\n",
        "          ax2.hist(X[feature[i]], bins=50, range=(xmin, xmax), alpha=.25, color=color2, density=show_density)\n",
        "          ax2.tick_params(axis='y', labelcolor=color2)\n",
        "          ax2.set_ylabel('Histogram of : {}'.format(features[i]), color=color2, fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # determine the title\n",
        "    #tar_ylabel = '{}'.format(target_name) if target_name else 'target'\n",
        "    #fig.suptitle(\"Predicted value of {} as a function of feature(s).\\nModel: {}. Dataset: {}.\".format(tar_ylabel,model_name,dataset), fontsize=16, color=color1)\n",
        "    #fig.suptitle(\"Predicted value of {} as a function of feature(s).\".format(tar_ylabel), fontsize=16, color=color1)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2x8p_vHdOYG"
      },
      "source": [
        "To understand what these plots are all about, suppose you want to know how the predictions of a complex ML model depend on some feature $X$. One thing you could do is to consider different values $x$ for the feature of interest, and for every such value, calculate the prediction in all the samples in your dataset when you force the value of $X$ in each sample to take the value $x$. You could then take the average of all the predictions, and plot this as a function of $x$. This dependency is exactly what is shown in a **partial dependence plot**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SRnVlLugyyx8",
        "cellView": "form"
      },
      "source": [
        "#@title Partial Dependence Plots (PDPs)\n",
        "\n",
        "#@markdown Select one of the models we trained:\n",
        "model = \"Random Forest\" #@param [\"Linear Model\", \"Decision Tree\", \"Random Forest\"]\n",
        "\n",
        "#@markdown Select which features(s) to visualize (picking **\"All\"** will plot dependencies for all features, but will take a bit more time):\n",
        "feature = \"All\" #@param ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', '<1H OCEAN', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN', \"All\"]\n",
        "\n",
        "#@markdown Select which dataset to do the plot in:\n",
        "dataset = \"Training\" #@param ['Training', 'Test']\n",
        "\n",
        "#@markdown <br><br>**Feel free to customize some aspects of the plot**\n",
        "\n",
        "#@markdown Choose or type the width and height for the figure in inches:\n",
        "figure_width = \"7\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_width==\"Auto\":\n",
        "  figure_width=7.0\n",
        "else:\n",
        "  figure_width=np.float(figure_width)\n",
        "\n",
        "figure_height = \"5\" #@param [\"Auto\",3,4,5,6,7,10]{allow-input: true}\n",
        "if figure_height==\"Auto\":\n",
        "  figure_height=5.0\n",
        "else:\n",
        "  figure_height=np.float(figure_height)\n",
        "\n",
        "#@markdown Check the box below if you would like to also see histograms of the feature values overlayed:\n",
        "\n",
        "#@markdown (This helps you interpret the PDPs more responsibly, as you can understand how representative that feature value is for your data.)\n",
        "\n",
        "show_histogram = True #@param {type:\"boolean\"}\n",
        "\n",
        "print(\"Your selection:\")\n",
        "print(\"Model:        {}\".format(model))\n",
        "print(\"Feature(s):   {}\".format(feature))\n",
        "print(\"Dataset:      {}\".format(dataset))\n",
        "\n",
        "# select the right model\n",
        "if model == \"Linear Model\":\n",
        "  use_model = linear_model\n",
        "elif model == \"Decision Tree\":\n",
        "  use_model = tree_model\n",
        "else:\n",
        "  use_model = rf_model\n",
        "\n",
        "if feature==\"All\":\n",
        "  feature = X_train.columns\n",
        "else:\n",
        "  feature = [feature]\n",
        "\n",
        "# select the right dataset\n",
        "if dataset==\"Training\":\n",
        "  X_data = X_train\n",
        "else :\n",
        "  X_data = X_test\n",
        "\n",
        "partial_dependence_plots(dataset=dataset, model_name=model, model=use_model, \\\n",
        "                         X=X_data, features=feature, target_name='median_house_value', \\\n",
        "                         show_hist=show_histogram, show_density=False, \\\n",
        "                         figsize=(figure_width,figure_height))\n",
        "                        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRk1Hl9GLYhI"
      },
      "source": [
        "Partial dependence plots (PDPs) are powerful tools to help you understand the inner workings on your trained Machine Learning model. To use them responsibly, it is worth knowing both the advantages and disadvantages.\n",
        "\n",
        "<font color=black>**Advantages:** </font>\n",
        "1. PDPs are an **intuitive** way to measure dependency: the average prediction if we forced all data points to assume a specific feature value\n",
        "2. PDPs are **easy to implement**: all that is required is to predict using our model, compute averages and plot\n",
        "3. PDPs yield a **causal interpretation for the model**: we are examining how changes in just one feature impact the prediction, so this is a causal dependency within the context of the model. (<font color=red>CAUTION: there is **no** causal statement here about real life!</font>)  \n",
        "4. PDPs are an **acccurate assesssment of dependency** if the plotted features are independent of all other features: in this case, the PDP perfectly interprets how the plotted features influences the prediction on average.\n",
        "\n",
        "<font color=black>**Disadvantages:** </font>\n",
        "1. You can visualize **at most two features** at once.  \n",
        "2. PDPs **assume independence**. We already mentioned this briefly above, but the calculations done with a PDP are guaranteed to be an accurate estimate of impact only when the plotted feature(s) are uncorrelated of other features. (Some other methods are more effective for correlated features, e.g., [Accumulated Local Effect plots](https://christophm.github.io/interpretable-ml-book/ale.html#ale).)  \n",
        "3. PDPs only capture averages, and thus might **hide heterogeneous effects**. (A potential fix are [Individual Conditional Expectation curves](https://christophm.github.io/interpretable-ml-book/ice.html#ice)).\n",
        "\n",
        "Some other tools are available to understand dependencies and to quantify the impact of features in **black-box** models. For more details and pointers, please refer to our final section on things left out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6uhpDOyR413"
      },
      "source": [
        "# <font color = red>**TASK #4**</font>\n",
        "<font color=purple>**A.** Conduct the following analysis **in EXCEL:**</font>\n",
        "1. Using the training data, train/fit a **linear regression model** to predict the **`median_house_value`** based on all the features. \n",
        "2. **According to your trained model**, what is the impact of `housing_median_age` and `median_income` on median house value?\n",
        "3. **According to your trained model**, what is the impact of geographic location on median house value?\n",
        "4. What is the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$ for your model?\n",
        "5. Are the coefficients in your model statistically significant? <br>(**Hint.** Look at the **P-value** column: if the value reported there is very small, it is an indication the coefficient is **significant**. More precisely, the likelihood that the coefficient is **zero** is at most **P-value**). \n",
        "6. Use your **linear regression model** to predict the median house value for the first 5 samples in the training set and for the first 5 samples in the test set, and calculate the **relative errors** in the prediction. <br>(**Hint.** It may be illustrative to do this \"by hand\", i.e., use the coefficients and predict. In that case, Excel's [SUMPRODUCT](https://support.microsoft.com/en-us/office/sumproduct-function-16753e75-9f68-4874-94ac-4d2145a2fd2e) function will be very useful! For the training set, you can also use the **Regression** functionality in **Data > Data Analysis**, and select **\"Residuals\"** when you train the model).\n",
        "\n",
        "<font color=purple>**B.** Using the information in the Notebook above, answer the following questions:</font>\n",
        "1. **Train a Decision Tree model with `max_depth=3` and answer the following questions:** \n",
        " - Can you interpret how the model would predict for a new sample?\n",
        " - **According to this model**, what features seem \"most important\" for predicting?  \n",
        " - What is the mean-squared error when predicting in the training set?   \n",
        " - What is the mean-squared error when predicting in the test set?   \n",
        "\n",
        "2. **For the Decision Tree model, what is the impact of the `max_depth` parameter used during training?** <br>\n",
        "Specifically, train a decision tree model where `max_depth=None`, and consider the following questions:\n",
        " - How do the errors/predictive accuracy change in the training set?  \n",
        " - How about the test set?   \n",
        " - What happens to the interpretability of the model?  \n",
        " - What does this tell you about the relationship between the accuracy of prediction and the complexity of the ML model?  \n",
        " - Is it always \"best\" to select a very complex ML model (e.g., large depth for the tree)?  \n",
        "\n",
        "3. **Consider the Random Forest model.**  \n",
        "  * Do you find any evidence of \"bias\" in how the algorithm is predicting in different geographic locations?    \n",
        "  * **According to the model**, does a larger `housing_median_age` lead to a lower or higher median house value?   \n",
        "  * **According to the model**, does a larger `median_income` lead to a lower or higher median house value?  \n",
        "  * What features seem to be \"important\" overall  for prediction in that model?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxA6BIHMyyyM"
      },
      "source": [
        "# **9. Things We Are Leaving Out**\n",
        "Due to the limited class time, our discussion here is far from exhaustive and  is leaving out several important topics. Those of you who are interested in more depth can uncomment this section for a brief discussion and a few pointers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXc_oHbtyyxB"
      },
      "source": [
        "### Avoiding **Sampling Bias** Through **Stratified Sampling**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBNLqwlguOu7"
      },
      "source": [
        "To understand what this is about, think about running a poll in which you select 1,000 people to ask a specific question. In a well-designed poll, you should ensure that the 1,000 people adequately represent the true demographics. For instance, the proportion of females in your sample is the same as the true proportion of females in the overall population, the proportion across age groups reflects the overall population, etc. This is known as **stratified sampling**: the population is divided into homogenous subgroups (called *strata*), and the right number of instances are sampled from each subgroup to guarantee that the training/test set are representative of the overall population. How we define the strata depends on the context, but generally we want to ensure that the training/test set adequately reflect the distribution of features that are important for predictive purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkmf9bRZyyxC"
      },
      "source": [
        "For a concrete example in the context of our problem, suppose we would like to ensure that the data used to train our algorithms properly represents different **income categories** in the population. Since our `median_income` feature has continuous values, we must first create a discrete set of income categories that will represent our strata. Below, we use the `pandas` function [`cut`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) to create a new data feature called `income_cat` that records whether the median income in a district falls in one of five income categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwndfW7aVBPp",
        "cellView": "form"
      },
      "source": [
        "#@title Create a new categorical feature based on income\n",
        "# create a new feature that records whether the income falls in one of 5 categories\n",
        "data[\"income_cat\"] = pd.cut(data[\"median_income\"],\n",
        "                            bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                            labels=[1, 2, 3, 4, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgAU48MkyyxD"
      },
      "source": [
        "Let's have a look at the new feature. We will print the first few rows, count the values and plot a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4HJgXSAMyyxE"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nuwBdL1VBPn",
        "cellView": "form"
      },
      "source": [
        "#@markdown Count the number of districts falling in each category\n",
        "print(data[\"income_cat\"].value_counts())\n",
        "\n",
        "#@markdown Do a histogram\n",
        "data[\"income_cat\"].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXmSAJXmyyxH"
      },
      "source": [
        "Let us repeat the original **random sampling** procedure. Note that we are again using the seed of 42, so we should get identical train/test sets as above, but these will now also include the `income_cat` feature. (If you want, feel free to print the first few rows and check!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7usE_Bg9yyxH"
      },
      "source": [
        "# a simple random sampling\n",
        "random_train_set, random_test_set = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEB2h_l_yyxJ"
      },
      "source": [
        "Now we can use `Scikit-Learn` to do **stratified sampling** based on the income category that we just created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9bT1a_RVBPu"
      },
      "source": [
        "#@title Do stratified sampling for training/test sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(data, data[\"income_cat\"]):\n",
        "    strat_train_set = data.loc[train_index]\n",
        "    strat_test_set = data.loc[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-vrf07ayyxK"
      },
      "source": [
        "Let's compare the two methods above for obtaining training/test sets to see which is more precise at capturing the proportions of income categories that exist in our original dataset. Below, the columns named `Overall`, `Random` and `Stratified` correspond to the proportions in the **overall** data and in the training set obtained from the **random** sampling and the **stratified** sampling procedures, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi3vkK35yyxL"
      },
      "source": [
        "# create a dataframe with columns recording the proportion of income categories in the three datasets  \n",
        "compare_props = pd.DataFrame({\n",
        "    \"Overall\": data[\"income_cat\"].value_counts() / len(data),\n",
        "    \"Random\": random_train_set[\"income_cat\"].value_counts() / len(random_train_set),\n",
        "    \"Stratified\": strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)\n",
        "}).sort_index()\n",
        "compare_props[\"% Error Random\"] = 100 * (compare_props[\"Random\"] / compare_props[\"Overall\"] - 1)\n",
        "compare_props[\"% Error Stratified\"] = 100 * (compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 1)\n",
        "\n",
        "compare_props  # print this out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMf4wgalVBP7",
        "outputId": "13d51a91-c3e3-4dca-a6b0-241f57948321"
      },
      "source": [
        "As you can see above, the error from randomized sampling can be quite large, particularly for certain income categories like 4 and 5. Stratified sampling produces a training set with proportions that are much closer to the original dataset, and helps avoid potential sampling biases.\n",
        "\n",
        "To conclude our brief discussion, it is also worth noting that **sampling bias may also occur in terms of missing data**: when our dataset has missing values, we would want to make sure that both the training and the test set have missing data in proportions that match those in the entire dataset. Let's see how the random and stratified split were performing from this point of view."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ym8ZvzmXyyxQ"
      },
      "source": [
        "#@title Calculate how many missing entries are in the test set under each way of splitting\n",
        "print(\"The original data has {:.2f}% missing entries in the total_bedrooms column.\"\\\n",
        "      .format( 100 * data[\"total_bedrooms\"].isnull().sum() / len(data)))\n",
        "print(\"The train set from the simple random split has {:.2f}% missing entries in the total_bedrooms column.\"\\\n",
        "      .format( 100 * train_set[\"total_bedrooms\"].isnull().sum() / len(train_set)))\n",
        "print(\"The train set from stratified sampling has {:.2f}% missing entries in the total_bedrooms column.\"\\\n",
        "      .format(100 * strat_train_set[\"total_bedrooms\"].isnull().sum() / len(strat_train_set)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8sYMLfIyyxR"
      },
      "source": [
        "As it happens, the random split with a seed of 42 was an **extremely poor** representation of the missing values: the training set had **no** data with missing entries, i.e., all missing rows were placed in the test set! We note that we did **not** choose 42 adversarially here -- rather, we followed the choice in a famous ML book, and by pure chance this results in a very bad split of missing records! Note that the same seed results in a better split for the stratified set, where the proportion is a lot closer to the original set. However, this was pure luck, as we did not correct for the issue explicitly. In practice, it would be better to explicitly check for this by creating a strata based on the missing entries as well.\n",
        "\n",
        "<font color=red>**Balance sampling bias with snooping bias.**</font> Although stratified sampling has some important advantages over purely random sampling, it also poses a potentially serious pitfall: it may introduce a certain degree of \"data snooping\" bias, because we are **using the entire dataset** and also **specific features** to define what are the important strata and to judge the training/test sets. In practice it is important to not \"overdo\" this, and to remember that there should be a balance between sampling and snooping bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgKKX0Lu9BY6"
      },
      "source": [
        "# drop the new income category from the original dataset\n",
        "data.drop(\"income_cat\", axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAuyGadgAdYi"
      },
      "source": [
        "### **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnOv5WTQAdYm"
      },
      "source": [
        "In order to improve prediction in practical problems, one must often resort to introducing new features that are **derived** from the initial data. This  critical step of the ML pipeline is referred to as **feature engineering**, and is a combination of art and science, as domain knowledge can be critical when it comes to deriving meaningful features. In fact, experience suggests that this step is often **more important** in improving prediction than the specific algorithms used!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOaSuGdtAdYn"
      },
      "source": [
        "To understand what this is about in a more concrete example, you may recall from our Exploratory Data Analysis that several features in our data were not strongly correlated with the `median_house_value` target, despite our intuition that they should be meaningful. Two such examples were `total_bedrooms` and `total_rooms`: both calculated as **totals per district**, so were not as  informative of house value. But perhaps by considering some **derived** features such as the number of **rooms_per_household** or the number of **rooms_per_bedroom**, we could improve the predictions of our model, and even make its features more relevant. \n",
        "\n",
        "In fact, we can quickly confirm that this might be the case by calculating these new features and their correlations with the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaBzCAaEAdYn",
        "cellView": "form"
      },
      "source": [
        "#@title Derive some new features and calculate the correlation matrix\n",
        "data[\"rooms_per_household\"] = data[\"total_rooms\"]/data[\"households\"]\n",
        "data[\"rooms_per_bedroom\"] = data[\"total_rooms\"]/data[\"total_bedrooms\"]\n",
        "\n",
        "new_corr_matrix = data.corr()\n",
        "plt.figure(figsize = (10,5));\n",
        "sns.heatmap(new_corr_matrix, annot=True, fmt=\".2f\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT1iYccyAdYx"
      },
      "source": [
        "Note that the new features `rooms_per_household` (calculated as the ratio of `total_rooms` and `households`) and `rooms_per_bedroom` (calculated as the ration of `total_bedrooms` and `total_rooms`) are now more correlated with our `median_house_value` target than the simpler features we started off with, so including them may improve the accuracy of our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMHehu7Sl-x4"
      },
      "source": [
        "### Calibrating **hyper-parameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LISbqQaHmFOH"
      },
      "source": [
        "You may recall that when we created the decision tree and the random forest models, there were several **hyper-parameters** that we could specify, such as for instance `max_depth`. These values are very important in practice, and can critically drive the performance of our ML algorithms in new data (as our brief discussion of overfitting hopefully convinced you.) \n",
        "\n",
        "There is a systematic way to search for hyper-parameter values using a technique called **cross-validation**, and modern R or Python packages have automatic implementations that enable you to accomplish this. For instance, [here](https://scikit-learn.org/stable/modules/cross_validation.html) a description for the SciKit Learn Python package. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNYn5sn7mlzs"
      },
      "source": [
        "### Changing the **measure of accuracy** to reflect other business KPIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T64RxI9dyyyM"
      },
      "source": [
        "In practice, you may want more flexibility when defining the performance metrics of your ML model. For instance, instead of simply judging predictive accuracy using `mse`, you may want to define your own scoring rule. This is particularly important if you have an idea of **how the ML model will be used subsequently**, since you may be able to capture the relevant business KPIs much better. We will discuss this more in a future class, but for now, we just wanted to note that modern packages allow you to do easily change the scoring function used during trainig. For instance, [here](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)  is a description for SciKitLearn. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqKhhmH-uluI"
      },
      "source": [
        "### More Advanced Tools for **Interpretability**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dsc_AdJusgo"
      },
      "source": [
        "In business contexts, the ability to explain and interpret ML models is key to adoption. We discussed some ways to do this through **visualizations** and **partial dependence plots**, but other methods and tools have been especially designed for this purpose. Examples include:\n",
        "* computing **local surrogates** to explain specific predictions in your model. The idea here is that in order to explain why your model is predicting in a certain way for a specific sample/datapoint, you can purturb that point, calculate predictions for these slightly perturbed samples, and fit some simpler interpretable models to those predictions. Implementations are available for this idea in both R and Python (for more details, see [this](https://christophm.github.io/interpretable-ml-book/lime.html)).\n",
        "* computing **Shapley values** for your model. This is a slightly different way to explain how your model reaches predictions and to quantify the importance of features, based on ideas from economics. Both R and Python implementations exist (for details, see [this](https://christophm.github.io/interpretable-ml-book/shapley.html#disadvantages-13)). We include a quick example using [SHAP](https://github.com/slundberg/shap) Python implementation below, to show you the kinds of things that are possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjfvYgf4yyx_"
      },
      "source": [
        "#### Using **SHAP** to Explain Complex Models\n",
        "\n",
        "We will use [SHAP](https://github.com/slundberg/shap) to explain individual predictions for a random forest model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TehaQOayyyB",
        "cellView": "form"
      },
      "source": [
        "#@title import shap library\n",
        "!pip install shap &> /dev/null\n",
        "import shap\n",
        "\n",
        "#@markdown Train a **Random Forest** model with the first 1000 samples from our training data\n",
        "\n",
        "##@markdown Select how many samples to use in training:\n",
        "#num_samp = \"500\" #@param [500, 1000, 2000]\n",
        "num_samp = 1000  # first 1000 samples of the training set\n",
        "\n",
        "small_rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=42, oob_score=True)\n",
        "\n",
        "num_samp = min(1000, len(X_train.index))\n",
        "small_X_train = X_train.iloc[0:num_samp,:]\n",
        "small_y_train = y_train[0:num_samp]\n",
        "small_rf_model.fit(small_X_train, small_y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZoDGn1Z247N",
        "cellView": "form"
      },
      "source": [
        "#@markdown Select a sample to visualize:\n",
        "sample = 49 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "\n",
        "# explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(small_rf_model)\n",
        "shap_values = explainer.shap_values(small_X_train)\n",
        "\n",
        "# visualize the first prediction's explanation \n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values[sample,:], small_X_train.iloc[sample,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Td6Pa9nyyyC"
      },
      "source": [
        "**Interpretation**. The plot above shows how each feature is contributing to push the model output from a specific **base value** (equal to the **average predicted target** over the entire training dataset) to the actual model output **for a given sample**. Features pushing the prediction higher are shown in red and those pushing the prediction lower are in blue. These types of plots are called **force_plots**, because each feature is depicted like a force pushing the model output in a specific direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSEAnI3CyyyD"
      },
      "source": [
        "If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset as shown below. The following plot is interactive. Just scroll the mouse and see the different values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBMjH4xFyyyD",
        "cellView": "form"
      },
      "source": [
        "#@title Visualize the SHAP Explanations in the Entire Training Set\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values, small_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gXSiKv9yyyE"
      },
      "source": [
        "We can also compute the **importance** of each feature, i.e., a measure of how much each feature contributes to driving the model output; this is calculated **on average**, over the entire training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sf-2s3K3yyyE",
        "cellView": "form"
      },
      "source": [
        "#@title Compute the feature importance\n",
        "shap_values = shap.TreeExplainer(small_rf_model).shap_values(small_X_train)\n",
        "shap.summary_plot(shap_values, small_X_train, plot_type=\"bar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-vvBGV-yyyF"
      },
      "source": [
        "We can also get more actionable information by looking at a SHAP **summary plot**. This combines feature importance with feature effects, and is done with the points in the training data. It demonstrates the following information:\n",
        " - *Feature importance*: Variables are ranked in descending order.\n",
        " - *Impact*: The horizontal location shows whether the impact of the variable is associated with a higher or lower prediction in each sample.\n",
        " - *Correlations*: You can assess the magnitude and direction of the correlations between features and the predicted value through the color and location of points on the X-axis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpJmWsxyyyF",
        "cellView": "form"
      },
      "source": [
        "#@title Create a SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, small_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1jdHLXnyyyH"
      },
      "source": [
        "Lastly, we can use SHAP to do a **Dependence Plot**. This is a generalization of the partial dependence plots we saw earlier, and it shows the marginal effect one or two features on the predicted outcome in every sample. Using it, we can assess whether the relationship between the target and features is linear, monotonic or more complex. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "h_wQZTZByyyJ",
        "cellView": "form"
      },
      "source": [
        "#@title Create a SHAP Dependence Plot\n",
        "\n",
        "#@markdown Select which feature to visualize: <br>\n",
        "#@markdown (the function will automatically includes another feature that the chosen feature interacts most with)\n",
        "feature = \"median_income\" #@param ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
        "\n",
        "shap.dependence_plot(feature, shap_values, small_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxgPnN8zyyyN"
      },
      "source": [
        "## **10. References**\n",
        "If you would like to learn more about some of the topics covered here, the following are a great set of resources:\n",
        " - [An Introduction to Statistical Learning with Applications in R](https://www.springer.com/gp/book/9781461471370) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani. Excellent and very accesible reference for conceptual elements + implementation in R.\n",
        " - [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291) by Aurélien Géron. Great reference for Machine Learning in Python, primarily on the coding and implementation side.\n",
        " - [Interpretable ML Book: A Guide to Making Black-Box Models Interpretable](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar. Great reference for going deeper on the topic of Interpretable Machine Learning."
      ]
    }
  ]
}